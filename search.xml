<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[谈谈Web开发中的跨域问题]]></title>
    <url>%2F2019%2F08%2F14%2Fcross-domain%2F</url>
    <content type="text"><![CDATA[谈谈Web开发中的跨域问题 什么是跨域『跨域』对于Web开发者来说是永远绕不过去的一个问题，日常开发中在和前端联调阶段经常就会因为跨域而联调失败。那么啥是跨域问题呢？简单地说，跨域问题的根源是浏览器的同源策略引起的，这是出于安全考虑的。 那么什么是同源呢？同源：同协议&amp;同域名&amp;同端口同源策略：针对跨域请求，请求发送了，服务器响应了，但是无法被浏览器接收看几个例子1234567举例来说：http://www.example.com/dir/page.html协议是http://，域名是www.example.com，端口是80（默认端口可以省略）。它的同源情况如下：http://www.example.com/dir2/other.html：同源https://example.com/dir/other.html ：非同源（协议不同）http://v2.www.example.com/dir/other.html：非同源（域名不同）http://www.example.com:81/dir/other.html：非同源（端口不同） 假如没有同源策略又会怎么样？我们看个例子：假设你找到了一个盗版网站(www.meiju.com) 正在观看热播的美剧《绝命毒师》，看完一集之后突然想起来昨天加了购物车的宝贝还没有支付，于是打开了淘宝(www.taobao.com) ，登录之后就去支付，接着就愉快地去追剧了。but，事情并没有这么简单，晚上睡前你又看了下今天的订单，发现多了很多不是自己下的订单并且已经支付。我们看看到底发生了什么，由于浏览器没有同源限制，在你登录了淘宝网之后携带了相应的cookie，而这时候盗版网站内置的js发送了很多购物的请求（淘宝网的服务器发现携带了你的登录cookie），这不是坑爹呢么。 所以同源策略是相当必要的，但是在我们日常开发中经常有跨域的需求。一般公司都会有多个域名，假设公司A，他有这些域名，www.A.com,m.A.com,g.A.com如果本次开发任务的页面是www.A.com 这个域名，而后端开发的ajax接口域名为m.A.com ，这时候就会因为浏览器的同源策略导致调用接口失败。 对于这种合理的跨域请求，有啥解决办法吗？我们继续往下看 解决跨域的方法一般常见的有两种方法，JSONP和CORS我们要先明确几个点1.一般跨域都是针对ajax接口来讲的2.同源策略限制的只是浏览器能否正常展示后端返回值，即使存在跨域问题，前端的请求也是可以打到后端服务器的3.所以我们需要对信任的域名开白处理 JSONP原理：浏览器同源策略并不限制请求不同源的JS脚本，这就为开发者留下了后门，我们可以把原本要返回的json格式返回值，封装为一段JS代码，前端获得代码后会触发前后端约定好的函数从而完成整套逻辑。举个例子：1234567&lt;script src='http://localhost:8080/testJsonp?callback=luyu05' type='text/javascript'/&gt;&lt;/script&gt;&lt;script&gt; function luyu05(data)&#123;//data需要的数据 alert(data.message); &#125;&lt;/script&gt; 当页面载入时候，会去拉取src中指定的JS脚本，即发送一个GET请求（这也是JSONP的短板，只能支持GET方法）http://localhost:8080/testJsonp?callback=luyu05 到后端服务器。我们再看看后端代码1234567891011@Controllerpublic class TestJsonP &#123; @RequestMapping("/testJsonp") @ResponseBody public Object test() &#123; JsonObject res = new JsonObject(); res.put("key0", "value0"); res.put("key1", "value1"); return res; &#125;&#125; Spring提供了统一地解决方案，只要配置了如下的bean就ok了，我们看到这里借助了Spring的aop机制（其实是针对每个@ResponseBody的返回值进行织入）123456@ControllerAdvicepublic class JsonpAdvice extends AbstractJsonpResponseBodyAdvice &#123; public JsonpAdvice() &#123; super("callback"); &#125;&#125; 看下返回值1234luyu05(&#123; "key0": "value0", "key1": "value1"&#125;); 当前端接收到这串代码时候，就会执行前端的luyu05方法（这个就是所谓的回调），而我们的json就是方法的入参data，从而完成整个流程。 CORS相比JSONP的取巧方式，个人觉得CORS更加合规一些。 原理：跨域资源共享标准新增了一组HTTP首部字段，通过这些字段让浏览器与服务器进行沟通，从而决定请求应该成功还是失败。 分类：浏览器将跨域请求分为两类请求 只要同时满足以下两大条件，就属于简单请求。1234567请求方法是以下三种方法之一：HEAD、GET、POSTHTTP的头信息不超出以下几种字段：AcceptAccept-LanguageContent-LanguageLast-Event-IDContent-Type：只限于三个值application/x-www-form-urlencoded、multipart/form-data、text/plain 凡是不同时满足上面两个条件，就属于非简单请求。 浏览器对这简单请求和非简单请求的处理是不一样的。 针对简单请求需要在Response的header里的Access-Allow-Origin的值包含请求头中Origin的域名（支持通配符和完整匹配两种）后端代码如下12345public static void crossDomain(HttpServletRequest request,HttpServletResponse response)&#123; String origin = request.getHeader("Origin"); if(!whiteList.contains(origin)) return; response.setHeader("Access-Control-Allow-Origin", origin);&#125; 针对非简单请求会执行一次预检查请求，这次的请求方法类型是OPTIONS，代表这次是用来询问的（如果servlet针对OPTIONS请求也进行处理可能会出现接口被调用两次的情况），除了Access-Allow-Origin外，此时还需要Access-Control-Request-Method和Access-Control-Request-Headers，这个其实很好理解，这次预检请求就是问问服务器是否支持当前的非简单请求（Method or Header特殊），如果支持的话会发送下次的真正请求，反之直接抛出跨域错误123456789101112public static void crossDomain(HttpServletRequest request,HttpServletResponse response)&#123; String origin = request.getHeader("Origin"); //过滤非法域名的请求 if(!whiteList.contains(origin)) return; response.setHeader("Access-Control-Allow-Origin", origin); //如果需要http请求中带上cookie，需要前后端都设置credentials，且后端设置指定的origin response.setHeadr("Access-Control-Allow-Credentials", true); //需要针对非简单请求的特异项进行配置，比如本次请求方法是PUT方法，那么需要在Access-Control-Allow-Methods中写入PUT //相应地如果本次请求加入了自定义的header项，那么需要在Access-Control-Allow-Headers中写入对应的header项 response.setHeader("Access-Control-Allow-Methods", "GET, POST, PUT, DELETE"); response.setHeader("Access-Control-Allow-Headers", "customer-header");&#125; 两种方案优缺点对比1.JSONP因为采用获取资源的方式，所以只能支持GET方法；而CORS基本支持所有类型的方法2.JSONP方案前端需要进行特殊的编码；而CORS方案对前端几乎完全透明3.可能某些老版本的浏览器不支持CORS方式，此时只能使用JSONP4.使用CORS时可能出现预检请求触发了业务逻辑的问题，这点要注意5.两种方案如果不做特殊处理都会执行对应ajax的逻辑，只是不返回到浏览器而已（因此某些场景要考虑过滤无效请求or保证接口幂等） 参考资料：http://www.ruanyifeng.com/blog/2016/04/cors.htmlhttps://segmentfault.com/a/1190000015597029]]></content>
      <categories>
        <category>Java-Web</category>
      </categories>
      <tags>
        <tag>Java-Web</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java核心技术拓展]]></title>
    <url>%2F2019%2F07%2F26%2FJava-Core-Answer%2F</url>
    <content type="text"><![CDATA[读极客时间《Java核心技术36讲》有感 解释执行和编译执行？解释执行可以理解为一个中国人和外国人对话，解释器相当于翻译人员，解释执行就是中国人说一句，翻译人员就翻译一句，告诉外国人而编译执行，翻译人员直接讲全文翻译成英文，再把翻译文件给外国人看 对于Java这种解释和编译共存的语言就类似于： 而日常对话中有很多常用语句，翻译人员已经翻译无数次了，她想到了巧妙的方式，索性把常用语句直接写到白板上，这样当下次中国人再想说常用语句时候直接举白板就ok了上述例子，中国人：高级语言（Java） 外国人：计算机 翻译：JVM 解释器 中国话：字节码 外国话：机器码显然编译执行很快，但是需要生成中间文件（目标程序），不适合内存紧张的场景；而解释执行，无需中间文件（目标程序），解释一句执行一句 是不是可以理解为，对计算机来讲输入都是一样的『机器码』，而解释执行是在内存中生成机器码用后即删，而编译执行会持久化到本地反复使用 Java异常体系Error VS ExceptionError更为严重，一般是JVM报的错误，比如OOM，SOF等，一般无法恢复；Exception一般都是可以通过编码避免的，分为两个类型checked和unchecked的，换言之就是编译器异常和运行期异常，前者包括sleep、io等操作时候必须catch的异常InterruptException、IOException等；后者包括NPE、ArrayIndexOutOfBoundException 1）除非有特殊用途否则不要catch Error 或者 Throwable2）catch到的异常一般不推荐使用printStackTrace,最好使用日志组件，将日志统一打印到一个地方3）敏感信息不要打印到日志中 ClassNotFoundException VS NoClassDefFoundErrorClassNotFoundException相对简单，它是一个编译期异常，当我们使用Class.forName时候就会要求我们检查该异常，举个例子12345678class C &#123; public static void main(String[] args) throws ClassNotFoundException &#123; test(); &#125; public static void test() throws ClassNotFoundException &#123; Class.forName("Foo"); &#125;&#125; 输出： java.lang.ClassNotFoundException: B 而后者可能模拟起来相对困难，看下面的例子12345678910111213141516public class MockNoClassDefFoundError &#123; public static void main(String[] args) throws InterruptedException &#123; new Thread(new Runnable() &#123; public void run() &#123; new B(); &#125; &#125;).start(); new B(); &#125;&#125;class B &#123; static &#123; int a = 1/0; &#125;&#125; 输出：java.lang.NoClassDefFoundError: Could not initialize class com.luyu.B 这里为啥要使用一个多线程呢？因为如果不用多线程的话，主线程直接就会抛java.lang.ExceptionInInitializerError，而使用多线程其中一个线程用来触发B的clinit方法，此时类初始化失败，而另一个线程再去使用B的时候就报NoClassDefFoundError了 Java中的引用我们知道Java中共有四种引用，强软弱虚。为啥需要这么多引用呢，如果只有可回收与不可回收两种状态，那么对于一些食之无味弃之可惜的对象就没办法表示了，所以引用不能非黑即白。1）强引用，即使发生OOM也不会回收的对象2）软引用，垃圾回收一般不会回收，但是发生OOM前会将软引用对象回收3）弱引用，垃圾回收会收走 Ps：Guava Cache/ThreadLocal都是该类型，和软引用一样适用于内存敏感情况下的缓存情景4）虚引用，在执行过finalize之后处于虚引用状态，唯一存在的目的就是在对象被回收时可以发送一个系统通知]]></content>
      <categories>
        <category>Java 进阶</category>
      </categories>
      <tags>
        <tag>Java 进阶</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring Framework常见扩展点及其应用]]></title>
    <url>%2F2019%2F06%2F26%2FSpringFrame-Extension-Point%2F</url>
    <content type="text"><![CDATA[本文主要介绍Spring Framework常见扩展点及其在工程上的应用 综述Spring Framework包含两个核心概念，IOC(Inversion of Control)和AOP(Aspect Oriented Programming) Spring Framework的设计理念：解耦 为什么引入IOC？先来看一个例子123456class Programer &#123; Computer computer = new Mac2015(); private void work() &#123; computer.help(); &#125;&#125; 此时有一个问题就是programer和computer耦合在一起，这个programer不具备扩展性（它只会用mac2015），如果此时公司换了一批电脑Mac2016，那么需要换一批新的程序员，这显然是不合理的。从设计的角度来讲，类本身就是定义的一个模板亦或是一种抽象，如果抽象和具体的实现绑定或者耦合在一起，那么就不是纯粹的抽象了。这也违背了设计模式中的don’t call me法则。 所以这个时候要把computer和programer解耦。解耦的方法很简单，computer的具体实现由调用方指定就ok，而不是在类内部自行指定。那么类就需要暴露一些点供外界实现注入功能。 常见的方法有三种1)构造函数注入2)set注入3)接口注入 这就是Spring 依赖注入的基本思路那么选择何种注入方式以及需要注入哪些属性是怎么通知到Spring的呢？ 常见的方法有两种1)注解（@resource/@autowire）2)配置文件（xml/properties） 举个例子12345@Serviceclass TestService &#123; @Resource //注解方式通知Spring 1.通过set注入方式 2.注入userService属性 private UserService userService;&#125; 123&lt;bean id='testService' class='com.luyu.TestService'&gt; &lt;property name="userService" ref="userService"&gt; //配置文件方式通知Spring 通过set注入方式 注入userService属性&lt;bean /&gt; IOC：实现了当前类和所依赖类的具体实现间的解耦 为什么引入AOP？AOP旨在将辅助逻辑（日志、安全、监控等）从业务主体逻辑中进行剥离，实现关注点分离，以提高程序的模块化程度 举个例子，下面是一个向db提交任务的简单方法，该方法在提交到db前需要放到blocking queue中，后台启动线程将queue中的任务插入es中12345public long addTask(DispatchTaskEntity taskEntity) &#123; //... taskDao.addTask(entity); //...&#125; 123456&lt;bean id="myIntercepter" class="com.MyIntercepter"/&gt;&lt;!-- 新增任务aop --&gt;&lt;bean id="addOfdispatchTaskAdvisor" class="org.springframework.aop.aspectj.AspectJExpressionPointcutAdvisor"&gt; &lt;property name="advice" ref="myIntercepter" /&gt; &lt;property name="expression" value="execution(* com.TaskDaoImpl.addTask(..))" /&gt; &lt;/bean&gt; 12345678910111213141516171819202122232425public class MyIntercepter implements MethodInterceptor&#123; @Override public Object invoke(MethodInvocation invocation) throws Throwable &#123; Method method = invocation.getMethod(); String methodName = method.getName(); if("addTask".equalsIgnoreCase(methodName))&#123; return dealAdd(invocation); &#125; //origin method return invocation.proceed(); &#125; private Object dealAdd(MethodInvocation invocation) throws Throwable &#123; Object result = invocation.proceed(); Long taskId = (Long) result; if(taskId &gt; 0)&#123; TaskDo taskEntity = taskDao.loadTaskById(taskId); //异步插入es，通过先插入bq实现 EsHelper.recordDetail(taskEntity, MonitorType.Add); &#125; return result; &#125; &#125; AOP：实现了业务逻辑与辅助逻辑之间的解耦 Bean的生命周期Bean作为Spring Framework的核心，我们有必要研究下它的生命周期 结论简单地将Bean的生命周期分为五个主干流程1.容器启动阶段（严格来讲这个不属于Bean的生命周期）2.Bean（单例非懒加载）的实例化阶段3.Bean的属性注入阶段4.Bean的初始化阶段5.Bean的销毁阶段下图是整个Spring容器的启动流程，可以看到除了上述五个主干流程外，Spring还提供了很多扩展点（这部分将在第三节讲述） 验证通过一个实际的例子验证上图首先定义一个业务Bean，实现了诸多上述的扩展点接口123456789101112131415161718192021222324252627282930313233343536373839public class Car implements BeanFactoryAware, BeanNameAware, InitializingBean, DisposableBean &#123; private String carName; private BeanFactory beanFactory; private String beanName; public Car() &#123; System.out.println("bean的构造函数"); &#125; public void setCarName(String carName) &#123; System.out.println("属性注入"); this.carName = carName; &#125; // 这是BeanFactoryAware接口方法 public void setBeanFactory(BeanFactory arg0) throws BeansException &#123; System.out.println("BeanFactoryAware.setBeanFactory()"); this.beanFactory = arg0; &#125; // 这是BeanNameAware接口方法 public void setBeanName(String arg0) &#123; System.out.println("BeanNameAware.setBeanName()"); this.beanName = arg0; &#125; // 这是InitializingBean接口方法 public void afterPropertiesSet() throws Exception &#123; System.out.println("InitializingBean.afterPropertiesSet()"); &#125; // 这是DiposibleBean接口方法 public void destroy() throws Exception &#123; System.out.println("DiposibleBean.destory()"); &#125; // 通过&lt;bean&gt;的init-method属性指定的初始化方法 public void myInit() &#123; System.out.println("&lt;bean&gt;的init-method属性指定的初始化方法"); &#125; // 通过&lt;bean&gt;的destroy-method属性指定的初始化方法 public void myDestory() &#123; System.out.println("&lt;bean&gt;的destroy-method属性指定的初始化方法"); &#125;&#125; 自定义了一个特殊的BeanPostProcessor类型的Bean123456789101112131415161718192021public class MyBeanPostProcessor implements BeanPostProcessor &#123; public MyBeanPostProcessor() &#123; super(); System.out.println("BeanPostProcessor构造函数"); &#125; public Object postProcessAfterInitialization(Object arg0, String arg1) throws BeansException &#123; //可以针对指定的Bean做一些操作 if (arg1.equals("car")) &#123; System.out.println("BeanPostProcessor.postProcessAfterInitialization()"); &#125; return arg0; &#125; public Object postProcessBeforeInitialization(Object arg0, String arg1) throws BeansException &#123; if (arg1.equals("car")) &#123; System.out.println("BeanPostProcessor.postProcessBeforeInitialization()"); &#125; return arg0; &#125;&#125; 自定义了一个特殊的BeanFactoryPostProcessor类型的Bean12345678910public class MyBeanFactoryPostProcessor implements BeanFactoryPostProcessor &#123; public MyBeanFactoryPostProcessor() &#123; super(); System.out.println("BeanFactoryPostProcessor构造函数"); &#125; public void postProcessBeanFactory(ConfigurableListableBeanFactory arg0) throws BeansException &#123; System.out.println("BeanFactoryPostProcessor.postProcessBeanFactory()"); &#125;&#125; 自定义了一个特殊的InstantiationAwareBeanPostProcessor类型的Bean1234567891011121314151617181920212223242526272829303132public class MyInstantiationAwareBeanPostProcessor extends InstantiationAwareBeanPostProcessorAdapter &#123; public MyInstantiationAwareBeanPostProcessor() &#123; super(); System.out.println("InstantiationAwareBeanPostProcessor的构造函数"); &#125; // 接口方法、实例化Bean之前调用 @Override public Object postProcessBeforeInstantiation(Class beanClass, String beanName) throws BeansException &#123; if(beanName.equals("car")) &#123; System.out.println("InstantiationAwareBeanPostProcessor.postProcessBeforeInstantiation()"); &#125; return null; &#125; // 接口方法、实例化Bean之后调用 @Override public boolean postProcessAfterInstantiation(Object bean, String beanName) throws BeansException &#123; if(beanName.equals("car")) &#123; System.out.println("InstantiationAwareBeanPostProcessor.postProcessAfterInstantiation()"); &#125; return true; &#125; // 接口方法、设置某个属性时调用 @Override public PropertyValues postProcessPropertyValues(PropertyValues pvs, PropertyDescriptor[] pds, Object bean, String beanName) throws BeansException &#123; if(beanName.equals("car")) &#123; System.out.println("InstantiationAwareBeanPostProcessor.postProcessPropertyValues()"); &#125; return pvs; &#125;&#125; 将上述1个业务Bean和3个特殊的Bean 配置到xml中1234&lt;bean id="beanPostProcessor" class="test.MyBeanPostProcessor"/&gt;&lt;bean id="instantiationAwareBeanPostProcessor" class="test.MyInstantiationAwareBeanPostProcessor"/&gt;&lt;bean id="beanFactoryPostProcessor" class="test.MyBeanFactoryPostProcessor"/&gt;&lt;bean id="car" class="test.Car" init-method="myInit" destroy-method="myDestory" scope="singleton" p:carName="BMW"/&gt; 下面来见证下整个流程1234public static void main(String[] args) &#123; ClassPathXmlApplicationContext factory = new ClassPathXmlApplicationContext("spring/applicationContext.xml"); factory.destroy();&#125; 控制台输出如下1234567891011121314151617BeanFactoryPostProcessor构造函数BeanFactoryPostProcessor.postProcessBeanFactory()InstantiationAwareBeanPostProcessor的构造函数BeanPostProcessor构造函数InstantiationAwareBeanPostProcessor.postProcessBeforeInstantiation()bean的构造函数InstantiationAwareBeanPostProcessor.postProcessAfterInstantiation()InstantiationAwareBeanPostProcessor.postProcessPropertyValues()属性注入BeanNameAware.setBeanName()BeanFactoryAware.setBeanFactory()BeanPostProcessor.postProcessBeforeInitialization()InitializingBean.afterPropertiesSet()&lt;bean&gt;的init-method属性指定的初始化方法BeanPostProcessor.postProcessAfterInitialization()DisposableBean.destory()&lt;bean&gt;的destroy-method属性指定的初始化方法 我们可以看到验证与预期相符合。值得注意的是而在整个Bean的生命周期中除了上文提到的五大核心流程外，还穿插了很多Spring FrameWork提供的扩展点，这也是我们下文讨论的重点。 Spring Framework扩展点及其应用日常开发中几乎用不到这些『屠龙技』，但是当阅读Java中间件源码的时候经常会见到它们。我们接下来会按照下图中的顺序逐一介绍这些扩展点。 BeanFactoryPostProcessor#postProcessBeanFactory背景：由于历史原因，团队有些service项目很臃肿，整个工程中bean的数量有上百个，而大部分单测依赖都是整个工程的xml，导致单测执行时需要很长时间（大部分时间耗费在xml中数百个单例非懒加载的bean的实例化及初始化过程）解决方法：利用Spring提供的扩展点将xml中的bean设置为懒加载模式，省去了Bean的实例化与初始化时间123@RunWith(SpringJUnit4ClassRunner.class)@ContextConfiguration(locations = &#123;"a.xml","b.xml"&#125;)public abstract class AbstractLightTest &#123;&#125; 12//a.xml&lt;bean id="myBeanFactoryPostProcessor" class="com.luyu.LazyBeanFactoryProcessor"/&gt; 1234567891011public class LazyBeanFactoryProcessor implements BeanFactoryPostProcessor &#123; @Override public void postProcessBeanFactory(ConfigurableListableBeanFactory beanFactory) throws BeansException &#123; DefaultListableBeanFactory fac = (DefaultListableBeanFactory) beanFactory; Map&lt;String, AbstractBeanDefinition&gt; map = (Map&lt;String, AbstractBeanDefinition&gt;) ReflectionTestUtils.getField(fac, "beanDefinitionMap"); for (Map.Entry&lt;String, AbstractBeanDefinition&gt; entry : map.entrySet()) &#123; //设置为懒加载 entry.getValue().setLazyInit(true); &#125; &#125;&#125; 提速非常明显，原来启动测试要1min多钟，懒加载的话只要十几秒就好了Ps：这个问题建议通过xml环境隔离解决，这里只是提供一个通过Spring扩展点解决的思路 InstantiationAwareBeanPostProcessor#postProcessPropertyValues123456789Tips：在容器启动阶段，除了实例化容器，还会针对指定的xml中的bean进行注册（所谓注册就是以BeanDefination的形式放到容器的BeanDefinationMap中）流程大体可以概括为：1）解析xml中的元素 2）注册Bean对于常规的配置项&lt;bean/&gt; Spring会有默认的解析器对其进行解析并注册而非常规的配置项比如，&lt;context:component-scan base-package="com.luyu" /&gt; &lt;aop:aspectj-autoproxy/&gt; Spring也提供了与之对应的特殊解析器正是通过这些特殊的解析器才使得对应的配置项能够生效Ps：xml中的每一个配置项都有与之对应的解析器 举个例子12345@Serviceclass MyServiceImpl implements MyService &#123; @Resource private ThirdService thirdService;&#125; 我们知道当使用@Service以及@Resource注解时候，需要在xml中配置如下项1&lt;context:component-scan base-package="com.luyu" /&gt; 而针对这个特殊注解的解析器为 ComponentScanBeanDefinitionParser在这个解析器(parser)的解析方法(parse)中，注册了很多特殊的Bean，下面代码只截取了针对@Resource和@Autowire注解的bean1234567891011121314151617181920212223242526272829public BeanDefinition parse(Element element, ParserContext parserContext) &#123; //... registerComponents(parserContext.getReaderContext(), beanDefinitions, element); //... return null;&#125;public static Set&lt;BeanDefinitionHolder&gt; registerAnnotationConfigProcessors( BeanDefinitionRegistry registry, Object source) &#123; Set&lt;BeanDefinitionHolder&gt; beanDefs = new LinkedHashSet&lt;BeanDefinitionHolder&gt;(4); //... //@Autowire if (!registry.containsBeanDefinition(AUTOWIRED_ANNOTATION_PROCESSOR_BEAN_NAME)) &#123; RootBeanDefinition def = new RootBeanDefinition(AutowiredAnnotationBeanPostProcessor.class); def.setSource(source); beanDefs.add(registerPostProcessor(registry, def, AUTOWIRED_ANNOTATION_PROCESSOR_BEAN_NAME)); &#125; // Check for JSR-250 support, and if present add the CommonAnnotationBeanPostProcessor. //@Resource if (jsr250Present &amp;&amp; !registry.containsBeanDefinition(COMMON_ANNOTATION_PROCESSOR_BEAN_NAME)) &#123; //特殊的Bean RootBeanDefinition def = new RootBeanDefinition(CommonAnnotationBeanPostProcessor.class); def.setSource(source); beanDefs.add(registerPostProcessor(registry, def, COMMON_ANNOTATION_PROCESSOR_BEAN_NAME)); &#125; //... return beanDefs;&#125; 我们这里以@Resource为例，看看这个特殊的bean做了啥123456789101112131415public class CommonAnnotationBeanPostProcessor extends InitDestroyAnnotationBeanPostProcessor implements InstantiationAwareBeanPostProcessor, BeanFactoryAware, Serializable &#123; public PropertyValues postProcessPropertyValues(PropertyValues pvs, PropertyDescriptor[] pds, Object bean, String beanName) throws BeansException &#123; InjectionMetadata metadata = findResourceMetadata(beanName, bean.getClass()); try &#123; //属性注入 metadata.inject(bean, beanName, pvs); &#125; catch (Throwable ex) &#123; throw new BeanCreationException(beanName, "Injection of resource dependencies failed", ex); &#125; return pvs; &#125;&#125; 我们看到在postProcessPropertyValues方法中，进行了属性注入 invokeAware这里截取了一段Spring启动过程中的代码，可以看到包含了我们上面3~6四个步骤123456789101112131415161718protected Object initializeBean(final String beanName, final Object bean, RootBeanDefinition mbd) &#123; //③ invokeAwareMethods(beanName, bean); Object wrappedBean = bean; if (mbd == null || !mbd.isSynthetic()) &#123; //④ wrappedBean = applyBeanPostProcessorsBeforeInitialization(wrappedBean, beanName); &#125; try &#123; //⑤ invokeInitMethods(beanName, wrappedBean, mbd); &#125; if (mbd == null || !mbd.isSynthetic()) &#123; //⑥ wrappedBean = applyBeanPostProcessorsAfterInitialization(wrappedBean, beanName); &#125; return wrappedBean;&#125; 这里关注下invokeAwareMethods方法12345678private void invokeAwareMethods(final String beanName, final Object bean) &#123; if (bean instanceof Aware) &#123; //... if (bean instanceof BeanFactoryAware) &#123; ((BeanFactoryAware) bean).setBeanFactory(AbstractAutowireCapableBeanFactory.this); &#125; &#125;&#125; 举个例子：1234567@Beanclass BeanFactoryHolder implements BeanFactoryAware&#123; private static BeanFactory beanFactory; public void setBeanFactory(BeanFactory beanFactory) throws BeansException &#123; this.beanFactory = beanFactory; &#125;&#125; 实现BeanFactoryAware接口的类，会由容器执行setBeanFactory方法将当前的容器BeanFactory注入到类中 BeanPostProcessor#postProcessBeforeInitialization实现ApplicationContextAware接口的类，会由容器执行setApplicationContext方法将当前的容器applicationContext注入到类中，而该方法调用处位于ApplicationContextAwareProcessor#postProcessBeforeInitialization中123456789101112131415161718@Beanclass ApplicationContextAwareProcessor implements BeanPostProcessor &#123; private final ConfigurableApplicationContext applicationContext; public ApplicationContextAwareProcessor(ConfigurableApplicationContext applicationContext) &#123; this.applicationContext = applicationContext; &#125; @Override public Object postProcessBeforeInitialization(final Object bean, String beanName) throws BeansException &#123; //... invokeAwareInterfaces(bean); return bean; &#125; private void invokeAwareInterfaces(Object bean) &#123; if (bean instanceof ApplicationContextAware) &#123; ((ApplicationContextAware) bean).setApplicationContext(this.applicationContext); &#125; &#125;&#125; 我们看到正是在BeanPostProcessor的postProcessBeforeInitialization中进行了setApplicationContext方法的调用123456class ApplicationContextHolder implements ApplicationContextAware&#123; private static ApplicationContext applicationContext; public void setApplicationContext(ApplicationContext applicationContext) throws BeansException &#123; this.applicationContext = applicationContext; &#125;&#125; afterPropertySet() / init-method目前很多Java中间件都是基本Spring Framework搭建的，而这些中间件经常把入口放到afterPropertySet或者自定义的init中，比如大众点评开源的rpc框架pigeon入口就是配置的bean的init 方法，即ProxyBeanFactory#init（如下）Ps:https://github.com/dianping/pigeon12345678&lt;bean id="remoteService" class="com.luyu.ProxyBeanFactory" init-method="init"&gt; &lt;property name="serviceName" value="xxx" /&gt; &lt;property name="iface" value="xxx" /&gt; &lt;property name="serialize" value="xxx" /&gt; &lt;property name="callMethod" value="xxx" /&gt; &lt;property name="timeout" value="1000" /&gt;&lt;/bean&gt; 此外，值得一提的是Pigeon的ProxyBeanFactory类本身是FactoryBean接口的一个实现，而FactoryBean也是Spring提供的一个扩展点，实现该接口的类在作为bean被实例化的时候会返回该接口getObject方法返回的值123public interface FactoryBean&lt;T&gt; &#123; T getObject() throws Exception;&#125; 熟悉aop的同学应该知道，aop底层是通过动态代理实现的，不知道大家有没有思考过动态代理是如何在调用方无感知情况下替换原始对象的？这里提供Spring解决该问题的两种思路，思路1：通过覆写FactoryBean的getObject方法实现的，该方法返回对应的动态代理对象，该过程对调用方全部透明；思路2见3.6举个例子（思路1）123456789101112131415161718public interface MyService &#123; void test();&#125;public class MyServiceImpl implements MyService &#123; public void test() &#123; System.out.println("i am service test()"); &#125;&#125;public class MyAfterAdvice implements AfterReturningAdvice &#123; public void afterReturning(Object returnValue, Method method, Object[] args, Object target) throws Throwable &#123; System.out.println("after method advice"); &#125;&#125;public class MyBeforeAdvice implements MethodBeforeAdvice &#123; public void before(Method method, Object[] args, Object target) throws Throwable &#123; System.out.println("before method advice"); &#125;&#125; 123456789101112131415161718&lt;bean id="myService" class="com.techlab.spring.aop.MyServiceImpl"/&gt;&lt;bean id="beforeAdvice" class="com.techlab.spring.aop.MyBeforeAdvice"/&gt;&lt;bean id="afterAdvice" class="com.techlab.spring.aop.MyAfterAdvice"/&gt;&lt;bean id="testAop" class="org.springframework.aop.framework.ProxyFactoryBean"&gt; &lt;property name="interfaces" value="com.techlab.spring.aop.MyService"/&gt; &lt;property name="target"&gt; &lt;ref bean="myService"/&gt; &lt;/property&gt; &lt;property name="interceptorNames"&gt; &lt;list&gt; &lt;value&gt;beforeAdvice&lt;/value&gt; &lt;value&gt;afterAdvice&lt;/value&gt; &lt;/list&gt; &lt;/property&gt;&lt;/bean&gt; 12345678public class EntranceApp &#123; public static void main(String[] args) throws Exception &#123; ApplicationContext context = new ClassPathXmlApplicationContext("beans.xml"); //此时b是ProxyFactoryBean#getObject方法返回的动态代理对象 MyService b = (MyService)context.getBean("testAop"); b.test(); &#125;&#125; 控制台输出before method advicei am service test()after method advice BeanPostProcessor#postProcessAfterInitialization我们知道当配置了&lt;aop:aspectj-autoproxy/&gt;时候，默认开启aop功能，相应地调用方需要被aop织入的对象也需要替换为动态代理对象，和上文显式地通过FactoryBean偷梁换柱不同，此时通过BeanPostProcessor的postProcessAfterInitialization实现『偷梁换柱』在3.2节中提到了针对&lt;aop:aspectj-autoproxy/&gt; Spring也提供了特殊的解析器，和其他的解析器类似，在核心的parse方法中注册了特殊的bean，这里是一个BeanPostProcessor类型的bean123456789class AspectJAutoProxyBeanDefinitionParser implements BeanDefinitionParser &#123; @Override public BeanDefinition parse(Element element, ParserContext parserContext) &#123; //注册特殊的bean AopNamespaceUtils.registerAspectJAnnotationAutoProxyCreatorIfNecessary(parserContext, element); extendBeanDefinition(element, parserContext); return null; &#125;&#125; 我们看到该扩展点的方法的返回值时Object，只要将于当前bean对应的动态代理对象返回即可，该过程对调用方全部透明123456789101112public class AnnotationAwareAspectJAutoProxyCreator extends AspectJAwareAdvisorAutoProxyCreator &#123; public Object postProcessAfterInitialization(Object bean, String beanName) throws BeansException &#123; if (bean != null) &#123; Object cacheKey = getCacheKey(bean.getClass(), beanName); if (!this.earlyProxyReferences.containsKey(cacheKey)) &#123; //如果该类需要被代理，返回动态代理对象；反之，返回原对象 return wrapIfNecessary(bean, beanName, cacheKey); &#125; &#125; return bean; &#125;&#125; 正是利用Spring的这个扩展点实现了动态代理对象的替换 destroy() / destroy-methodbean生命周期的最后一个扩展点，该方法用于执行一些bean销毁前的准备工作，比如将当前bean持有的一些资源释放掉 以上です 资料推荐官网：https://docs.spring.io/spring/docs/4.3.18.RELEASE/spring-framework-reference/htmlsingle/《Spring 3.x企业应用与开发实战》《Spring揭秘》《Spring技术内幕》《Spring源码深度解析》Ps：建议阅读顺序从上至下]]></content>
      <categories>
        <category>Spring</category>
      </categories>
      <tags>
        <tag>Spring</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL核心-索引]]></title>
    <url>%2F2019%2F04%2F01%2FMySql-Index%2F</url>
    <content type="text"><![CDATA[索引的概念、数据结构、原理等 索引是啥数据库索引可以类比为一本书的目录，主要的目的就是快速的定位到目标。这需要支持快速搜索的数据结构的支持，比如哈希表、有序数组和搜索树。 索引的几种形态根据底层数据结构的不同，大致有三种类型的索引结构 1）基于哈希表的索引适用于等值查询的场景缺点：对区间查找不友好 2）有序数组：优点：利用二分查找，等值查询和范围查询性能都很好缺点：对动态插入、删除、更新不友好适用于静态数据或变动不频繁的场景 3）搜索树目前大部分数据库引擎都用的该方案，这里引出几个问题 为什么不用二叉搜索树？答：因为二叉树的高度很高，而索引会存放在磁盘上，这将导致数据查询过程需要多次访问磁盘，这会大幅降低搜索效率，所以使用多(1000+)叉树 B树和B+树的区别？答：B+树和B树相比，主要的不同点在以下3项： 内部节点中，关键字的个数与其子树的个数相同，不像B树，子树的个数总比关键字个数多1个 所有指向文件的关键字及其指针都在叶子节点中，不像B树，有的指向文件的关键字是在内部节点中。换句话说，B+树中，内部节点仅仅起到索引的作用，真正的数据都存储在叶子节点 在搜索过程中，如果查询和内部节点的关键字一致，那么搜索过程不停止，而是继续向下搜索这个分支，直到叶子节点 根据B+树的结构，我们可以发现B+树相比于B树，在文件系统，数据库系统当中，更有优势，原因如下： B+树的磁盘读写代价更低 B+树的内部结点并没有指向关键字具体信息的指针。因此其内部结点相对B树更小。如果把所有同一内部结点的关键字存放在同一盘块中,那么盘块所能容纳的关键字数量也越多。一次性读入内存中的需要查找的关键字也就越多。相对来说I/O读写次数也就降低了。 B+树的查询效率更加稳定 由于内部结点并不是最终指向文件内容的结点，而只是叶子结点中关键字的索引。所以任何关键字的查找必须走一条从根结点到叶子结点的路。所有关键字查询的路径长度相同，导致每一个数据的查询效率相当。 B+树更有利于对数据库的扫描 B树在提高了磁盘IO性能的同时并没有解决元素遍历的效率低下的问题，而B+树只需要遍历叶子节点就可以解决对全部关键字信息的扫描，所以对于数据库中频繁使用的range query，B+树有着更高的性能。 索引数据结构InnoDB与MyISAMMyISAM使用的是B+Tree，叶节点存放的数据是记录的地址，即一份映射关系。主键索引树和非主键索引树结构上没区别 InnoDB使用的也是B+Tree，和MyISAM相比最大的区别是他的数据文件本身就是索引文件，而MyISAM索引文件和数据文件是分离的。非主键树叶子节点存放的是主键的值，而主键树叶子节点存放的是整条记录。因此InnoDB要求表必须要有主键，如果未配置那么会默认生成一个主键。同样的如果重建主键索引（先drop再add），相当于整个表都要重建（包括其他非聚合索引树） 延伸：两种引擎最大的区别就是InnoDB支持事务处理、外键和行级锁；MyISAM强调的是性能，更多适用读的场景。 页分裂与优化 B+树索引的维护 新插入数据，要在主键树节点（存放在Page中：类似数组）中插入 插入尾部无影响（追加操作；自增主键就是这样做的） 插入中间需要移位 如果Page满了，还需要进行页分裂，要申请一个新的数据页并移动数据，性能很差 页分裂 分裂后，两个page数据各占50% 会导致空间利用率下降，优点是适合于随机插入的场景 目前InnoDB的优化策略（0%分裂策略） 页满了之后，申请新的空间并不移动老数据 针对递增递减场景很合适，如果随机插入反而不如50%策略 InnoDB为每个索引页维护了一个上次插入的位置，以及上次的单调情况 根据这些信息判断插入到页面的记录是否满足单调条件，如果满足那么执行0%分裂，否则执行50%分裂 这样会有bug，3,4,5,6在一个块且递增，接下来插入9-&gt;8-&gt;7都会执行0%策略导致页利用率极低 优化策略，分裂时采用『1策略』，至少带着最后一位一起分裂 因删除操作而导致的页合并 为什么经常要求table有一个自增主键？ 如果是业务主键无法保证有序递增，这可能会导致移位（50%分裂） 降低空间利用率 在其他非主键索引构成的树中，叶子存放的是主键，所以使用自增主键占用的空间更小 主键长度越小，普通索引的叶子节点就越小，普通索引占用的空间就越小 联合索引的原理key ‘Index_Uni’ (‘name’,’age’) 假设根据name和age两个字段建立了联合索引，那么这颗树叶子节点应该是这样的(‘luyu’,10) (‘luyu’,11) (‘luyu’,12) (‘mike’,10) (‘mike’,12) …当我们执行如下查询1select * from table where name = 'luyu' and age = 12; 此时第一步会根据二分查找定位到’luyu’Ps这里有个猜测，因为找到’luyu’之后如果要找age=12的记录采用遍历的话时间复杂度较高，所以找到’luyu’应该是第一个和最后一个，然后再根据age在[10,12]这里进行二分查找 联合索引实例只有待查询的数据在当前范围内有序才能使用索引 a/b/c联合索引：select a from table where a = 10 and b &lt;= 20 and c &gt; 20a b会生效select a from table where a &lt; 10 and b &lt;= 20 and c &gt; 20a会生效 name/age联合索引：select * from table1 where name like ‘张%’ and age = 12;只有name会生效 为什么会有最左匹配看了联合索引树叶子节点的排列形式应该就清楚为什么要最左匹配了因为如果最左都不匹配根本无法获取当前列的有序数据，更别提使用算法去查找了 索引覆盖为了避免回表（额外访问1次主键树）比如 select Id from t where k = 1 此时Id已经存在叶子节点了 无需回表 select v from t where k = 1 如果存在 k v的联合索引，此时也无需回表查询这也是常见的一种优化手段 索引下推key ‘Index_Uni’ (‘name’,’age’)select * from tuser where name like ‘张%’ and age=10;此时只有name的索引会生效，MySQL5.6之后采用了优化策略，在定位到’张%’后，无需立即回表查询对应age=10的记录，而是可以先在索引树上遍历age=10的记录再回表查询。 数据库索引的查询算法t1(a int primary key, b int key)1select * from t1 where b &gt;= 4; 上面sql等价于查找第一个b=4的位置 1select * from t1 where b &lt;= 4; 上面sql等价于查找最后一个b=4的位置 后续会单独写一篇二分查找的文章解决类似的问题 一条SQL语句慢的原因偶尔慢1.数据库在刷页，redo log和内存中的事务刷到磁盘中2.拿不到锁，block住 日常慢1.没有索引2.有索引但是没用到 2.1 索引field函数操作/不符合最左匹配 2.2 数据库选错索引 有时候即使一个字段上有索引也不会走，数据库会判断走索引和全表扫描的优劣，判断是通过采样来决定的（这意味着有可能判断错误），此时可以使用force index(xx)]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>MySQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring容器优雅关闭]]></title>
    <url>%2F2019%2F03%2F27%2FJVM-Close%2F</url>
    <content type="text"><![CDATA[由一个线上报错引发的关于Spring容器优雅关闭的探究 背景内部一个用于任务调度的项目，依赖该项目的工程在发布部署时候经常会有这个报错12java.lang.IllegalStateException: BeanFactory not initialized or already closed call 'refresh' before accessing beans via the ApplicationContext 经排查问题的原因是工程启动后会开启一个daemon线程不断轮询db中的任务并执行，执行过程中会调用ApplicationContext的getBean()方法，而当项目重新部署时会先停掉tomcat，这时候触发了Spring容器的关闭，而后台线程还在调用getBean方法，此时就会提示这个错误。 解决方法Listener基于Spring FrameWork的web项目，在web.xml中都会配置一个Spring相关的Listener123&lt;listener&gt; &lt;listener-class&gt;org.springframework.web.context.ContextLoaderListener&lt;/listener-class&gt;&lt;/listener&gt; 这个Listener提供了很多回调的方法，eg：contextInitialized/contextDestroyed前者执行了Spring容器的启动过程（refresh()）；后者执行了容器的销毁工作，主要包括销毁所有bean（单例）以及关闭容器(BeanFactory)。在销毁Bean的过程中执行了，每个bean注册的destroy()方法(如果注册了的话)。解决方法比较简单，配置一个自定义的Listener，放到上面这个Listener的后面，值得一提的是Listener的contextInitialized顺序和contextDestroyed的执行顺序刚好相反，前者和配置的上下顺序一致，后者反之。但是，本工程是以jar包的形式供业务使用，采用本方案需要业务方自行在web.xml中配置Listener，这对业务的侵入性较大，于是否定了该方案。 ShutDownHook在项目工程启动时候注册一个钩子方法，方法中执行调度的关闭123456Runtime.getRuntime().addShutdownHook(new Thread(new Runnable() &#123; @Override public void run() &#123; stopScheduler(); &#125; &#125;)); 最开始用的该方法，后来发现可能有问题。我们发现Tomcat启动时候也会注册一个hook方法，代码如下123456789public void start() &#123;//....if (useShutdownHook) &#123;if (shutdownHook == null) &#123; shutdownHook = new CatalinaShutdownHook();&#125;Runtime.getRuntime().addShutdownHook(shutdownHook);//....&#125; 123456789101112131415161718protected class CatalinaShutdownHook extends Thread &#123; public void run() &#123; try &#123; if (getServer() != null) &#123; Catalina.this.stop(); &#125; &#125; catch (Throwable ex) &#123; log.error(sm.getString("catalina.shutdownHookFail"), ex); &#125; finally &#123; // If JULI is used, shut JULI down *after* the server shuts down // so log messages aren't lost LogManager logManager = LogManager.getLogManager(); if (logManager instanceof ClassLoaderLogManager) &#123; ((ClassLoaderLogManager) logManager).shutdown(); &#125; &#125; &#125;&#125; 上面代码中的Catalina.this.stop()会触发tomcat一系列组件的关闭，包括StandardWrapper，而该组件会触发Servlet的关闭，进而被特定的Listener监听到执行Spring的销毁工作。 关于hook java doc中这样描述的12When the virtual machine begins its shutdown sequence it will start all registered shutdown hooks in some unspecified order and let them run concurrently. 这意味着hook之间是并行执行的，所以通过该方法并不能完全保证，先关闭任务调度再关闭Spring容器，所以排除了该方法。 Bean#destroy()该方案利用了Spring在销毁BeanFactory之前会销毁所有单例的bean这一特性。编写自定义的bean，在destroy方法中实现关闭调度任务的逻辑。该方法可行，但需要配置一个和业务无关的bean，有些不够优雅。 ApplicationListener我们先看看ApplicationContext的关闭方法12345678910111213protected void doClose() &#123; // Publish shutdown event. publishEvent(new ContextClosedEvent(this)); // Destroy all cached singletons in the context's BeanFactory. destroyBeans(); // Close the state of this context itself. closeBeanFactory(); // Let subclasses do some final clean-up if they wish... onClose(); synchronized (this.activeMonitor) &#123; this.active = false; &#125;&#125; 我们看到在销毁Beans以及关闭BeanFactory之前会发布容器关闭的事件。我们可以在这里去自定义监听器，去实现停止调度的逻辑。123456789@Servicepublic class MyListener implements ApplicationListener &#123; @Override public void onApplicationEvent(ApplicationEvent event) &#123; if(event instanceof ContextClosedEvent) &#123; stopScheduler(); &#125; &#125;&#125; 最终采用该方案，利用Application内置的监听机制，在容器关闭前执行一些业务的清理工作。]]></content>
      <categories>
        <category>Spring</category>
      </categories>
      <tags>
        <tag>Spring</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring bean循环依赖]]></title>
    <url>%2F2019%2F02%2F16%2FSpring-bean-circular-reference%2F</url>
    <content type="text"><![CDATA[1.两种bean的循环依赖形式2.通过源码分析Spring的处理方式 两种循环依赖其实就是Spring中bean的两种注入方式，一种是构造器注入，一种是属性注入 构造器注入:123public class A &#123; public A(B b) &#123;&#125;&#125; 123public class B &#123; public B(A b) &#123;&#125;&#125; 1234567&lt;bean id="testA" class="com.spring.A"&gt; &lt;constructor-arg ref="testB"/&gt;&lt;/bean&gt;&lt;bean id="testB" class="com.spring.B"&gt; &lt;constructor-arg ref="testA"/&gt;&lt;/bean&gt; 此时容器启动会报如下错误123org.springframework.beans.factory.BeanCurrentlyInCreationException: &lt;br&gt;Error creating bean with name 'testA': Requested bean is currently in creation: &lt;br&gt;Is there an unresolvable circular reference? 属性注入:123456public class C &#123; D d; public void setD(D d) &#123; this.d = d; &#125;&#125; 123456public class D &#123; C c; public void setC(C c) &#123; this.c = c; &#125;&#125; 1234567&lt;bean id="testC" class="com.spring.C"&gt; &lt;property name="d" ref="testD"/&gt;&lt;/bean&gt;&lt;bean id="testD" class="com.spring.D"&gt; &lt;property name="c" ref="testC"/&gt;&lt;/bean&gt; 此时容器可以正常启动，不会报错 小结：这里我们先抛出结论，Spring Bean的循环依赖分为构造器和属性两种，其中前者会报错而后者不会报错，下面我们通过源码具体看看Spring是怎么处理这两种情况的 构造器注入源码分析因为之前已经分析过Spring bean构造的全过程了Spring源码阅读(2)-IOC之bean的实例化.所以接下来不会逐步分析每个细节，只会对之前文章中未提及的一些点进行剖析。这里使用了idea的debug来查看调用栈显然，容器会先实例化testA这个bean，而testA又依赖了testB，我们利用条件断点定位到获取testB的位置，观察下调用栈从下向上看，大部分内容我们之前都已经分析过了，之前阅读ioc源码主要看的是一般情况，一些特殊情况和细节当时并未关注。所以之前对于bean的实例化只是看了默认的无参数构造函数的实例化，对于这种构造器注入的方式并未关注，那接下来就从autowireConstructor这里开始。12345678910111213141516171819202122232425public BeanWrapper autowireConstructor( final String beanName, final RootBeanDefinition mbd, Constructor&lt;?&gt;[] chosenCtors, final Object[] explicitArgs) &#123; //... ConstructorArgumentValues cargs = mbd.getConstructorArgumentValues(); resolvedValues = new ConstructorArgumentValues(); //在这个方法中对bean进行了解析，在本例中就是对testB进行了解析 minNrOfArgs = resolveConstructorArguments(beanName, mbd, bw, cargs, resolvedValues);&#125;private int resolveConstructorArguments( String beanName, RootBeanDefinition mbd, BeanWrapper bw, ConstructorArgumentValues cargs, ConstructorArgumentValues resolvedValues) &#123; //... Object resolvedValue = valueResolver.resolveValueIfNecessary("constructor argument", valueHolder.getValue()); //...&#125;public Object resolveValueIfNecessary(Object argName, Object value) &#123; //... return resolveReference(argName, ref);&#125;private Object resolveReference(Object argName, RuntimeBeanReference ref) &#123; //终于抓到了，也就是在这里进行了对testB这个bean的获取 Object bean = this.beanFactory.getBean(refName); this.beanFactory.registerDependentBean(refName, this.beanName); return bean;&#125; 好我们已经看到了在对testA实例化过程中，触发了对testB的实例化，同样地testB也会触发testA的实例化，那么Spring会如何处理呢？我们继续debug仿佛是一个轮回我们看到果然又一次触发了testA的实例化，代码来到了doGetBean中的getSingleton方法中12345678public Object getSingleton(String beanName, ObjectFactory&lt;?&gt; singletonFactory) &#123; //... beforeSingletonCreation(beanName); //执行createBean(beanName, mbd, args); singletonObject = singletonFactory.getObject(); afterSingletonCreation(beanName); //...&#125; 我们看看beforeSingletonCreation(beanName)这个方法123456protected void beforeSingletonCreation(String beanName) &#123; if (!this.inCreationCheckExclusions.contains(beanName) &amp;&amp; !this.singletonsCurrentlyInCreation.add(beanName)) &#123; throw new BeanCurrentlyInCreationException(beanName); &#125;&#125; 这里主要看this.singletonsCurrentlyInCreation.add(beanName)。我们知道上述代码是每个bean实例化前都要执行的，也就是说testA也执行过（从调用栈也能看到最开始的getSingleton方法），这意味着singletonsCurrentlyInCreation这个set中有testA。那么当testB通过构造器注入时触发testA的实例化过程后，testA会再次执行这段代码，而这个时候set中已经存有一份testA了，此时add操作会返回false，进而执行throw new BeanCurrentlyInCreationException(beanName)于是会将错误抛出。 显然在完成实例化后，afterSingletonCreation(String beanName)会将beanName从set中移除123456protected void afterSingletonCreation(String beanName) &#123; if (!this.inCreationCheckExclusions.contains(beanName) &amp;&amp; !this.singletonsCurrentlyInCreation.remove(beanName)) &#123; throw new IllegalStateException("Singleton '" + beanName + "' isn't currently in creation"); &#125;&#125; 至此，构造器形式的循环依赖已经分析完毕。 小结：1)当testA在实例化前会先将自己的beanName存放到一个正在实例化的beanName的set中2)testA实例化过程触发了testB的实例化3)testB也将自己加入正在实例化的beanName的set中4)testB实例化又触发了testA的实例化5)而这个时候再将testA加入set中会返回false，意味着产生了循环依赖6)于是抛出Exception 属性注入源码分析既然是循环依赖，显然C实例化过程也会触发D的实例化我们看看这时候的调用栈我们看到此时testC已经执行到了applyPropertyValues这里，而这里是执行属性注入的地方，这意味着testC已经完成了实例化+属性注入。这里还有一些没有展示出来的细节我们关注下在doCreateBean方法中执行了addSingletonFactory12345678910protected void addSingletonFactory(String beanName, ObjectFactory&lt;?&gt; singletonFactory) &#123; Assert.notNull(singletonFactory, "Singleton factory must not be null"); synchronized (this.singletonObjects) &#123; if (!this.singletonObjects.containsKey(beanName)) &#123; this.singletonFactories.put(beanName, singletonFactory); this.earlySingletonObjects.remove(beanName); this.registeredSingletons.add(beanName); &#125; &#125;&#125; 注意此时还未执行bean的属性注入，我们看到这里将bean存放到了singletonFactories中，此时的bean仅仅是一个空壳（其属性并未完成填充）显然testD实例化过程也会触发testC的实例化，我们debug也看到了确实是这样接着代码进入到了doGetBean中，首先会从容器中看看是否已经有了对应bean的缓存123456789101112131415161718protected &lt;T&gt; T doGetBean( final String name, final Class&lt;T&gt; requiredType, final Object[] args, boolean typeCheckOnly) throws BeansException &#123; //... //对于构造器循环依赖，执行到这里返回值都是null Object sharedInstance = getSingleton(beanName); //... //注意区分这两个getSingleton，前者单纯地从缓存中捞取数据 //而后者主要目的是执行createBean操作 if (mbd.isSingleton()) &#123; sharedInstance = getSingleton(beanName, new ObjectFactory&lt;Object&gt;() &#123; public Object getObject() throws BeansException &#123; return createBean(beanName, mbd, args); &#125; &#125;); bean = getObjectForBeanInstance(sharedInstance, name, beanName, mbd); &#125; &#125; 这个方法注意和构造器依赖中的方法做区分，二者都在doGetBean中被调用123456789101112131415161718192021public Object getSingleton(String beanName) &#123; return getSingleton(beanName, true);&#125;protected Object getSingleton(String beanName, boolean allowEarlyReference) &#123; Object singletonObject = this.singletonObjects.get(beanName); if (singletonObject == null &amp;&amp; isSingletonCurrentlyInCreation(beanName)) &#123; synchronized (this.singletonObjects) &#123; singletonObject = this.earlySingletonObjects.get(beanName); if (singletonObject == null &amp;&amp; allowEarlyReference) &#123; ObjectFactory&lt;?&gt; singletonFactory = this.singletonFactories.get(beanName); if (singletonFactory != null) &#123; singletonObject = singletonFactory.getObject(); this.earlySingletonObjects.put(beanName, singletonObject); this.singletonFactories.remove(beanName); &#125; &#125; &#125; &#125; return (singletonObject != NULL_OBJECT ? singletonObject : null);&#125; 我们看到上面这个方法，首先从singletonObjects中查找，而singletonObjects存放的是已经彻底完成实例化的bean；接着从earlySingletonObjects中获取，如果再获取不到就去singletonFactories中获取，获取到后会将bean存放到earlySingletonObjects中并将其从singletonFactories中移除。 testC-&gt;testD-&gt;testC 当第二次触发testC时候执行getSingleton时候，就会在singletonFactories中获取到并未填充属性的testC的实例，从而完成testD的实例化与属性注入接着完成testC的注入。 小结：属性注入的循环依赖的解决其实是基于Java的引用传递，bean完成实例化后会将自己的引用暴露出来以供其他bean注入使用 总结：1）一个bean在实例化之前会将自己存放到名为singletonsCurrentlyInCreation的set中表征自己正处于创建状态2）如果set执行add返回false表征发生了构造器类型的循环依赖，此时会抛出错误3）在实例化之后会将自己从singletonsCurrentlyInCreation中移除4）一个bean在实例化后属性注入前会将自己存放到名为singletonFactories的map中5）当bean2触发bean1实例化时，会先去singletonObjects中获取bean1，获取不到会去earlySingletonObjects中获取bean1，再获取不到就会去singletonFactories中获取。如果在singletonFactories中获取到，就将bean1从singletonFactories中移除，并存放到earlySingletonObjects中 以上です]]></content>
      <categories>
        <category>Spring</category>
      </categories>
      <tags>
        <tag>Spring</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL核心-事务、锁以及日志]]></title>
    <url>%2F2019%2F02%2F14%2FInnoDB-RR%2F</url>
    <content type="text"><![CDATA[MySQL核心-事务、锁以及日志 锁分类粒度划分：表锁、行锁、范围锁（GAP）锁的模式：共享、排他、意向 1.表/行（S锁、X锁）S:读锁X:写锁 2.意向锁（IS锁、IX锁）意向锁是表（共享锁、排它锁）和行（共享锁、排它锁）之间的桥梁表级别的锁一个事务A给某张表加了一个意向锁IX，暗示接下来要对该表的一行数据加行X锁，这样事务B如果要给整张表加X锁就不用遍历每一行查看是否能加X锁了。反过来，一个事务如果要给某张表的某一行加S锁，那么需要先获得整张表的IS（X同理）IS和表级别的S不互斥，IX和表级别的X互斥。 3.AI自增锁-&gt;表级别保证同一个事务连续的insert操作的自增列（一般id）连续 4.Next-Key锁(临键锁/下文有详细介绍) 事务分类这里重点强调下更新丢失 第一类更新丢失执行时间重合的两个事务，均更新了某条记录的field1，第一个事务更新成功并commit，第二个事务却更新失败并执行回滚这时候如果没有任何处理措施，会将第一个事务的更新覆盖。这种情况称为第一类更新丢失，目前这四个隔离级别都没有这个问题了，具体解决方式不清楚，后续查下资料。 第二类更新丢失就是业务上常见的场景，同样执行时间重合的两个事务，均更新了某条记录的fiedl1（均是先读取再更新），事务1和事务2读取到了当前db中记录的值，接着事务1对它进行了+1并commit，而事务2进行了-1并commit，这时候因为事务2读取的是旧值而将事务1的更新覆盖了。 针对第二类更新丢失常见的解决方法如下 方法1：单条语句的原子性update T set account = account + 1 where userId = 1 Ps：update Ticket set count = count - 1 where ticketId = 1 and count &gt; 0 方法2：悲观锁1234start transactionint b = select account from T where userId = 1 for updateb = b + 1update T set account = b where userId = 1 1234start transactionint b = select account from T where userId = 1 for updateb = b - 1update T set account = b where userId = 1 方法3：乐观锁一般和while循环一起使用1234start transactionint b,v1 = select account, version from T where userId = 1 b = b + 1update T set account = b, version = v1 + 1 where userId = 1 and version = v1; 1234start transactionint b,v1 = select account, version from T where userId = 1 b = b - 1update T set account = b, version = v1 + 1 where userId = 1 and version = v1; 快照读RR隔离级别，事务启动时会创建一个整个库的视图Ps RC隔离级别，每条sql单独创建一个视图 辅助知识点 db中每行数据都有多个版本，每个版本有自己的row trx_id（对其修改的事务id） 具体的实现方式 1.事务启动时，会维持一个数组，用来保存启动瞬间，当前『活跃』的事务ID 2.小于数组中最低ID的数据都是可见的；大于数组中最大ID的数据都是不可见的 3.大于最低ID&amp;小于最高ID的数据，如果位于数组中那么是不可见的；反之可见 MVCC基于快照读，保证读读不加锁，读写不加锁，极大提升了性能 类似JUC里的CopyOnWriteList 当前读update/delete/select for update/select lock in share mode都属于当前读前3个语句会对行加X锁，最后一个加S锁Ps:如果存在unique index那么insert时候也会加锁 幻读条件 插入 当前读 本质：无法对未来插入的符合加锁条件的行加锁，从而产生一些『不可控的状况』 举例Ps：假设此时并未引入Next-Key Lock(其实就是RC隔离级别的做法)table(id key, name) id name 1 mike 5 luyu 10 jack session1 session2 T1 begin;select * from table where id=5 for update;update table set id=2 where id=5; T2 insert into table values(5, john);update table set name=’mary’ where name = ‘john’; T3 commit; 问题 语义上：session1的select语句表示要将id=5的记录都上锁，不准其他事务进行读写；但是session2中对id=5的记录进行了update操作 一致性：上述情况执行完毕binlog会如下记载 123insert into table values(5, john);update table set name='jack' where name = 'john';update table set id=2 where id=5; 如果这个binlog拿去从库执行，就会出现主从数据的不一致 如何解决幻读Next-Key锁归根结底，问题出现的原因就是，session2中insert语句的执行此时需要引入next-key lock(RR默认)，对可能执行id=5插入的区间加锁，在上面例子中RR级别下id位于[1,10)的记录的insert操作都将会block 加锁举例RR隔离级别table(id key, name) id name 1 mike 5 luyu 10 jack select * from table where id = 5 for update; 表中横轴代表id是哪种类型的索引，纵轴代表sql语句执行的结果（范围锁一般只针对非唯一索引） 索引数据结构的演进从二叉树到B树B树其实就是多叉树，当数据量较大时二叉树的深度过大，而索引可能是存储在磁盘上的，这可能会导致某个具体数据的定位过程耗时较长，为此引入了B树（多叉树） 从B树到B+树相比B树，B+树只有叶子节点才存储具体的数据，这意味着非叶子节点可以存储更多的索引意味着树的高度会相应降低；而且因为所有的data都存储在叶子节点，range查询变得更为方便 InnoDB VS MyISAMInnodb与MyISAM的索引都采用B+树，二者的区别主要如下 InnoDB的数据文件本身就是索引文件;MyISAM索引文件和数据文件是分离的，索引文件仅保存数据记录的地址 InnoDB的辅助索引data域存储相应记录主键的值而不是地址 MySQL logredo log前滚日志、物理日志记录数据页的修改事务1，执行了多个update操作，需要修改不同的磁盘Page，如果直接去修改磁盘那么性能简直爆炸。为此不直接写磁盘，而是在内存中进行事务提交，然后再通过后台线程异步地把内存中的数据写到磁盘中。but，如果宕机，会出现数据丢失问题为此，我们在提交到内存后-》会去写日志（Write-Ahead log）-》后台线程异步地把内存中的数据写到磁盘中如果宕机的话可以通过日志降低数据丢失量 InnoDB引擎的Write-Ahead log就是Redo Log。而Redo log的刷盘也是异步的（Redo buffer 到 Redo log），可以配置三种刷盘策略1）秒级别 /s2）事务级别 /transaction 一般使用该策略 几乎不会丢失数据 但是性能较差3）不刷盘（指定时间） log刷盘一般是刷到同一块盘，性能比刷内存提交要好，这就是redo log存在的意义 undo log回滚日志、逻辑日志、记录行记录的变更本质：『备份数据』，事务未提交之前的时间里的备份数据，提交事务后，如果没有其他事务引用历史版本，就可以删除了如下图，当T100（事务100）完成时候就可以删除V95了。UNDO LOG是MVCC的基础，和JUC中的CopuOnWrite思想很像，实现了读读不加锁，读写不冲突。RR隔离级别下，事务启动瞬时会去寻找小于当前事务id的最新的数据视图，当事务修改数据后会将新的数据放到链表尾部显然，当比较耗时的事务执行过程中会使得很多数据的老版本无法删除（这条链可能会很长） bin log位于Server层，所有引擎通用逻辑日志，常用于归档bin log和redo log是二阶段提交 为啥需要二阶段提交？1）假如（执行了一条update后）先写redo再写binlog，写binlog时MySQL crash了，进程恢复后可以根据redo log恢复上次的update操作 但是binlog中没有记录这次变更，如果拿这个binlog去备库执行，那么会丢失数据 2）先写bin log再写redo log，同样crash崩溃恢复后会丢失本次更新，但是bin log已经记录，再拿去备库执行，备库会比主库多一次数据变更 和redo log的异同？1）redo时InnoDB引擎特有；binlog时MySQL的Server层实现的，所有引擎都可以适用2）redo是物理日志，记录数据页变更；binlog是逻辑日志，记录的是语句的逻辑，比如更新id=2的记录 参考文章：http://blog.jobbole.com/24006/http://hedengcheng.com/?p=771#_%E7%BB%84%E5%90%88%E4%B8%80%EF%BC%9Aid%E4%B8%BB%E9%94%AE+RC]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>数据库</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[01-Bag]]></title>
    <url>%2F2019%2F01%2F15%2F01-Bag%2F</url>
    <content type="text"><![CDATA[动态规划小试牛刀：01背包问题汇总 物品数目为1，一定承载能力的背包最多装多少kg？12345678910111213141516171819202122232425262728293031323334353637static int max;static void maxWeight(int[] arr, int target, int i, int cur) &#123; //end condition if (cur &gt; target) &#123; return; &#125; max = Math.max(cur, max); if (i &gt;= arr.length || max == target) &#123; return; &#125; //recur maxWeight(arr, target, i + 1, cur); maxWeight(arr, target, i + 1, cur + arr[i]);&#125;static int dp(int[] arr, int target) &#123; boolean[][] dp = new boolean[arr.length][target + 1]; for (int i = 0; i &lt; arr.length; i++) &#123; for (int j = 1; j &lt; target + 1; j++) &#123; if (i &gt; 0 &amp;&amp; dp[i - 1][j]) &#123; dp[i][j] = true; if (j + arr[i] &lt;= target) &#123; dp[i][j + arr[i]] = true; &#125; &#125; //由于物品数量为1 所以下面的逻辑要放到递推之后 if (j == arr[i]) &#123; dp[i][j] = true; &#125; &#125; &#125; for (int k = target; k &gt;= 0; k--) &#123; if (dp[arr.length - 1][k]) return k; &#125; return -1;&#125; 物品数目为1，装满背包最少需要几件物品？12345678910111213141516171819202122232425262728293031323334353637383940static int min = Integer.MAX_VALUE;static void recur(int[] arr, int target, int i, int cur, int count) &#123; //end if (cur == target) &#123; min = Math.min(min, count); return; &#125; if (i == arr.length || cur &gt; target) &#123; return; &#125; //recur recur(arr, target, i + 1, cur, count); recur(arr, target, i + 1, cur + arr[i], count + 1);&#125;static int dp(int[] arr, int target) &#123; int n = arr.length; int[][] dp = new int[n][target + 1]; for (int i = 0; i &lt; n; i++) &#123; for (int j = 1; j &lt;= target; j++) &#123; if (i &gt; 0 &amp;&amp; dp[i - 1][j] &gt; 0) &#123; dp[i][j] = Math.min(dp[i - 1][j], dp[i][j] &gt; 0 ? dp[i][j] : Integer.MAX_VALUE); if (j + arr[i] &lt;= target) &#123; dp[i][j + arr[i]] = dp[i][j] + 1; &#125; &#125; //由于物品数量为1 所以下面的逻辑要放到递推之后 if (j == arr[i]) &#123; dp[i][j] = 1; &#125; &#125; &#125; int min = Integer.MAX_VALUE; for (int k = arr.length - 1; k &gt;= 0; k--) &#123; if (dp[k][target] &gt; 0) min = Math.min(dp[k][target], min); &#125; return min;&#125; 物品数目不限，一定承载能力的背包最多装多少kg？1234567891011121314151617181920212223242526272829303132static int max = Integer.MIN_VALUE;static void recur(int[] arr, int target, int i, int cur) &#123; //end if(cur &gt; target) &#123; return; &#125; max = Math.max(max, cur); if(i == arr.length) return; //recur recur(arr, target, i+1, cur); recur(arr, target, i, cur+arr[i]);&#125;static int dp(int[] arr, int target) &#123; boolean[][] dp = new boolean[arr.length][target+1]; for(int i = 0; i &lt; arr.length; i++) &#123; dp[i][arr[i]] = true; for(int j = 0; j &lt;= target; j++) &#123; if(i &gt; 0 &amp;&amp; dp[i-1][j]) &#123; dp[i][j] = true; &#125; if(dp[i][j] &amp;&amp; j+arr[i] &lt;= target) &#123; dp[i][j+arr[i]] = true; &#125; &#125; &#125; for(int k = target; k &gt;= 0; k--) &#123; if(dp[arr.length-1][k]) return k; &#125; return -1;&#125; 物品数目不限，一定承载能力的背包装满需要最少物品的数量？1234567891011121314151617181920212223242526272829303132333435363738static int min = Integer.MAX_VALUE;static void recur(int[] arr, int target, int i, int cur, int count) &#123; //end if(cur == target) &#123; min = Math.min(min, count); return; &#125; if(cur &gt; target || i==arr.length) &#123; return; &#125; //recur recur(arr, target, i+1, cur, count); recur(arr, target, i, cur+arr[i], count+1);&#125;static int dp(int[] arr, int target) &#123; int[][] dp = new int[arr.length][target+1]; for (int i = 0; i &lt; arr.length; i++) &#123; dp[i][arr[i]] = 1; for (int j = 1; j &lt;= target; j++) &#123; //from top if(i &gt; 0 &amp;&amp; dp[i-1][j] &gt; 0) &#123; dp[i][j] = Math.min(dp[i-1][j], dp[i][j] != 0 ? dp[i][j] : Integer.MAX_VALUE); &#125; //from left if(dp[i][j] &gt; 0 &amp;&amp; j+arr[i] &lt;= target) &#123; dp[i][j+arr[i]] = Math.min(dp[i][j] + 1, dp[i][j+arr[i]] &gt; 0 ? dp[i][j+arr[i]] : Integer.MAX_VALUE); &#125; &#125; &#125; int min = Integer.MAX_VALUE; for(int k = arr.length-1; k &gt;= 0; k--) &#123; min = Math.min(min, dp[k][target]); &#125; return min;&#125; 物品数目不限，装满背包有多少方案？123456789101112131415161718192021222324252627282930313233static int sum;static void recur(int[]arr, int target, int i, int cur) &#123; //end if(target == cur) &#123; sum ++; return; &#125; if(cur &gt; target || i==arr.length) &#123; return; &#125; //recur recur(arr, target, i+1, cur); recur(arr, target, i, cur+arr[i]);&#125;static int dp(int[]arr, int target) &#123; int[][] dp = new int[arr.length][target+1]; for (int i = 0; i &lt; arr.length; i++) &#123; dp[i][arr[i]] = 1; for (int j = 1; j &lt;= target; j++) &#123; //from top if(i &gt; 0 &amp;&amp; dp[i-1][j] &gt; 0) &#123; dp[i][j] += dp[i-1][j]; &#125; //from left if(dp[i][j] &gt; 0 &amp;&amp; j+arr[i] &lt;= target) &#123; dp[i][j+arr[i]] += dp[i][j]; &#125; &#125; &#125; return dp[arr.length-1][target];&#125; Tips: 01背包，回溯方法都相对简单，对于是否可以重复拿区别只在回溯时候i是否加1 01背包 优先采用dp，一般地将物品的重量作为行，0~背包重量作为列 dp解法中，如果求最小值一般用int[][]，求最大值一般用boolean[][] 想不清楚的时候，先画图去递推 优雅简洁的代码，写起来也很顺手，思路也比较清楚]]></content>
      <categories>
        <category>Algorithm</category>
      </categories>
      <tags>
        <tag>01背包</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[网络协议(2)]]></title>
    <url>%2F2018%2F12%2F20%2Fnetwork-protocal%2F</url>
    <content type="text"><![CDATA[看了刘超大佬的课程记的一些笔记 四层协议 服务器收到数据包后，层层拆包（很多工作是内核完成的），直到传输层，获取当前的端口号，将数据包发往监听当前端口号的应用程序(可能是tomcat(8080)也可能是浏览器(80)) 只要是在网络上跑的包，都是完整的。可以有下层没上层，绝对不可能有上层没下层。所以对于TCP协议来说，三次握手/重试 只要想发出包，就要经过IP层和MAC层 一个在网络传递的包如下所示 IP地址ip地址类比大学宿舍 mac地址类比身份证idip分为公有ip 和 私有ip 一般公司都会有一个统一的公有ip 和 许多私有ip同一个局域网内的设备ip地址不能相同，不然ARP根本找不到目标机器 CIDR无类型域间选路（CIDR）网络号 + 主机号 10.100.122.2/24 前24位是网络号 后8位是主机号广播地址 10.100.122.255 发送到这个地址所有的10.100.122.x都能收到子网掩码 255.255.255.0子网掩码和IP地址执行AND 就可以得到网络号 DHCP自动分配ip-DHCP协议（池化思想，用后即还） 新机器加入网络，以0.0.0.0为ip地址（具有本机的mac地址），向255.255.255.255发送广播包 DHCP Server会收到广播包，并且将分配给新人的ip一并返回给新人（此时目标ip还是255.255.255.255） 再次发送广播包，ip和第一次发的一样，内容是接收哪一个DHCP Server分配的ip DHCP Server再次发送和2步骤类似的包，表示ACK 二层设备二层设备（mac）：解决多路访问的堵车问题 局域网LAN 集线器hub（广播模式），通过mac地址解决『广播』问题，此时仍然广播但是由各个主机判断是否接收当前消息；通过信道划分解决消息混乱无序问题 交换机：相比hub，它知道每个口对应的mac地址，它会有记忆功能（凡是它转发过的都会牢记），一段时间后，就能知道整个网络的结构了，此时可以精准转发不再需要广播（注意交换机最开始还是广播的，它是逐渐变得聪明的） VLAN 虚拟局域网 在二层头部新增一个TAG 里面有VLAN ID 支持VLAN的交换机可以连接多个不同LAN的机器，VLAN交换机间通过Trunk口连接 交换机环路问题，两个交换机左侧是相同的lan 右侧也是相同的lan STP：解决环路问题，将有环路的图变成无环路的树 ARP：已知IP地址求MAC地址 基本靠吼 三层设备 三层设备：ip 路由器 ICMP：ping指令基于该协议，ICMP报文封装在IP包中 最重要的两个参数，类型（主动请求是8 主动请求的应答是0 还有很多差错报文类型表示不同的问题）和序号 一个电脑有多个网卡就可以充当路由器，一个网卡连接外网网口，一个网卡连到交换机 网关地址一定和源ip地址在同一个网段 网关往往是一个路由器，是一个三层转发设备（三层设备：拆下mac头和ip头接下来进行转发） 路由器是一台设备，它有五个网口或者网卡，相当于有五只手，分别连着五个局域网。每只手的 IP 地址都和局域网的IP地址相同的网段，每只手都是它握住的那个局域网的网关。任何一个想发往其他局域网的包，都会到达其中一只手，被拿进来，拿下 MAC 头和 IP 头，看看，根据自己的路由算法，选择另一只手，加上 IP 头和 MAC 头，然后扔出去。（可以在我们家用的路由器外边再抽象出一个路由器） 静态路由：配置一条条规则（寻找下一跳），想玩三国杀要从2号口出去 转发网关：只改变MAC地址，不改变IP地址 （前提是在不同局域网IP地址不冲突） NAT网关：改变IP地址，可能我在中国的地址和美国的mike地址完全一样，这个时候就要把我的地址映射为一个在国际上肯定不会冲突的地址（比如加上国家的名字）；当前局域网的ip和国际ip需要有个映射关系并且记录在当前网关内，在国际上（不同局域网间）目标ip地址不会改变了 这种场景很常用，家用网段基本都一样 我想和邻居沟通的话 肯定要做一层映射改变当前的家用ip 动态路由：网络-&gt;图 寻找最短路径 OSPF：基于链路状态的路由协议/寻找最短路径（适用于内网，随便怎么走不需要别人允许） BGP：外网路由协议（除了路径长短还要考虑当地政策问题） TCP传输层：TCP/UDP IP头部会标识当前是tcp还是udp TCP面向连接（在交互前tcp会先建立连接/双方建立一定的数据结构来维护交互状态）、面向字节流（流：没头没尾 而udp继承了ip特性 基于数据包 一个一个地发 一个一个地收）、具有拥塞控制 UDP（可以说是ip的亲儿子）源端口号+目的端口号+udp长度+udp校验和+数据 TCP:源端口号+目的端口号+序号（解决乱序问题）+确认序号（解决丢包问题）+窗口大小（流量控制）+状态位（SYN ACK RST FIN） TCP三次握手：保证基本的消息有来有回所以最少三次 握手与挥手 在内核中，会为每个Socket维护两个队列，分别是已经建立连接的队列(established)以及还未完全建立连接点队列(syn_rcvd)注意上图中的几种状态 重点是TIME-WAIT状态（计时等待） 主动发起关闭侧在close之前最后的一个状态，持续时间2MSL 如果最后一个ACK发送失败，右侧会重新发送FIN，左侧再次发送ACK 另一个作用是保证，右侧发到左侧的包都在网络中彻底过期，不然可能该端口新的应用会收到混乱的消息（化身） 【问题1】为什么连接的时候是三次握手，关闭的时候却是四次握手？答：因为当Server端收到Client端的SYN连接请求报文后，可以直接发送SYN+ACK报文。其中ACK报文是用来应答的，SYN报文是用来同步的。但是关闭连接时，当Server端收到FIN报文时，很可能并不会立即关闭SOCKET，所以只能先回复一个ACK报文，告诉Client端，”你发的FIN报文我收到了”。只有等到我Server端所有的报文都发送完了，我才能发送FIN报文，因此不能一起发送。故需要四步握手。Client发送FIN表示，我没话想跟你说了，但是Server会先回一个我知道了，但是可能Server还有话对Client说，所以大多数时候FIN和ACK不会同时发送到Client端。 【问题2】为什么TIME_WAIT状态需要经过2MSL(最大报文段生存时间)才能返回到CLOSE状态？答：1）2MSL可以保证从我这里发送的所有包都不会有『有效』应答了，那么我可以放心的closed了2）注意client是在收到FIN后才变为time-wait状态的，假设网络状况极差ACK到达服务端已经过了MSL，这时候服务端要再次发送FIN而这个FIN的最大值也是MSL，所以这里取2MSL 【问题3】 为什么seq要和时间戳挂钩？不能每次都从1递增？ 答：假如A发送了1，2，3到B，然后A下线了，不久又上线了如果再次从1发送可能会收到无效的B发来的应答 状态机 缓存区发送端 发送了并且已经确认的包 发送了并且尚未确认的包 未发送并且等待发送的包 未发送并且暂时不发送的包这里面2+3要等于接收端滑动窗口（Advertised window）的大小 接收端 接收并确认过（还未被应用层读取到） 未接收马上能接收的（能承受的最大工作量，也就是Advertised window） 第二部分收到的包可能不是有序的，会出现空挡，只有和第一部分连续的，可以马上回复，中间空着的部分需要等待，哪怕后面的已经来了 未接收且没法接收的 容错机制超时重试对每个已经发送但是没收到ACK的包都有一个定时器，如果时间到了会重新尝试，这个时间一般稍大于往返时间RTT 发送端策略：自适应重传算法：RTT通过采样计算加权平均，这个值随着网络状况不断变化超时间隔加倍：每当遇到一次超时重传，都会将下次超时时间设置为先前的两倍 接收端策略：快重传：接收方收到一个序号大于期望序号的报文时候，会发送三个冗余的ACK，发送方收到后，会在定时器到期之前，重传丢失的报文段(一般和拥塞控制的快恢复一起使用)SACK：每次ACK的时候将接收方缓存的报文情况一并发送到发送方，这样发送方就知道哪个包丢失了 拥塞控制拥塞窗口cwnd，防止把网络塞满 目的是为了避免包丢失和超时重传 慢开始（指数启动）：涨到一定的值就不能再这么快地涨了(ssthresh) 拥塞避免（线性启动）：涨到一定的值就不能再涨了（代表当前出现了堵塞），接着把ssthresh设置为当前cwnd的一半，并且把cwnd置为1 急刹车：无论慢开始阶段还是拥塞避免阶段，一旦出现网络堵塞（ACK超时），就要把ssthresh设置为当前cwnd的一半，并且把cwnd置为1 算法假设丢包都是因为过程设备缓存满了，如果真的满了那么其实已经为时过晚；而如果是普通的丢包就使用默认的快速下降策略未免有些不妥，为此引入了BBR算法 发送的报文大小 &lt; min{cwnd,rwnd} 拥塞窗口和滑动窗口一起控制发送的速度 快恢复：上面那种cwnd直接减半并置为1的做法过于激进，相当于开车开到了200码突然停下来，于是就有了快恢复。快恢复一般和快重传一起使用，当收到连续三个冗余ACK后（表示有丢包但网络情况不是特别差），此时执行快恢复算法，令ssthresh = cwnd/2，cwnd = ssthresh，接着执行拥塞避免（线性启动） BBR拥塞算法：并非将整个网络都塞满（包括设备缓存区）or 丢包才执行拥塞算法，它力图寻找高带宽和低延迟的平衡点，它触发的点是管道填满，但是中间设备缓存不填满（缓存被填满，时延会增大） 1 设备缓存会导致延时？假如经过设备的包都不需要进入缓存，那么得到的速度是最快的。进入缓存且等待，等待的时间就是额外的延时。BBR就是为了避免这些问题：充分利用带宽；降低buffer占用率。 2 降低发送packet的速度，为何反而提速了？标准TCP拥塞算法是遇到丢包的数据时快速下降发送速度，因为算法假设丢包都是因为过程设备缓存满了。快速下降后重新慢启动，整个过程对于带宽来说是浪费的。通过packet速度-时间的图来看，从积分上看，BBR充分利用带宽时发送效率才是最高的。可以说BBR比标准TCP拥塞算法更正确地处理了数据丢包。对于网络上有一定丢包率的公网，BBR会更加智慧一点。回顾网络发展过程，带宽的是极大地改进的，而最小延迟会受限与介质传播速度，不会明显减少。BBR可以说是应运而生。 Socket编程 在内核中，会为每个Socket维护两个队列，分别是已经建立连接的队列(established)以及还未完全建立连接点队列(syn_rcvd) 接着服务端调用accept方法，会从内核中拿出一个处于established状态的socket，如果还没有的话会一直等待 Socket在linux中以文件的形式存在，所以会对应内核中的一个文件描述符 Ps：每个进程都有一个名为task_struct的数据结构，里面指向一个文件描述符数组，列出这个进程打开的所有文件的文件描述符。所以各种io 会有文件描述符数量的限制 select操作的底层：由于 Socket 是文件描述符，因而某个线程盯的所有的 Socket，都放在一个文件描述符集合 fd_set 中，这就是项目进度墙，然后调用 select 函数来监听文件描述符集合是否有变化。一旦有变化，就会依次查看每个文件描述符。那些发生变化的文件描述符在 fd_set 对应的位都设为 1，表示 Socket 可读或者可写，从而可以进行读写操作，然后再调用 select，接着盯着下一轮的变化。因为每次集合发生变动都要轮询整个集合。所以select操作同时管理的项目(Socket)数量受限。 epoll操作的底层：通过注册callback函数的方式，如果某个文件描述符发生变化，就会主动通知，当前epoll要监听的Socket都放在红黑树中 HTTP浏览器的80端口：http请求底层tcp协议的源端口，用于接收服务器返回的html Ps在浏览器里发送的请求 源端口号都是80 tomcat的8080端口：http请求底层tcp协议的目的端口，用于接收并处理客户端发送的http请求 HTTP 1.1 默认开启了keep-Alive 这样建立的tcp连接，可以在多次请求中复用 但是无法简洁地实现请求的并行 以纯文本的形式进行通信，每次都要完整的HTTP头 HTTP 2.0 对HTTP的头进行了一定压缩，将原来每次都要携带的大量 key value 在两端建立一个索引表，对相同的头只发送索引表中的索引 HTTP 2.0 协议将一个 TCP 的连接中，切分成多个流，每个流都有自己的 ID 而且流可以是客户端发往服务端，也可以是服务端发往客户端 压缩、分帧、二进制编码、多路复用(多个请求stream共享一个tcp连接的方式，并且响应可以同时返回)、请求优先级（为共享同一个tcp连接的请求设置优先级）]]></content>
      <categories>
        <category>网络协议</category>
      </categories>
      <tags>
        <tag>网络协议</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring源码阅读(5)-基于@Aspect的AOP实现原理]]></title>
    <url>%2F2018%2F11%2F19%2FSpring0-3%2F</url>
    <content type="text"><![CDATA[上篇文章分析了比较原始的AOP的实现方式，最后我们也分析了那种方式的弊端，本文分析一种更为炫酷的AOP的实现方式。 自动化Demo123456789101112131415161718192021@Aspectpublic class AspectJTest &#123; @Pointcut("execution(* *.test(..))") public void test() &#123;&#125; @Before("test()") public void beforeTest() &#123; System.out.println("before method advice"); &#125; @After("test()") public void afterTest() &#123;System.out.println("after method advice");&#125;&#125;public class MyServiceImpl implements MyService &#123; public void test() &#123; System.out.println("i am service test()"); &#125;&#125; 12345&lt;aop:aspectj-autoproxy/&gt;&lt;bean id="myService" class="com.techlab.spring.aop.MyServiceImpl"/&gt;&lt;bean class="com.techlab.spring.aop.AspectJTest"/&gt; 12345678public class EntranceApp &#123; public static void main(String[] args) throws Exception &#123; ApplicationContext context = new ClassPathXmlApplicationContext("beans.xml"); MyService b = (MyService)context.getBean("myService"); b.test(); &#125;&#125; 控制台输出123before method advicei am service test()after method advice 源码分析AnnotationAwareAspectJAutoProxyCreator的注册诶，我们看到配置文件清爽了很多，而且我们不需要针对每一个代理对象引入一个ProxyFactoryBean了。显然是这段代码起的作用&lt;aop:aspectj-autoproxy/&gt;这里要普及一个知识点，对于自定义的注解（annotation），一定有与之相匹配的解析器（parser）。那么我们就可以去寻找和当前注解匹配的解析器，我们全局搜索了下这个注解找到了与之相匹配的解析器。 Tips：在容器启动阶段，1)需要根据xml生成相应的resource；2)接着根据resource去获取document；3)最后对document进行解析来生成一个个beanDefination并注册到容器中。而对&lt;aop:aspectj-autoproxy/&gt;的解析就在第三步骤中，找到了对应的parser并调用parse方法，该方法内部会将一个特殊的BeanPostProcessor以BeanDefination形式注册到容器中。同理针对@Resource以及@Autowire注解也有对应的AutowiredAnnotationBeanPostProcessor,CommonAnnotationBeanPostProcessor，这两个类一般是通过postProcessPropertyValues方法来发挥作用。而这两个BeanPostProcessor的BeanDefination注册也是通过对&lt;context:annotation-config/&gt;的解析实现的。 Ps 这两个BeanPostProcessor都是InstantiationAwareBeanPostProcessor的实现果然不出老夫所料，是在ComponentScanBeanDefinitionParser的parse方法中调用了AnnotationConfigUtils的registerAnnotationConfigProcessors方法，该方法直接new了很多相关BeanPostProcessor的defination。Ps有点举一反三的能力了 12345678class AspectJAutoProxyBeanDefinitionParser implements BeanDefinitionParser &#123; @Override public BeanDefinition parse(Element element, ParserContext parserContext) &#123; AopNamespaceUtils.registerAspectJAnnotationAutoProxyCreatorIfNecessary(parserContext, element); extendBeanDefinition(element, parserContext); return null; &#125;&#125; 我们重点关注第一行代码，根据名字我们就知道这个方法是用来注册一个『支持AspectJ注解的自动代理创建器』，我们深入这个方法去瞅瞅123456789101112131415161718192021222324public static BeanDefinition registerAspectJAnnotationAutoProxyCreatorIfNecessary(BeanDefinitionRegistry registry, Object source) &#123; return registerOrEscalateApcAsRequired(AnnotationAwareAspectJAutoProxyCreator.class, registry, source);&#125;private static BeanDefinition registerOrEscalateApcAsRequired(Class&lt;?&gt; cls, BeanDefinitionRegistry registry, Object source) &#123; Assert.notNull(registry, "BeanDefinitionRegistry must not be null"); if (registry.containsBeanDefinition(AUTO_PROXY_CREATOR_BEAN_NAME)) &#123; BeanDefinition apcDefinition = registry.getBeanDefinition(AUTO_PROXY_CREATOR_BEAN_NAME); if (!cls.getName().equals(apcDefinition.getBeanClassName())) &#123; int currentPriority = findPriorityForClass(apcDefinition.getBeanClassName()); int requiredPriority = findPriorityForClass(cls); if (currentPriority &lt; requiredPriority) &#123; apcDefinition.setBeanClassName(cls.getName()); &#125; &#125; return null; &#125; RootBeanDefinition beanDefinition = new RootBeanDefinition(cls); beanDefinition.setSource(source); beanDefinition.getPropertyValues().add("order", Ordered.HIGHEST_PRECEDENCE); beanDefinition.setRole(BeanDefinition.ROLE_INFRASTRUCTURE); registry.registerBeanDefinition(AUTO_PROXY_CREATOR_BEAN_NAME, beanDefinition); return beanDefinition;&#125; 首先先去看看是否已经创建了自动代理创建器。如果创建了那么和当前的创建器进行优先级的比较，并将beanDefination替换为优先级较高的那个并返回；如果未创建，那么需要new一个BeanDefinition、填充部分属性并注册到容器中。至此，『支持AspectJ注解的自动代理创建器』已经注册到容器中了，我们接下来看看这个创建器到底是何方神圣。 AnnotationAwareAspectJAutoProxyCreator详解我们先来看看这个AnnotationAwareAspectJAutoProxyCreator的类结构图我们定位到BeanPostProcessor这个接口，我们知道这个接口是容器的一个扩展点，可以在bean初始化的前后执行一些自定义的操作。而我们重点关注的就是AnnotationAwareAspectJAutoProxyCreator#postProcessAfterInitialization方法12345678910@Overridepublic Object postProcessAfterInitialization(Object bean, String beanName) throws BeansException &#123; if (bean != null) &#123; Object cacheKey = getCacheKey(bean.getClass(), beanName); if (!this.earlyProxyReferences.contains(cacheKey)) &#123; return wrapIfNecessary(bean, beanName, cacheKey); &#125; &#125; return bean;&#125; 这里主要看下1234567891011121314protected Object wrapIfNecessary(Object bean, String beanName, Object cacheKey) &#123; //... // Create proxy if we have advice. Object[] specificInterceptors = getAdvicesAndAdvisorsForBean(bean.getClass(), beanName, null); if (specificInterceptors != DO_NOT_PROXY) &#123; this.advisedBeans.put(cacheKey, Boolean.TRUE); Object proxy = createProxy(bean.getClass(), beanName, specificInterceptors, new SingletonTargetSource(bean)); this.proxyTypes.put(cacheKey, proxy.getClass()); return proxy; &#125; this.advisedBeans.put(cacheKey, Boolean.FALSE); return bean;&#125; 和ProxyFactoryBean类似，这里也是分成了两个步骤，首先是获取当前bean匹配的advisor list；之后创建代理对象并返回。 获取advisor list12345678@Overrideprotected Object[] getAdvicesAndAdvisorsForBean(Class&lt;?&gt; beanClass, String beanName, TargetSource targetSource) &#123; List&lt;Advisor&gt; advisors = findEligibleAdvisors(beanClass, beanName); if (advisors.isEmpty()) &#123; return DO_NOT_PROXY; &#125; return advisors.toArray();&#125; 我们看到最终都是以advisor的形式返回给调用方的。 而且这里值得注意的是，如果当前没有匹配的advisor（说明无需代理）那么直接返回无需代理的标识。AnnotationAwareAspectJAutoProxyCreator默认对所有bean生效，但是对于不需要代理的对象会直接返回。123456789protected List&lt;Advisor&gt; findEligibleAdvisors(Class&lt;?&gt; beanClass, String beanName) &#123; List&lt;Advisor&gt; candidateAdvisors = findCandidateAdvisors(); List&lt;Advisor&gt; eligibleAdvisors = findAdvisorsThatCanApply(candidateAdvisors, beanClass, beanName); extendAdvisors(eligibleAdvisors); if (!eligibleAdvisors.isEmpty()) &#123; eligibleAdvisors = sortAdvisors(eligibleAdvisors); &#125; return eligibleAdvisors;&#125; findAdvisorBeans步骤一：我们看第一行代码，根据方法名称猜测是找到所有的Advisor，注意这个时候是没有传入beanClass和beanName的囧，这里之前跟代码居然跟错了，本来应该是调用子类的方法却跟到了父类中，调试真乃阅读源码之神器1234567protected List&lt;Advisor&gt; findCandidateAdvisors() &#123; // Add all the Spring advisors found according to superclass rules. List&lt;Advisor&gt; advisors = super.findCandidateAdvisors(); // Build Advisors for all AspectJ aspects in the bean factory. advisors.addAll(this.aspectJAdvisorsBuilder.buildAspectJAdvisors()); return advisors;&#125; 首先调用父类的findCandidateAdvisors方法，而这个方法很简单核心就是这行代码BeanFactoryUtils.beanNamesForTypeIncludingAncestors(this.beanFactory, Advisor.class, true, false)，就是找到所有普通的Advisor。接着添加了aspectJ定义的advisors，我们重点关注这种情况。123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960public List&lt;Advisor&gt; buildAspectJAdvisors() &#123; List&lt;String&gt; aspectNames = null; synchronized (this) &#123; aspectNames = this.aspectBeanNames; if (aspectNames == null) &#123; List&lt;Advisor&gt; advisors = new LinkedList&lt;Advisor&gt;(); aspectNames = new LinkedList&lt;String&gt;(); String[] beanNames = BeanFactoryUtils.beanNamesForTypeIncludingAncestors(this.beanFactory, Object.class, true, false); for (String beanName : beanNames) &#123; if (!isEligibleBean(beanName)) &#123; continue; &#125; // We must be careful not to instantiate beans eagerly as in this // case they would be cached by the Spring container but would not // have been weaved Class&lt;?&gt; beanType = this.beanFactory.getType(beanName); if (beanType == null) &#123; continue; &#125; if (this.advisorFactory.isAspect(beanType)) &#123; aspectNames.add(beanName); AspectMetadata amd = new AspectMetadata(beanType, beanName); if (amd.getAjType().getPerClause().getKind() == PerClauseKind.SINGLETON) &#123; MetadataAwareAspectInstanceFactory factory = new BeanFactoryAspectInstanceFactory(this.beanFactory, beanName); //这里一个Aspect类型的bean，是有可能包含多于一个的Advisor的 List&lt;Advisor&gt; classAdvisors = this.advisorFactory.getAdvisors(factory); if (this.beanFactory.isSingleton(beanName)) &#123; this.advisorsCache.put(beanName, classAdvisors); &#125; else &#123; this.aspectFactoryCache.put(beanName, factory); &#125; advisors.addAll(classAdvisors); &#125; //... &#125; &#125; this.aspectBeanNames = aspectNames; return advisors; &#125; &#125; if (aspectNames.isEmpty()) &#123; return Collections.emptyList(); &#125; List&lt;Advisor&gt; advisors = new LinkedList&lt;Advisor&gt;(); for (String aspectName : aspectNames) &#123; List&lt;Advisor&gt; cachedAdvisors = this.advisorsCache.get(aspectName); if (cachedAdvisors != null) &#123; advisors.addAll(cachedAdvisors); &#125; else &#123; MetadataAwareAspectInstanceFactory factory = this.aspectFactoryCache.get(aspectName); advisors.addAll(this.advisorFactory.getAdvisors(factory)); &#125; &#125; return advisors;&#125; 方法有点长，我们一点点去分析。（1）首先加了个锁，很简单因为List类型的aspectNames是非线程安全的（2）接着代码进入到同步代码块中，在这里非常丧病的找到了当前容器中的所有beanName（3）遍历每个beanName，根据beanName获取beanType，接着根据是否有注解去判断是否是Aspect类型的（4）将Aspect类型的advisor一同放到advisors中（这里一个Aspect类型的bean，是有可能包含多于一个的Advisor的） findAdvisorsThatCanApply步骤二：接着从步骤一返回的Advisor中找到和当前bean匹配的Advisor Ps代码中忽略了introductionAdvisor12345678910111213141516171819202122232425262728293031323334353637383940414243444546public static List&lt;Advisor&gt; findAdvisorsThatCanApply(List&lt;Advisor&gt; candidateAdvisors, Class&lt;?&gt; clazz) &#123; //... for (Advisor candidate : candidateAdvisors) &#123; //... if (canApply(candidate, clazz, hasIntroductions)) &#123; eligibleAdvisors.add(candidate); &#125; &#125; //... return eligibleAdvisors;&#125;public static boolean canApply(Advisor advisor, Class&lt;?&gt; targetClass, boolean hasIntroductions) &#123; //.. else if (advisor instanceof PointcutAdvisor) &#123; PointcutAdvisor pca = (PointcutAdvisor) advisor; return canApply(pca.getPointcut(), targetClass, hasIntroductions); &#125; else &#123; // It doesn't have a pointcut so we assume it applies. return true; &#125;&#125;public static boolean canApply(Pointcut pc, Class&lt;?&gt; targetClass, boolean hasIntroductions) &#123; Assert.notNull(pc, "Pointcut must not be null"); if (!pc.getClassFilter().matches(targetClass)) &#123; return false; &#125; MethodMatcher methodMatcher = pc.getMethodMatcher(); //... Set&lt;Class&lt;?&gt;&gt; classes = new HashSet&lt;Class&lt;?&gt;&gt;(ClassUtils.getAllInterfacesForClassAsSet(targetClass)); classes.add(targetClass); //这里只要advisor和当前bean or bean的父类的任意方法匹配，就会被加入到合格的advisor list中 for (Class&lt;?&gt; clazz : classes) &#123; Method[] methods = clazz.getMethods(); for (Method method : methods) &#123; if ((introductionAwareMethodMatcher != null &amp;&amp; introductionAwareMethodMatcher.matches(method, targetClass, hasIntroductions)) || methodMatcher.matches(method, targetClass)) &#123; return true; &#125; &#125; &#125; return false;&#125; 我们截取的代码忽略了关于IntroductionAdvisor的相关代码，可以看到代码逻辑相对简单，遍历所有的advisor找到和当前对象相匹配的advisor们(利用Advisor的PointCut判断是否和当前类的任何方法相匹配)，最后将符合条件的advisor放到eligibleAdvisors中 Ps：在我们Demo中普通类型的Advisor数量是0；AspectJ类型的Advisor数量是2。 根据获取的增强去创建代理1234567891011121314protected Object wrapIfNecessary(Object bean, String beanName, Object cacheKey) &#123; //... // Create proxy if we have advice. Object[] specificInterceptors = getAdvicesAndAdvisorsForBean(bean.getClass(), beanName, null); if (specificInterceptors != DO_NOT_PROXY) &#123; this.advisedBeans.put(cacheKey, Boolean.TRUE); Object proxy = createProxy(bean.getClass(), beanName, specificInterceptors, new SingletonTargetSource(bean)); this.proxyTypes.put(cacheKey, proxy.getClass()); return proxy; &#125; this.advisedBeans.put(cacheKey, Boolean.FALSE); return bean;&#125; 接着是这个方法12345678910111213141516171819202122232425262728293031protected Object createProxy( Class&lt;?&gt; beanClass, String beanName, Object[] specificInterceptors, TargetSource targetSource) &#123; ProxyFactory proxyFactory = new ProxyFactory(); // Copy our properties (proxyTargetClass etc) inherited from ProxyConfig. proxyFactory.copyFrom(this); if (!shouldProxyTargetClass(beanClass, beanName)) &#123; // Must allow for introductions; can't just set interfaces to // the target's interfaces only. Class&lt;?&gt;[] targetInterfaces = ClassUtils.getAllInterfacesForClass(beanClass, this.proxyClassLoader); for (Class&lt;?&gt; targetInterface : targetInterfaces) &#123; proxyFactory.addInterface(targetInterface); &#125; &#125; Advisor[] advisors = buildAdvisors(beanName, specificInterceptors); for (Advisor advisor : advisors) &#123; proxyFactory.addAdvisor(advisor); &#125; proxyFactory.setTargetSource(targetSource); customizeProxyFactory(proxyFactory); proxyFactory.setFrozen(this.freezeProxy); if (advisorsPreFiltered()) &#123; proxyFactory.setPreFiltered(true); &#125; return proxyFactory.getProxy(this.proxyClassLoader);&#125; 将我们获得的与当前类匹配的Advisors放到ProxyFactory中，接着创建动态代理，Proxy.newProxyInstance(classLoader, proxiedInterfaces, this); 至此，自动的AOP和半自动的AOP一样走到了同一个岔路口，可谓殊途同归。既然代理对象的创建的逻辑都一样，那么后续Proxy的invoke方法逻辑也就完全相同了。（包括根据class&amp;method找到匹配的interceptor、调用方法with interceptor chain） 小结：1）IOC容器启动阶段需要对document进行解析以生成一个个的BeanDefination并注册到容器中2）在这个步骤中会对我们在xml中写的&lt;aop:aspectj-autoproxy/&gt;进行解析3）解析的过程就是创建了一个AnnotationAwareAspectJAutoProxyCreator.class类型的BeanDefination并注册到了容器中4）IOC流程继续执行的时候会调用BeanPostProcessor.postProcessAfterInitialiization()5）而我们注册的AnnotationAwareAspectJAutoProxyCreator就是一个BeanPostProcessor，此时执行它对应的postProcessAfterInitialiization方法6）该方法首先获取advisor list，包括普通的advisor以及aspectJ类型的advisor（只有和任意当前类以及当前类实现的接口的方法相匹配的advisor才会被加入到list中）7）接着根据获取到的advisor创建动态代理并返回（此时获取到的bean已经是动态代理对象了）8）当调用bean的方法时候，会触发动态代理对象的invoke方法，该方法会找到和当前class &amp; method匹配的advisor并包装成interceptor，接着执行经典的proceed方法。 Tips：和上文的半自动aop不同，最开始寻找advisor chain的时候只会找到和当前bean的任意接口任意方法匹配的advisor，如果找到才说明它需要被代理，而在真正执行invoke方法时，才会根据method进行进一步的甄别并封装成interceptor。而上文的半自动aop会找到配置的所有的advisor，并且一定会将配置的target包装为代理。这是两者的区别，请注意。无论是哪种方式，最终都是要创建持有advisor的动态代理对象，而该对象内部的invoke方法会从advisor中找到和当前class method匹配的advisor并包装为interceptor，最后执行方法调用。 至此，AOP专题到此结束，源码分析过程主要分析了一些主干流程，很多也很重要的点被砍掉了。以后有机会对那些部分再深入研究。 That’s all.2018-11-19 20:06:01 上海]]></content>
      <categories>
        <category>Spring源码阅读</category>
      </categories>
      <tags>
        <tag>Spring源码阅读</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring源码阅读(4)-基于FactoryBean的AOP实现原理]]></title>
    <url>%2F2018%2F11%2F19%2FSpring0-2%2F</url>
    <content type="text"><![CDATA[上篇文章分析了FactoryBean的工作原理，本文以FactoryBean的实现ProxyFactoryBean为切入点分析AOP的实现原理。 Demo123456789101112131415161718public interface MyService &#123; void test();&#125;public class MyServiceImpl implements MyService &#123; public void test() &#123; System.out.println("i am service test()"); &#125;&#125;public class MyAfterAdvice implements AfterReturningAdvice &#123; public void afterReturning(Object returnValue, Method method, Object[] args, Object target) throws Throwable &#123; System.out.println("after method advice"); &#125;&#125;public class MyBeforeAdvice implements MethodBeforeAdvice &#123; public void before(Method method, Object[] args, Object target) throws Throwable &#123; System.out.println("before method advice"); &#125;&#125; 123456789101112131415161718&lt;bean id="myService" class="com.techlab.spring.aop.MyServiceImpl"/&gt;&lt;bean id="beforeAdvice" class="com.techlab.spring.aop.MyBeforeAdvice"/&gt;&lt;bean id="afterAdvice" class="com.techlab.spring.aop.MyAfterAdvice"/&gt;&lt;bean id="testAop" class="org.springframework.aop.framework.ProxyFactoryBean"&gt; &lt;property name="interfaces" value="com.techlab.spring.aop.MyService"/&gt; &lt;property name="target"&gt; &lt;ref bean="myService"/&gt; &lt;/property&gt; &lt;property name="interceptorNames"&gt; &lt;list&gt; &lt;value&gt;beforeAdvice&lt;/value&gt; &lt;value&gt;afterAdvice&lt;/value&gt; &lt;/list&gt; &lt;/property&gt;&lt;/bean&gt; 12345678public class EntranceApp &#123; public static void main(String[] args) throws Exception &#123; ApplicationContext context = new ClassPathXmlApplicationContext("beans.xml"); MyService b = (MyService)context.getBean("testAop"); b.test(); &#125;&#125; 控制台输出123before method advicei am service test()after method advice 关于这个配置文件有几个地方要说明，这里的interceptorNames属性，既可以配置advice也可以配置advisor，如果只配置advice的情况下，那么默认的pointCut就是匹配所有的类所有的方法。如果想实现自定义的方法匹配，需要自己实现advisor。interceptorNames还支持通配符，这样会找到所有匹配的advice and advisor。既然是FactoryBean那么它最核心的方法就是getObject()方法了 ProxyFactoryBean#getObject()1234567891011121314@Overridepublic Object getObject() throws BeansException &#123; initializeAdvisorChain(); if (isSingleton()) &#123; return getSingletonInstance(); &#125; else &#123; if (this.targetName == null) &#123; logger.warn("Using non-singleton proxies with singleton targets is often undesirable. " + "Enable prototype proxies by setting the 'targetName' property."); &#125; return newPrototypeInstance(); &#125;&#125; 先来概览下上面这个方法，首先是初始化AdvisorChain；接着返回代理对象。 initializeAdvisorChain12345678910111213141516171819private synchronized void initializeAdvisorChain() throws AopConfigException, BeansException &#123; if (this.advisorChainInitialized) &#123; return; &#125; if (!ObjectUtils.isEmpty(this.interceptorNames)) &#123; //... for (String name : this.interceptorNames) &#123; //... Object advice; if (this.singleton || this.beanFactory.isSingleton(name)) &#123; // Add the real Advisor/Advice to the chain. advice = this.beanFactory.getBean(name); &#125; //... addAdvisorOnChainCreation(advice, name); &#125; &#125; this.advisorChainInitialized = true;&#125; 这个方法简化后如上所示，主要就是遍历interceptorNames，利用BeanFactory找到每个advice的实例。之后调用addAdvisorOnChainCreation(advice, name)。 Ps 这里的参数名字有点意思interceptorNames，无论配置的是advice还是advisor最后都会被包装成匹配的interceptor，实际上最开始即使配置了advice也会先包装成advisor，定位到具体的类和方法后，会卸去包装还原为advice，然后再将advice包装成interceptor。 123456789private void addAdvisorOnChainCreation(Object next, String name) &#123; // We need to convert to an Advisor if necessary so that our source reference // matches what we find from superclass interceptors. Advisor advisor = namedBeanToAdvisor(next); if (logger.isTraceEnabled()) &#123; logger.trace("Adding advisor with name '" + name + "'"); &#125; addAdvisor(advisor);&#125; 我们可以简单地理解为 Advisor = Advice(逻辑+相对方法的方位) + POINTCUT（定位），而POINTCUT=ClassFilter（类定位）+ MethodMatcher（方法定位）首先，将advice类型的bean包装成advisor，如果本身就是advisor那么不需要包装。接着，将advisor放到advisor list中。第二步骤很简单，我们重点看第一个步骤。123456789101112131415161718192021222324private Advisor namedBeanToAdvisor(Object next) &#123; //... try &#123; return this.advisorAdapterRegistry.wrap(next); &#125;&#125;public Advisor wrap(Object adviceObject)&#123; //如果是advisor那么无须包装 if (adviceObject instanceof Advisor) &#123; return (Advisor) adviceObject; &#125; //... return new DefaultPointcutAdvisor(advice);&#125;public DefaultPointcutAdvisor(Advice advice) &#123; this(Pointcut.TRUE, advice);&#125;public DefaultPointcutAdvisor(Pointcut pointcut, Advice advice) &#123; this.pointcut = pointcut; setAdvice(advice);&#125; 可以看到如果我们只传advice进来，那么包装成advisor的时候，默认的Pointcut就是Pointcut.TRUE，表示匹配所有类的所有方法。而接下来的方法很简单了，就是保存pointcut和advice。包装成Advisor后，塞到Advisor链中，至此第一个步骤就结束了。 小结：（1）根据xml中配置的interceptorNames的list属性，去利用BeanFactory.getBean()找到每个对应的实例（2）如果bean本身就是advisor类型的，那么直接加入到advisor的list中（3）如果bean是advice类型的，那么还需要对它进行包装，包装为advisor（Ps此时配套的PointCut是匹配所有的方法的）（4）包装结束后，同样加入到advisor的list中，这个list由ProxyFactoryBean持有 getSingletonInstance()1234private synchronized Object getSingletonInstance() &#123; //... return getProxy(createAopProxy());&#125; 这里又分为两个步骤 createAopProxy()1234567891011121314151617181920212223protected final synchronized AopProxy createAopProxy() &#123; if (!this.active) &#123; activate(); &#125; return getAopProxyFactory().createAopProxy(this);&#125;public AopProxy createAopProxy(AdvisedSupport config) throws AopConfigException &#123; if (config.isOptimize() || config.isProxyTargetClass() || hasNoUserSuppliedProxyInterfaces(config)) &#123; Class&lt;?&gt; targetClass = config.getTargetClass(); if (targetClass == null) &#123; throw new AopConfigException("TargetSource cannot determine target class: " + "Either an interface or a target is required for proxy creation."); &#125; if (targetClass.isInterface()) &#123; return new JdkDynamicAopProxy(config); &#125; return new ObjenesisCglibAopProxy(config); &#125; else &#123; return new JdkDynamicAopProxy(config); &#125;&#125; 第一个方法中我们看到createAopProxy的入参是this，这里this表示的就是ProxyFactoryBean（持有advisorChain）第二个方法中，如果目标类未实现任何接口，那么通过cglib创建动态代理；反之，通过jdk默认的方式创建动态代理。这里就是简单的new了一个代理对象JdkDynamicAopProxy,并且把一些配置参数（config=proxyFactoryBean）一起传给Proxy。 getProxy(AopProxy)12345678910protected Object getProxy(AopProxy aopProxy) &#123; return aopProxy.getProxy(this.proxyClassLoader);&#125;public Object getProxy(ClassLoader classLoader) &#123; //... Class&lt;?&gt;[] proxiedInterfaces = AopProxyUtils.completeProxiedInterfaces(this.advised); findDefinedEqualsAndHashCodeMethods(proxiedInterfaces); return Proxy.newProxyInstance(classLoader, proxiedInterfaces, this);&#125; 我们看到了经典的Proxy.newProxyInstance(classLoader, proxiedInterfaces, this)，注意这里的this，此时表示的是JdkDynamicAopProxy实例。我们看过了getObject方法，下面要看看代理类在调用方法的过程中，具体的执行逻辑也就是invoke方法 动态代理的invoke方法代码来到，JdkDynamicAopProxy#invoke1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283public Object invoke(Object proxy, Method method, Object[] args) throws Throwable &#123; MethodInvocation invocation; Object oldProxy = null; boolean setProxyContext = false; TargetSource targetSource = this.advised.targetSource; Class&lt;?&gt; targetClass = null; Object target = null; try &#123; //如果代理接口中没有覆写hashCode和equals方法，那么直接返回当前类覆写的对应方法 //并且不会执行代理逻辑 if (!this.equalsDefined &amp;&amp; AopUtils.isEqualsMethod(method)) &#123; // The target does not implement the equals(Object) method itself. return equals(args[0]); &#125; if (!this.hashCodeDefined &amp;&amp; AopUtils.isHashCodeMethod(method)) &#123; // The target does not implement the hashCode() method itself. return hashCode(); &#125; if (!this.advised.opaque &amp;&amp; method.getDeclaringClass().isInterface() &amp;&amp; method.getDeclaringClass().isAssignableFrom(Advised.class)) &#123; // Service invocations on ProxyConfig with the proxy config... return AopUtils.invokeJoinpointUsingReflection(this.advised, method, args); &#125; Object retVal; if (this.advised.exposeProxy) &#123; // Make invocation available if necessary. oldProxy = AopContext.setCurrentProxy(proxy); setProxyContext = true; &#125; // May be null. Get as late as possible to minimize the time we "own" the target, // in case it comes from a pool. target = targetSource.getTarget(); if (target != null) &#123; targetClass = target.getClass(); &#125; // Get the interception chain for this method. List&lt;Object&gt; chain = this.advised.getInterceptorsAndDynamicInterceptionAdvice(method, targetClass); // Check whether we have any advice. If we don't, we can fallback on direct // reflective invocation of the target, and avoid creating a MethodInvocation. if (chain.isEmpty()) &#123; // We can skip creating a MethodInvocation: just invoke the target directly // Note that the final invoker must be an InvokerInterceptor so we know it does // nothing but a reflective operation on the target, and no hot swapping or fancy proxying. retVal = AopUtils.invokeJoinpointUsingReflection(target, method, args); &#125; else &#123; // We need to create a method invocation... invocation = new ReflectiveMethodInvocation(proxy, target, method, args, targetClass, chain); // Proceed to the joinpoint through the interceptor chain. retVal = invocation.proceed(); &#125; // Massage return value if necessary. Class&lt;?&gt; returnType = method.getReturnType(); if (retVal != null &amp;&amp; retVal == target &amp;&amp; returnType.isInstance(proxy) &amp;&amp; !RawTargetAccess.class.isAssignableFrom(method.getDeclaringClass())) &#123; // Special case: it returned "this" and the return type of the method // is type-compatible. Note that we can't help if the target sets // a reference to itself in another returned object. retVal = proxy; &#125; else if (retVal == null &amp;&amp; returnType != Void.TYPE &amp;&amp; returnType.isPrimitive()) &#123; throw new AopInvocationException("Null return value from advice does not match primitive return type for: " + method); &#125; return retVal; &#125; finally &#123; if (target != null &amp;&amp; !targetSource.isStatic()) &#123; // Must have come from TargetSource. targetSource.releaseTarget(target); &#125; if (setProxyContext) &#123; // Restore old proxy. AopContext.setCurrentProxy(oldProxy); &#125; &#125;&#125; 我们还是人为的分为两个步骤，1）根据class和method获取匹配的interceptorChain；2）调用方法with interceptorChain chain先来看步骤一 根据class和method获取匹配的interceptorChain12345678910public List&lt;Object&gt; getInterceptorsAndDynamicInterceptionAdvice(Method method, Class&lt;?&gt; targetClass) &#123; MethodCacheKey cacheKey = new MethodCacheKey(method); List&lt;Object&gt; cached = this.methodCache.get(cacheKey); if (cached == null) &#123; cached = this.advisorChainFactory.getInterceptorsAndDynamicInterceptionAdvice( this, method, targetClass); this.methodCache.put(cacheKey, cached); &#125; return cached;&#125; 可以看到，会针对与当前class &amp; method匹配的chain进行缓存。我们观察无缓存的情况123456789101112131415161718192021222324252627282930public List&lt;Object&gt; getInterceptorsAndDynamicInterceptionAdvice( Advised config, Method method, Class&lt;?&gt; targetClass) &#123; //... List&lt;Object&gt; interceptorList = new ArrayList&lt;Object&gt;(config.getAdvisors().length); //... for (Advisor advisor : config.getAdvisors()) &#123; if (advisor instanceof PointcutAdvisor) &#123; // Add it conditionally. PointcutAdvisor pointcutAdvisor = (PointcutAdvisor) advisor; if (config.isPreFiltered() || pointcutAdvisor.getPointcut().getClassFilter().matches(targetClass)) &#123; MethodInterceptor[] interceptors = registry.getInterceptors(advisor); MethodMatcher mm = pointcutAdvisor.getPointcut().getMethodMatcher(); if (MethodMatchers.matches(mm, method, targetClass, hasIntroductions)) &#123; if (mm.isRuntime()) &#123; // Creating a new object instance in the getInterceptors() method // isn't a problem as we normally cache created chains. for (MethodInterceptor interceptor : interceptors) &#123; interceptorList.add(new InterceptorAndDynamicMethodMatcher(interceptor, mm)); &#125; &#125; else &#123; interceptorList.addAll(Arrays.asList(interceptors)); &#125; &#125; &#125; &#125; //... &#125; return interceptorList;&#125; 遍历每个advisor（包括本身配置的以及advice被包裹成的advisor），找到和当前class、method匹配的advisor，在这中间调用了一个很重要的方法registry.getInterceptors(advisor)该方法会将advisor包裹为对应类型的interceptor123456789101112131415161718192021222324//DefaultAdvisorAdapterRegistry.classpublic DefaultAdvisorAdapterRegistry() &#123; registerAdvisorAdapter(new MethodBeforeAdviceAdapter()); registerAdvisorAdapter(new AfterReturningAdviceAdapter()); registerAdvisorAdapter(new ThrowsAdviceAdapter());&#125; public MethodInterceptor[] getInterceptors(Advisor advisor) throws UnknownAdviceTypeException &#123; List&lt;MethodInterceptor&gt; interceptors = new ArrayList&lt;MethodInterceptor&gt;(3); Advice advice = advisor.getAdvice(); if (advice instanceof MethodInterceptor) &#123; interceptors.add((MethodInterceptor) advice); &#125; //判断属于哪种类型的advisor，前置、后置、异常 并构造对应interceptor for (AdvisorAdapter adapter : this.adapters) &#123; if (adapter.supportsAdvice(advice)) &#123; interceptors.add(adapter.getInterceptor(advisor)); &#125; &#125; if (interceptors.isEmpty()) &#123; throw new UnknownAdviceTypeException(advisor.getAdvice()); &#125; return interceptors.toArray(new MethodInterceptor[interceptors.size()]);&#125; 我们看到类的构造方法中，先是将3个Adapter放到了list中。以MethodBeforeAdviceAdapter为例，我们看看这个类1234567891011121314class MethodBeforeAdviceAdapter implements AdvisorAdapter, Serializable &#123; @Override public boolean supportsAdvice(Advice advice) &#123; return (advice instanceof MethodBeforeAdvice); &#125; @Override public MethodInterceptor getInterceptor(Advisor advisor) &#123; MethodBeforeAdvice advice = (MethodBeforeAdvice) advisor.getAdvice(); return new MethodBeforeAdviceInterceptor(advice); &#125; &#125; 可以看到这个类还是很简单的，第一个方法就是用来判断当前的advice是否是MethodBeforeAdvice类型的；第二个方法是将advice封装到intreceptor中。Ps：注意这里已经定位到了具体的类和方法，此时Advisor可以甩下累赘，只用advice来包装成interceptor了123456789101112131415161718192021public class MethodBeforeAdviceInterceptor implements MethodInterceptor, Serializable &#123; private MethodBeforeAdvice advice; /** * Create a new MethodBeforeAdviceInterceptor for the given advice. * @param advice the MethodBeforeAdvice to wrap */ public MethodBeforeAdviceInterceptor(MethodBeforeAdvice advice) &#123; Assert.notNull(advice, "Advice must not be null"); this.advice = advice; &#125; @Override public Object invoke(MethodInvocation mi) throws Throwable &#123; this.advice.before(mi.getMethod(), mi.getArguments(), mi.getThis() ); return mi.proceed(); &#125;&#125; 这里先留意下，invoke方法，我们看到先是调用了advice的before方法，再调用了proceed，这个proceed方法我们后续会去分析。至此，已经将匹配的advice包裹成了标准的interceptor，并存放在interceptors这个list中（这个就是interceptor chain）Tips：一个advisor(advice),可能被封装为多个interceptor,因为advisor(advice)可能同时实现了多个接口。 调用方法with interceptor chain123456789101112if (chain.isEmpty()) &#123; // We can skip creating a MethodInvocation: just invoke the target directly // Note that the final invoker must be an InvokerInterceptor so we know it does // nothing but a reflective operation on the target, and no hot swapping or fancy proxying. retVal = AopUtils.invokeJoinpointUsingReflection(target, method, args);&#125;else &#123; // We need to create a method invocation... invocation = new ReflectiveMethodInvocation(proxy, target, method, args, targetClass, chain); // Proceed to the joinpoint through the interceptor chain. retVal = invocation.proceed();&#125; 如果interceptorChain是空的，那么很简单直接通过反射执行方法就ok了；反之，代码来到了proceed12345678910111213141516171819202122232425262728public Object proceed() throws Throwable &#123; // We start with an index of -1 and increment early. if (this.currentInterceptorIndex == this.interceptorsAndDynamicMethodMatchers.size() - 1) &#123; return invokeJoinpoint(); &#125; Object interceptorOrInterceptionAdvice = this.interceptorsAndDynamicMethodMatchers.get(++this.currentInterceptorIndex); if (interceptorOrInterceptionAdvice instanceof InterceptorAndDynamicMethodMatcher) &#123; // Evaluate dynamic method matcher here: static part will already have // been evaluated and found to match. InterceptorAndDynamicMethodMatcher dm = (InterceptorAndDynamicMethodMatcher) interceptorOrInterceptionAdvice; if (dm.methodMatcher.matches(this.method, this.targetClass, this.arguments)) &#123; return dm.interceptor.invoke(this); &#125; else &#123; // Dynamic matching failed. // Skip this interceptor and invoke the next in the chain. return proceed(); &#125; &#125; else &#123; // It's an interceptor, so we just invoke it: The pointcut will have // been evaluated statically before this object was constructed. return ((MethodInterceptor) interceptorOrInterceptionAdvice).invoke(this); &#125;&#125; 这里的interceptorsAndDynamicMethodMatchers其实就是刚刚的interceptorChain这里的index是从-1开始的，所以最开始代码的意思是如果这个array被遍历光了，那么就可以执行本身的方法了；如果并未遍历结束，那么获取下一个interceptor并调用其invoke方法。我们通过通过Demo中的例子来看看这个方法，假设在chain中的顺序是{MyAfterInterceptor,MyBeforeAdviceInterceptor}，此时先执行after的invoke方法12345public Object invoke(MethodInvocation mi) throws Throwable &#123; Object retVal = mi.proceed(); this.advice.afterReturning(retVal, mi.getMethod(), mi.getArguments(), mi.getThis()); return retVal;&#125; 我们看到此时会继续调用proceed方法，此时遍历到了MyBeforeAdvice，执行它的invoke方法1234public Object invoke(MethodInvocation mi) throws Throwable &#123; this.advice.before(mi.getMethod(), mi.getArguments(), mi.getThis() ); return mi.proceed();&#125; 显然代码的执行顺序是，before-&gt;method-&gt;afterReturning。这和我们的预期也是相符的。 小结：1.ProxyFactoryBean的getObject方法返回了一个代理对象到调用方2.而在getObject方法中做了两件事，1）初始化advisor链，2）创建动态代理对象并返回（此时代理持有advisor链）3.接着执行代理对象的方法会调用其invoke方法，而invoke方法也做了两件事，1）根据class和method找到匹配的advisor并将其封装为interceptor放到interceptor链中；2）调用proceed方法，该方法会按我们的预期的执行顺序对方法进行织入 再小结：1.初始化advisorChain2.返回代理对象（jdk or cglib）3.调用方法时，根据方法&amp;类找到匹配的interceptor chain4.执行方法 with interceptor chain 这里尤其要注意Advice,Advisor,Interceptor之间的关系Advice = 织入逻辑 + 相对方法的织入位置Advisor = Advice + PointcutPointcut = ClassFilter + MethodMatcherAdvice : Advisor : Interceptor = 1 : 1 : n，注意一个advice可能继承了很多接口，那么它对应的Interceptor就可能有多个，Interceptor保证了方法的有序调用。 本文分析了基于ProxyFactoryBean实现的AOP，所有aop的实现原理都大体是这样的。但是这种方式的缺点是显而易见的，我们需要针对每个目标对象，给出他们各自对应的ProxyFactoryBean的配置，下篇文章我们看看有没有『智能』一些的方法解决这个问题。]]></content>
      <categories>
        <category>Spring源码阅读</category>
      </categories>
      <tags>
        <tag>Spring源码阅读</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring源码阅读(3)-IOC&AOP的桥梁FactoryBean]]></title>
    <url>%2F2018%2F11%2F16%2FSpring0-1%2F</url>
    <content type="text"><![CDATA[IOC&amp;AOP桥梁FactoryBean的实现分析的入口点是refresh方法，从finishBeanFactoryInitialization(beanFactory)方法开始12345678910public void refresh() throws BeansException, IllegalStateException &#123; //... finishBeanFactoryInitialization(beanFactory); //...&#125;protected void finishBeanFactoryInitialization(ConfigurableListableBeanFactory beanFactory) &#123; //... beanFactory.preInstantiateSingletons();&#125; 重点看preInstantiateSingletons方法，注意本文不分析普通的Bean，本文的关注重点是FactoryBean1234567891011121314151617181920212223242526272829303132333435@Overridepublic void preInstantiateSingletons() throws BeansException &#123; //... List&lt;String&gt; beanNames; synchronized (this.beanDefinitionMap) &#123; beanNames = new ArrayList&lt;String&gt;(this.beanDefinitionNames); &#125; for (String beanName : beanNames) &#123; RootBeanDefinition bd = getMergedLocalBeanDefinition(beanName); if (!bd.isAbstract() &amp;&amp; bd.isSingleton() &amp;&amp; !bd.isLazyInit()) &#123; if (isFactoryBean(beanName)) &#123; final FactoryBean&lt;?&gt; factory = (FactoryBean&lt;?&gt;) getBean(FACTORY_BEAN_PREFIX + beanName); boolean isEagerInit; if (System.getSecurityManager() != null &amp;&amp; factory instanceof SmartFactoryBean) &#123; isEagerInit = AccessController.doPrivileged(new PrivilegedAction&lt;Boolean&gt;() &#123; @Override public Boolean run() &#123; return ((SmartFactoryBean&lt;?&gt;) factory).isEagerInit(); &#125; &#125;, getAccessControlContext()); &#125; else &#123; isEagerInit = (factory instanceof SmartFactoryBean &amp;&amp; ((SmartFactoryBean&lt;?&gt;) factory).isEagerInit()); &#125; if (isEagerInit) &#123; getBean(beanName); &#125; &#125; else &#123; getBean(beanName); &#125; &#125; &#125;&#125; 我们按照顺序来看，这个方法其实已经是IOC进行bean实例化的入口了，这里首先对BeanDefinationMap加锁，找到所有的beanNames，接着去遍历beanNames，针对每个非抽象&amp;单例&amp;非懒加载的bean执行实例化工作。代码先来到isFactoryBean(beanName)12345678910@Overridepublic boolean isFactoryBean(String name) throws NoSuchBeanDefinitionException &#123; //... return isFactoryBean(beanName, getMergedLocalBeanDefinition(beanName));&#125;protected boolean isFactoryBean(String beanName, RootBeanDefinition mbd) &#123; Class&lt;?&gt; beanType = predictBeanType(beanName, mbd, FactoryBean.class); return (beanType != null &amp;&amp; FactoryBean.class.isAssignableFrom(beanType));&#125; 可以看到通过beanName以及对应的beanDefination来判断当前这个bean是否被定义为FactoryBean类型。由于我们分析的就是FactoryBean所以代码进入if代码块。1234if (isFactoryBean(beanName)) &#123; //如果是工厂bean 那么为其添加前缀&amp; final FactoryBean&lt;?&gt; factory = (FactoryBean&lt;?&gt;) getBean(FACTORY_BEAN_PREFIX + beanName);&#125; 一路跟踪代码直到下面，此时 name = &amp; + beanName1234567891011121314151617181920212223protected &lt;T&gt; T doGetBean( final String name, final Class&lt;T&gt; requiredType, final Object[] args, boolean typeCheckOnly) throws BeansException &#123; //以&amp;开头的name - &amp; = beanName final String beanName = transformedBeanName(name); //... if (mbd.isSingleton()) &#123; sharedInstance = getSingleton(beanName, new ObjectFactory&lt;Object&gt;() &#123; @Override public Object getObject() throws BeansException &#123; try &#123; return createBean(beanName, mbd, args); &#125; catch (BeansException ex) &#123; destroySingleton(beanName); throw ex; &#125; &#125; &#125;); bean = getObjectForBeanInstance(sharedInstance, name, beanName, mbd); &#125; //...&#125; 上面这段代码，首先这里的transformedBeanName，会将&amp;+beanName去除掉&amp;，去掉之后对真正的BeanFactory执行实例化操作，接着调用getSingleton，该方法是任何一个单例bean实例化都要经历的一个方法，这个方法很简单就是调用匿名类的getObject()方法createBean并获得对应的bean的实例 此时的sharedInstance是一个真正的BeanFactory实例!!!，这意味着容器中该bean的缓存一直都是一个BeanFactory实例。并且我们发现无论是从缓存中(Object sharedInstance = getSingleton(beanName))获取到bean还是自己实例化bean，之后都会调用getObjectForBeanInstance这个方法，而该方法就是FactoryBean的核心。 在该方法中对于BeanFactory类型的bean，如果是&amp;开头那么最终返回就是BeanFactory反之返回覆写的getObject方法的返回值，注意这里将name也一起传递到了方法中（用来标识是否&amp;开头）1234567891011121314151617181920212223protected Object getObjectForBeanInstance( Object beanInstance, String name, String beanName, RootBeanDefinition mbd) &#123; //... //1.是FactoryBean并且name以&amp;开头，直接return 2.非FactoryBean直接return if (!(beanInstance instanceof FactoryBean) || BeanFactoryUtils.isFactoryDereference(name)) &#123; return beanInstance; &#125; Object object = null; if (mbd == null) &#123; object = getCachedObjectForFactoryBean(beanName); &#125; if (object == null) &#123; FactoryBean&lt;?&gt; factory = (FactoryBean&lt;?&gt;) beanInstance; if (mbd == null &amp;&amp; containsBeanDefinition(beanName)) &#123; mbd = getMergedLocalBeanDefinition(beanName); &#125; boolean synthetic = (mbd != null &amp;&amp; mbd.isSynthetic()); object = getObjectFromFactoryBean(factory, beanName, !synthetic); &#125; return object;&#125; 第一个if判断是说如果当前这个bean不是FactoryBean或者当前的name带有&amp;那就直接return当前的beanInstance。如果是FactoryBean且前缀无&amp;那么代码继续向下走，调用getObjectFromFactoryBean(factory, beanName, !synthetic)123456789101112131415protected Object getObjectFromFactoryBean(FactoryBean&lt;?&gt; factory, String beanName, boolean shouldPostProcess) &#123; //... return doGetObjectFromFactoryBean(factory, beanName, shouldPostProcess);&#125;private Object doGetObjectFromFactoryBean( final FactoryBean&lt;?&gt; factory, final String beanName, final boolean shouldPostProcess) throws BeanCreationException &#123; Object object; //... object = factory.getObject(); //... return object; &#125; 到这里就真相大白了，调用了factoryBean这个bean的getObject方法。 Tips：application启动的时候，放到Map&lt;String, Object&gt; singletonObjects中的bean是FactoryBean而不是getObject()返回的对象，当调用getBean()时候如果入参前缀为&amp;此时会直接return map中存放的对象(即使有&amp;也会被摘除)；反之，如果无前缀，这时候会return FactoryBean.getObject()方法。下面这段代码是关键的关键。12345678protected Object getObjectForBeanInstance( Object beanInstance, String name, String beanName, RootBeanDefinition mbd) &#123; //... if (!(beanInstance instanceof FactoryBean) || BeanFactoryUtils.isFactoryDereference(name)) &#123; return beanInstance; &#125; //...&#125; 就是通过BeanFactoryUtils.isFactoryDereference来决定是返回FactoryBean还是其getObject()方法的返回值。 在xml中是无法配置&amp;开头的名字的bean的，也就是说在beanDefinationMap中FactoryBean类型的bean的beanName是没有&amp;前缀的；前文所说的&amp; + beanName是在人为调用getBean()时候完成的 再梳理下整个流程，当我们执行getBean(“&amp;MyFactory”)时候，name=&amp;MyFactory beanName=MyFactory根据MyFactory获取到了缓存中的FactoryBean类型的实例，接着调用getObjectForBeanInstance方法，而该方法的入参除了beanName还有name，根据name是否有前缀决定下一步的行为，如果有那么直接return缓存中的FactoryBean实例，反之返回getObject()的返回值。 小结：每次调用getBean，在获取到bean的实例之后（无论是从缓冲中拿还是自己实例化），都会执行getObjectForBeanInstance方法，该方法对于普通的bean以及(FactoryBean类型并且名字是&amp;开头)的bean会直接return；反之，也就是说对于FactoryBean类型且bean的名字开头不是&amp;的bean会调用其getObject方法。 容器中只有FactoryBean类型的bean 最后，写文章一定要多问自己几个为什么，就像文末说的那行代码决定是返回factoryBean还是其getObject()方法的返回值，这个如果不仔细思考可能就遗漏了，追代码追到getObject就结束了。本文分析了factoryBean，下篇文章去看看和factoryBean大有渊源的AOP。]]></content>
      <categories>
        <category>Spring源码阅读</category>
      </categories>
      <tags>
        <tag>Spring源码阅读</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring源码阅读(2)-IOC之bean的实例化]]></title>
    <url>%2F2018%2F11%2F15%2FSpring0-5%2F</url>
    <content type="text"><![CDATA[本文分析IOC的后半部分-bean的实例化 概览1234567891011121314151617181920// Register bean processors that intercept bean creation.registerBeanPostProcessors(beanFactory);// Initialize message source for this context.initMessageSource();// Initialize event multicaster for this context.initApplicationEventMulticaster();// Initialize other special beans in specific context subclasses.onRefresh();// Check for listener beans and register them.registerListeners();// Instantiate all remaining (non-lazy-init) singletons.finishBeanFactoryInitialization(beanFactory);// Last step: publish corresponding event.finishRefresh(); 上面是refresh方法后续的一些流程，我们先看看registerBeanPostProcessors。registerBeanPostProcessors方法首先是对BeanFactory中BeanPostProcessor类型bean进行注册，这里首先找到对应的beanName，再调用getBean方法，最后保存到特定的list中。重点看下finishBeanFactoryInitialization。 finishBeanFactoryInitialization从名字就能看出来，这个方法是用来完成对BeanFactory的初始化的。12345protected void finishBeanFactoryInitialization(ConfigurableListableBeanFactory beanFactory) &#123; //... // Instantiate all remaining (non-lazy-init) singletons. beanFactory.preInstantiateSingletons();&#125; 1234567891011121314151617181920212223242526272829303132333435@Overridepublic void preInstantiateSingletons() throws BeansException &#123; //... List&lt;String&gt; beanNames; synchronized (this.beanDefinitionMap) &#123; beanNames = new ArrayList&lt;String&gt;(this.beanDefinitionNames); &#125; for (String beanName : beanNames) &#123; RootBeanDefinition bd = getMergedLocalBeanDefinition(beanName); if (!bd.isAbstract() &amp;&amp; bd.isSingleton() &amp;&amp; !bd.isLazyInit()) &#123; if (isFactoryBean(beanName)) &#123; final FactoryBean&lt;?&gt; factory = (FactoryBean&lt;?&gt;) getBean(FACTORY_BEAN_PREFIX + beanName); boolean isEagerInit; if (System.getSecurityManager() != null &amp;&amp; factory instanceof SmartFactoryBean) &#123; isEagerInit = AccessController.doPrivileged(new PrivilegedAction&lt;Boolean&gt;() &#123; @Override public Boolean run() &#123; return ((SmartFactoryBean&lt;?&gt;) factory).isEagerInit(); &#125; &#125;, getAccessControlContext()); &#125; else &#123; isEagerInit = (factory instanceof SmartFactoryBean &amp;&amp; ((SmartFactoryBean&lt;?&gt;) factory).isEagerInit()); &#125; if (isEagerInit) &#123; getBean(beanName); &#125; &#125; else &#123; getBean(beanName); &#125; &#125; &#125;&#125; （1）首先对beanDefinitionMap加锁，找到所有的beanName；（2）遍历每个beanName找到对应的beanDefination；（3）对于非抽象&amp;单例&amp;非懒加载的bean执行实例化操作，对于FactoryBean类型的bean我们在关于FactoryBean那篇文章Spring源码阅读(3)-IOC&amp;AOP的桥梁FactoryBean中已经分析过了，我们本文只关心普通类型的bean。1234@Overridepublic Object getBean(String name) throws BeansException &#123; return doGetBean(name, null, null, false);&#125; 12345678910111213141516171819202122232425262728293031323334353637383940protected &lt;T&gt; T doGetBean( final String name, final Class&lt;T&gt; requiredType, final Object[] args, boolean typeCheckOnly) Object bean; throws BeansException &#123; //1.根据beanName获取对应的bean（单例只会创建一次） Object sharedInstance = getSingleton(beanName); if (sharedInstance != null &amp;&amp; args == null) &#123; bean = getObjectForBeanInstance(sharedInstance, name, beanName, null); &#125; else&#123; final RootBeanDefinition mbd = getMergedLocalBeanDefinition(beanName); checkMergedBeanDefinition(mbd, beanName, args); String[] dependsOn = mbd.getDependsOn(); //2.优先实例化dependOn的bean if (dependsOn != null) &#123; for (String dependsOnBean : dependsOn) &#123; if (isDependent(beanName, dependsOnBean)) &#123; throw new BeanCreationException("Circular depends-on relationship between '" + beanName + "' and '" + dependsOnBean + "'"); &#125; registerDependentBean(dependsOnBean, beanName); getBean(dependsOnBean); &#125; &#125; if (mbd.isSingleton()) &#123; sharedInstance = getSingleton(beanName, new ObjectFactory&lt;Object&gt;() &#123; @Override public Object getObject() throws BeansException &#123; try &#123; //3.实例化bean return createBean(beanName, mbd, args); &#125; &#125; &#125;); bean = getObjectForBeanInstance(sharedInstance, name, beanName, mbd); &#125; &#125; //... return (T) bean; &#125; 即使删除了一些边角料代码还是相对冗长，我们将代码分成3个部分（1）检测单例类型的bean是否已经被实例化，如果有直接return（2）否则，优先实例化depend-on属性指定的bean（3）接下来开始真正实例化当前的bean，调用的方法是createbean 在深入createBean之前我们先看看下面这个入口方法1234567public Object getSingleton(String beanName, ObjectFactory singletonFactory) &#123; //... beforeSingletonCreation(beanName); singletonObject = singletonFactory.getObject(); afterSingletonCreation(beanName); //...&#125; 在该方法中执行bean的实例化前，beforeSingletonCreation(beanName)会将当前beanName存放到一个创建ing的bean的set中，如果对set的add操作失败则表示产生了循环依赖直接抛出错误；而afterSingletonCreation(beanName)方法会将创建ing的beanName从set中移除。12345678910111213@Overrideprotected Object createBean(final String beanName, final RootBeanDefinition mbd, final Object[] args) throws BeanCreationException &#123; //... // Give BeanPostProcessors a chance to return a proxy instead of the target bean instance. Object bean = resolveBeforeInstantiation(beanName, mbd); if (bean != null) &#123; return bean; &#125; //... Object beanInstance = doCreateBean(beanName, mbd, args); return beanInstance;&#125; 这里我们会遇到bean实例化阶段的第1个扩展点，InstantiationAwareBeanPostProcessor的postProcessBeforeInstantiation方法。该方法在resolveBeforeInstantiation方法中被回调。接下来根据beanDefination创建bean的实例。注意接下来这个方法是及其重要的一个方法，涉及bean的实例化、注入以及初始化12345678910111213141516171819protected Object doCreateBean(final String beanName, final RootBeanDefinition mbd, final Object[] args) &#123; //... //实例化 BeanWrapper instanceWrapper = createBeanInstance(beanName, mbd, args); final Object bean = (instanceWrapper != null ? instanceWrapper.getWrappedInstance() : null); Class&lt;?&gt; beanType = (instanceWrapper != null ? instanceWrapper.getWrappedClass() : null); //... Object exposedObject = bean; try &#123; //注入 populateBean(beanName, mbd, instanceWrapper); if (exposedObject != null) &#123; //初始化 exposedObject = initializeBean(beanName, exposedObject, mbd); &#125; &#125; //... return exposedObject;&#125; 我们逐一去分析这三个步骤 实例化我们只看最普通的情况，其他的代码全部省略。1234567891011protected BeanWrapper createBeanInstance(String beanName, RootBeanDefinition mbd, Object[] args) &#123; //... return instantiateBean(beanName, mbd);&#125;protected BeanWrapper instantiateBean(final String beanName, final RootBeanDefinition mbd) &#123; //... beanInstance = getInstantiationStrategy().instantiate(mbd, beanName, parent); BeanWrapper bw = new BeanWrapperImpl(beanInstance); initBeanWrapper(bw); return bw;&#125; 我们可以看到在bean实例化的时候，采用的是策略模式，默认选择的是CglibSubclassingInstantiationStrategy，也就是说cglib的模式。我们跟进方法看到，instantiate这个方法是位于父类的（SimpleInstantiationStrategy），该方法首先判断是否有method的重写（look-up等）如果没有的话，那么默认采用反射的方式实例化bean；反之采用子类cglib的方式实例化bean。接下来将Bean包装成BeanWrapper供执行注入操作。 注入123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354protected void populateBean(String beanName, RootBeanDefinition mbd, BeanWrapper bw) &#123; PropertyValues pvs = mbd.getPropertyValues(); //... if (!mbd.isSynthetic() &amp;&amp; hasInstantiationAwareBeanPostProcessors()) &#123; for (BeanPostProcessor bp : getBeanPostProcessors()) &#123; if (bp instanceof InstantiationAwareBeanPostProcessor) &#123; InstantiationAwareBeanPostProcessor ibp = (InstantiationAwareBeanPostProcessor) bp; if (!ibp.postProcessAfterInstantiation(bw.getWrappedInstance(), beanName)) &#123; //... &#125; &#125; &#125; &#125; if (mbd.getResolvedAutowireMode() == RootBeanDefinition.AUTOWIRE_BY_NAME || mbd.getResolvedAutowireMode() == RootBeanDefinition.AUTOWIRE_BY_TYPE) &#123; MutablePropertyValues newPvs = new MutablePropertyValues(pvs); // Add property values based on autowire by name if applicable. if (mbd.getResolvedAutowireMode() == RootBeanDefinition.AUTOWIRE_BY_NAME) &#123; autowireByName(beanName, mbd, bw, newPvs); &#125; // Add property values based on autowire by type if applicable. if (mbd.getResolvedAutowireMode() == RootBeanDefinition.AUTOWIRE_BY_TYPE) &#123; autowireByType(beanName, mbd, bw, newPvs); &#125; pvs = newPvs; &#125; boolean hasInstAwareBpps = hasInstantiationAwareBeanPostProcessors(); boolean needsDepCheck = (mbd.getDependencyCheck() != RootBeanDefinition.DEPENDENCY_CHECK_NONE); if (hasInstAwareBpps || needsDepCheck) &#123; PropertyDescriptor[] filteredPds = filterPropertyDescriptorsForDependencyCheck(bw, mbd.allowCaching); if (hasInstAwareBpps) &#123; for (BeanPostProcessor bp : getBeanPostProcessors()) &#123; if (bp instanceof InstantiationAwareBeanPostProcessor) &#123; InstantiationAwareBeanPostProcessor ibp = (InstantiationAwareBeanPostProcessor) bp; pvs = ibp.postProcessPropertyValues(pvs, filteredPds, bw.getWrappedInstance(), beanName); if (pvs == null) &#123; return; &#125; &#125; &#125; &#125; if (needsDepCheck) &#123; checkDependencies(beanName, mbd, filteredPds, pvs); &#125; &#125; applyPropertyValues(beanName, mbd, bw, pvs);&#125; 这个方法也是很重要的，（1）这里首先引出了实例化阶段Spring扩展点的第2个回调InstantiationAwareBeanPostProcessor的postProcessAfterInstantiation方法；（2）接着如果bean的定义上有autowire_by_name或autowire_by_type属性，那么除了用户显式配置的property外，还要自动去寻找bean的类中『隐藏』的一些property（只针对bean），找到之后此时并未执行注入，而是简单地把propertyName和对应的bean放到最后一起注入使用的容器MutablePropertyValues pvs中。（3）接着来到了bean实例化阶段Spring扩展点的第3个回调InstantiationAwareBeanPostProcessor的postProcessPropertyValues方法，Ps对于@Resource,@Autowire注解都是在这里生效的(会提前执行inject操作)（4）最后，执行真正的注入操作。最终利用反射将属性注入到Bean中。 之前对最后一步的理解有问题，之前认为Bean内注入的Bean是一份深拷贝。刚才写了代码测试了下，对于单例的bean整个容器有且只有一份。 初始化123456789101112131415161718protected Object initializeBean(final String beanName, final Object bean, RootBeanDefinition mbd) &#123; //... invokeAwareMethods(beanName, bean); //... Object wrappedBean = bean; if (mbd == null || !mbd.isSynthetic()) &#123; wrappedBean = applyBeanPostProcessorsBeforeInitialization(wrappedBean, beanName); &#125; //... try &#123; invokeInitMethods(beanName, wrappedBean, mbd); &#125; //... if (mbd == null || !mbd.isSynthetic()) &#123; wrappedBean = applyBeanPostProcessorsAfterInitialization(wrappedBean, beanName); &#125; return wrappedBean;&#125; 我们看到这个方法也包含了很多的内容（1）如果当前的bean是和BeanFactory相关的Aware接口，那么调用setXxx方法，还记得在容器启动阶段ignoreDependencyInterface的那几个类吗？就是在这里调用对应的方法的。（BeanNameAware,BeanClassLoaderAware,BeanFactoryAware） 这里之前理解的有问题，这里要先判断当前的bean是否是指定的那几种类型，如果是则调用setXxx方法，而不是无脑调用 （2）接下来是Spring扩展点的第4个回调BeanPostProcessor的postProcessBeforeInitialization方法，在refresh方法中的第三个步骤prepareBeanFactory(beanFactory)中将ApplicationContextAwareProcessor注入到了BeanFactory中，而该类就是在这里执行对应的postProcessBeforeInitialization方法。这个方法调用了ApplicationContext特色的各种XxxAware的类的setXxx方法，比如ApplicationContextAware,ApplicationEventPublisherAware,ResourceLoaderAware 这里也是一样，只有当前bean是对应类型的接口，才调用对用的setXxx方法；和上面invokeAware方法不同的是，这里仍然要遍历容器内的所有BeanPostProcessor并执行对应的postProcessBeforeInitialization方法（类似渔网，有些鱼能过去，有些鱼会被捉住） （3）调用bean的初始化方法12345678910111213141516protected void invokeInitMethods(String beanName, final Object bean, RootBeanDefinition mbd) throws Throwable &#123; boolean isInitializingBean = (bean instanceof InitializingBean); if (isInitializingBean &amp;&amp; (mbd == null || !mbd.isExternallyManagedInitMethod("afterPropertiesSet"))) &#123; //... ((InitializingBean) bean).afterPropertiesSet(); &#125; if (mbd != null) &#123; String initMethodName = mbd.getInitMethodName(); if (initMethodName != null &amp;&amp; !(isInitializingBean &amp;&amp; "afterPropertiesSet".equals(initMethodName)) &amp;&amp; !mbd.isExternallyManagedInitMethod(initMethodName)) &#123; invokeCustomInitMethod(beanName, bean, mbd); &#125; &#125;&#125; 如果bean的类型是InitializingBean，那么优先调用afterPropertiesSet方法；随后调用自定义的init方法。（4）接下来是Spring扩展点的第5个回调BeanPostProcessor的postProcessAfterInitialization方法 至此，Bean已经实例化、注入、初始化完成。 最后祭出一张图。]]></content>
      <categories>
        <category>Spring源码阅读</category>
      </categories>
      <tags>
        <tag>Spring源码阅读</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring源码阅读(1)-IOC之容器启动阶段]]></title>
    <url>%2F2018%2F11%2F14%2FSpring0-4%2F</url>
    <content type="text"><![CDATA[契机：之前写了很多宏观分析Spring的文章，很多细节都不是很清楚，所以想开一个源码阅读的专栏，希望能够坚持下去。本来不打算写关于IOC的文章的，后来看了下之前写的文章，都相对浅薄没有深入源码分析，于是做本文。IOC部分打算分为两篇文章来分别讲解，本文是该系列之一『容器启动阶段』 入口点12345678public class EntranceApp &#123; public static void main(String[] args) throws Exception &#123; ApplicationContext context = new ClassPathXmlApplicationContext("beans.xml"); MyService b = (MyService)context.getBean("myService"); b.test(); &#125;&#125; 12345678public ClassPathXmlApplicationContext(String[] configLocations, boolean refresh, ApplicationContext parent) throws BeansException &#123; super(parent); setConfigLocations(configLocations); if (refresh) &#123; refresh(); &#125;&#125; 上面这个例子我们看到是在main方法中直接触发的Spring，而我们日常Web开发中是在哪儿&amp;由谁来执行上面这部分代码的呢？我们进入Java Web项目的web.xml中会发现这样一段代码123&lt;listener&gt; &lt;listener-class&gt;org.springframework.web.context.ContextLoaderListener&lt;/listener-class&gt;&lt;/listener&gt; 而上述xml配置是基于Spring的Web项目必备的，我们看看这个ContextLoaderListener干了啥？1234567891011121314public class ContextLoaderListener extends ContextLoader implements ServletContextListener &#123; @Override public void contextInitialized(ServletContextEvent event) &#123; initWebApplicationContext(event.getServletContext()); &#125; /** * Close the root web application context. */ @Override public void contextDestroyed(ServletContextEvent event) &#123; closeWebApplicationContext(event.getServletContext()); ContextCleanupListener.cleanupAttributes(event.getServletContext()); &#125;&#125; 该类实现了ServletContextListener接口，并且覆写了contextInitialized方法和contextDestroyed方法。前者会在容器启动阶段被调用，且在任何servlet or filter执行之前；后者会在容器销毁阶段被调用，且在全部servlet and filter被销毁后执行。在容器启动阶段，会执行ContextLoaderListener#contextInitialized方法，而该方法会调用下面这段逻辑12// wac is instance of ConfigurableWebApplicationContextwac.refresh(); 我们发现最终也执行到了大名鼎鼎的refresh方法中 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253@Overridepublic void refresh() throws BeansException, IllegalStateException &#123; synchronized (this.startupShutdownMonitor) &#123; // Prepare this context for refreshing. prepareRefresh(); // Tell the subclass to refresh the internal bean factory. ConfigurableListableBeanFactory beanFactory = obtainFreshBeanFactory(); // Prepare the bean factory for use in this context. prepareBeanFactory(beanFactory); try &#123; // Allows post-processing of the bean factory in context subclasses. postProcessBeanFactory(beanFactory); // Invoke factory processors registered as beans in the context. invokeBeanFactoryPostProcessors(beanFactory); // Register bean processors that intercept bean creation. registerBeanPostProcessors(beanFactory); // Initialize message source for this context. initMessageSource(); // Initialize event multicaster for this context. initApplicationEventMulticaster(); // Initialize other special beans in specific context subclasses. onRefresh(); // Check for listener beans and register them. registerListeners(); // Instantiate all remaining (non-lazy-init) singletons. finishBeanFactoryInitialization(beanFactory); // Last step: publish corresponding event. finishRefresh(); &#125; catch (BeansException ex) &#123; // Destroy already created singletons to avoid dangling resources. destroyBeans(); // Reset 'active' flag. cancelRefresh(ex); // Propagate exception to caller. throw ex; &#125; &#125;&#125; 本文主要分析的部分是invokeBeanFactoryPostProcessors(beanFactory)和它之前的部分1）准备重启(refresh)2）获得BeanFactory(这里获得的BeanFactory已经持有注册好的BeanDefination了)3）准备BeanFactory4）对获得到的BeanFactory进行后处理5）调用BeanFactoryPostProcessors 本文的重点在2，3，5 obtainFreshBeanFactory()refreshBeanFactory()12345678910111213141516171819@Overrideprotected final void refreshBeanFactory() throws BeansException &#123; if (hasBeanFactory()) &#123; destroyBeans(); closeBeanFactory(); &#125; try &#123; DefaultListableBeanFactory beanFactory = createBeanFactory(); beanFactory.setSerializationId(getId()); customizeBeanFactory(beanFactory); loadBeanDefinitions(beanFactory); synchronized (this.beanFactoryMonitor) &#123; this.beanFactory = beanFactory; &#125; &#125; catch (IOException ex) &#123; throw new ApplicationContextException("I/O error parsing bean definition source for " + getDisplayName(), ex); &#125;&#125; 首先，判断当前是否已经有BeanFactory了，如果有那么要把beans全部销毁并关闭BeanFactory。接着，创建BeanFactory最后，解析BeanDefination并注册到容器中 我们重点看后两个步骤 createBeanFactory()12345678910protected DefaultListableBeanFactory createBeanFactory() &#123; return new DefaultListableBeanFactory(getInternalParentBeanFactory());&#125;public AbstractAutowireCapableBeanFactory() &#123; //this constructor just return instance super(); ignoreDependencyInterface(BeanNameAware.class); ignoreDependencyInterface(BeanFactoryAware.class); ignoreDependencyInterface(BeanClassLoaderAware.class);&#125; 代码最后追到上面那里，首先调用父类的构造函数（空实现），接着将上面三个XXXAware类放到Set中，在该Set中的类的Xxx属性会由容器注入，且在依赖检查的时候会被忽略。依赖检查已经被Spring抛弃了。所以不必过于纠结这里 延伸知识：In Spring,you can use dependency checking feature to make sure the required properties have been set or injected.none – No dependency checking.simple – If any properties of primitive type (int, long,double…) and collection types (map, list..) have not been set, UnsatisfiedDependencyException will be thrown.objects – If any properties of object type have not been set, UnsatisfiedDependencyException will be thrown.all – If any properties of any type have not been set, an UnsatisfiedDependencyExceptionwill be thrown. Tips：在Spring中名为xxxAware的bean，一般都会调用setXxx()以实现目标的注入。 loadBeanDefinitions(beanFactory)12345678910111213141516@Overrideprotected void loadBeanDefinitions(DefaultListableBeanFactory beanFactory) throws BeansException, IOException &#123; // Create a new XmlBeanDefinitionReader for the given BeanFactory. XmlBeanDefinitionReader beanDefinitionReader = new XmlBeanDefinitionReader(beanFactory); // Configure the bean definition reader with this context's // resource loading environment. beanDefinitionReader.setEnvironment(this.getEnvironment()); beanDefinitionReader.setResourceLoader(this); beanDefinitionReader.setEntityResolver(new ResourceEntityResolver(this)); // Allow a subclass to provide custom initialization of the reader, // then proceed with actually loading the bean definitions. initBeanDefinitionReader(beanDefinitionReader); loadBeanDefinitions(beanDefinitionReader);&#125; 这里首先为BeanFactory新建了一个BeanDefinationReader（此时Reader持有BeanFactory）；接着填充Reader的一些属性；最后调用loadBeanDefinitions(beanDefinitionReader)，构建document、解析并注册BeanDefination12345678//XmlBeanDefinationReader.classprotected int doLoadBeanDefinitions(InputSource inputSource, Resource resource) throws BeanDefinitionStoreException &#123; //... Document doc = doLoadDocument(inputSource, resource); return registerBeanDefinitions(doc, resource); //...&#125; doLoadDocument(inputSource, resource)根据输入流封装的inputSource以及resource去构建document，这里具体的代码就不去分析了，重点看这个。Ps：以后有document解析的需求可以参考这里的代码 registerBeanDefinitions(doc, resource)1234567891011121314151617181920212223242526272829303132333435363738public int registerBeanDefinitions(Document doc, Resource resource) throws BeanDefinitionStoreException &#123; BeanDefinitionDocumentReader documentReader = createBeanDefinitionDocumentReader(); documentReader.setEnvironment(this.getEnvironment()); int countBefore = getRegistry().getBeanDefinitionCount(); documentReader.registerBeanDefinitions(doc, createReaderContext(resource)); return getRegistry().getBeanDefinitionCount() - countBefore;&#125;@Overridepublic void registerBeanDefinitions(Document doc, XmlReaderContext readerContext) &#123; this.readerContext = readerContext; logger.debug("Loading bean definitions"); Element root = doc.getDocumentElement(); doRegisterBeanDefinitions(root);&#125;protected void parseBeanDefinitions(Element root, BeanDefinitionParserDelegate delegate)&#123; if (delegate.isDefaultNamespace(root)) &#123; NodeList nl = root.getChildNodes(); for (int i = 0; i &lt; nl.getLength(); i++) &#123; Node node = nl.item(i); if (node instanceof Element) &#123; Element ele = (Element) node; //常规bean的解析操作 if (delegate.isDefaultNamespace(ele)) &#123; parseDefaultElement(ele, delegate); &#125; //自定义注解的解析，eg:&lt;aop&gt; else &#123; delegate.parseCustomElement(ele); &#125; &#125; &#125; &#125; else &#123; delegate.parseCustomElement(root); &#125;&#125; 我们层层递进追到了上面这个方法中，我们在分析自动化aop的时候发现就是在这里的delegate.parseCustomElement(ele)对&lt;aop:aspectj-autoproxy/&gt;进行解析的。我们先来看默认的情况123456789101112131415private void parseDefaultElement(Element ele, BeanDefinitionParserDelegate delegate) &#123; if (delegate.nodeNameEquals(ele, IMPORT_ELEMENT)) &#123; importBeanDefinitionResource(ele); &#125; else if (delegate.nodeNameEquals(ele, ALIAS_ELEMENT)) &#123; processAliasRegistration(ele); &#125; else if (delegate.nodeNameEquals(ele, BEAN_ELEMENT)) &#123; processBeanDefinition(ele, delegate); &#125; else if (delegate.nodeNameEquals(ele, NESTED_BEANS_ELEMENT)) &#123; // recurse doRegisterBeanDefinitions(ele); &#125;&#125; 我们直接进入到processBeanDefinition(ele, delegate)12345678910111213141516protected void processBeanDefinition(Element ele, BeanDefinitionParserDelegate delegate) &#123; BeanDefinitionHolder bdHolder = delegate.parseBeanDefinitionElement(ele); if (bdHolder != null) &#123; bdHolder = delegate.decorateBeanDefinitionIfRequired(ele, bdHolder); try &#123; // Register the final decorated instance. BeanDefinitionReaderUtils.registerBeanDefinition(bdHolder, getReaderContext().getRegistry()); &#125; catch (BeanDefinitionStoreException ex) &#123; getReaderContext().error("Failed to register bean definition with name '" + bdHolder.getBeanName() + "'", ele, ex); &#125; // Send registration event. getReaderContext().fireComponentRegistered(new BeanComponentDefinition(bdHolder)); &#125;&#125; 这里主要分为了两个步骤，一个是BeanDefination的真正解析；一个是BeanDefination的注册。具体的解析步骤这里就不进行分析了。而BeanDefination的注册其实就是将beanName和beanDefination的映射关系保存在map中，this.beanDefinitionMap.put(beanName, beanDefinition);Ps:显然beanDefinitionMap是被BeanFactory持有的 getBeanFactory()这个步骤就是简单地将上一步骤中实例化以及初始化过得beanFactory返回。return this.beanFactory; prepareBeanFactory123456789101112131415protected void prepareBeanFactory(ConfigurableListableBeanFactory beanFactory) &#123; // Tell the internal bean factory to use the context's class loader etc. beanFactory.setBeanClassLoader(getClassLoader()); beanFactory.setBeanExpressionResolver(new StandardBeanExpressionResolver()); beanFactory.addPropertyEditorRegistrar(new ResourceEditorRegistrar(this, getEnvironment())); // Configure the bean factory with context callbacks. beanFactory.addBeanPostProcessor(new ApplicationContextAwareProcessor(this)); beanFactory.ignoreDependencyInterface(ResourceLoaderAware.class); beanFactory.ignoreDependencyInterface(ApplicationEventPublisherAware.class); beanFactory.ignoreDependencyInterface(MessageSourceAware.class); beanFactory.ignoreDependencyInterface(ApplicationContextAware.class); beanFactory.ignoreDependencyInterface(EnvironmentAware.class); //...&#125; 这里我们看到添加了一个BeanPostProcessor-ApplicationContextAwareProcessor；并且同样忽略了很多以Aware结尾的接口。关于ApplicationContextAwareProcessor我们会在Bean的实例化阶段再去分析。 invokeBeanFactoryPostProcessors(beanFactory)12345678910111213141516public static void invokeBeanFactoryPostProcessors( ConfigurableListableBeanFactory beanFactory, List&lt;BeanFactoryPostProcessor&gt; beanFactoryPostProcessors) &#123; //... List&lt;BeanFactoryPostProcessor&gt; regularPostProcessors = new LinkedList&lt;BeanFactoryPostProcessor&gt;(); for (BeanFactoryPostProcessor postProcessor : beanFactoryPostProcessors) &#123; regularPostProcessors.add(postProcessor); &#125; invokeBeanFactoryPostProcessors(regularPostProcessors, beanFactory);&#125;private static void invokeBeanFactoryPostProcessors( Collection&lt;? extends BeanFactoryPostProcessor&gt; postProcessors, ConfigurableListableBeanFactory beanFactory) &#123; for (BeanFactoryPostProcessor postProcessor : postProcessors) &#123; postProcessor.postProcessBeanFactory(beanFactory); &#125;&#125; 对每个普通的BeanFactoryProcessor都调用了postProcessBeanFactory方法。至此容器的启动阶段结束。 小结：1）如果当前存在BeanFactory那么销毁所有Beans并关闭BeanFactory2）实例化BeanFactory，与此同时指定了一些不会被依赖检查的类型的bean3）将xml解析为Document（xml-&gt;inputsource&amp;resource）4）利用BeanDefinitionParserDelegate对Document进行解析，解析的结果是BeanDefination（对特殊注解比如aspectJ aop的解析也在这里）5）对解析出来的BeanDefination进行注册（其实就是放到了一个ConcurrentHashMap中，该map被BeanFactory持有）6）回调BeanFactoryPostProcessor类型的bean的postProcessBeanFactory方法 以上です]]></content>
      <categories>
        <category>Spring源码阅读</category>
      </categories>
      <tags>
        <tag>Spring源码阅读</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Netty源码分析(6)-HashedWheelTimer]]></title>
    <url>%2F2018%2F11%2F08%2FNetty-7%2F</url>
    <content type="text"><![CDATA[Netty正牌哈希轮，还是自顶向下的去分析 宏观概览123456789101112131415161718192021222324252627282930313233343536HashedWheelTimer //inner class HashedWheelBucket HashedWheelTimeout Worker //method HashedWheelTimer HashedWheelTimer HashedWheelTimer HashedWheelTimer HashedWheelTimer HashedWheelTimer HashedWheelTimer createWheel newTimeout normalizeTicksPerWheel start stop //field cancelledTimeouts leak leakDetector logger mask startTime startTimeInitialized tickDuration timeouts wheel worker WORKER_STATE_INIT WORKER_STATE_SHUTDOWN WORKER_STATE_STARTED WORKER_STATE_UPDATER workerState workerThread 这是HashedWheelTimer的组成部分，这里有三个重要的类：1）HashedWheelBucket，表示哈希桶，是任务链的容器；包含一些添加、移除timeout的方法2）HashedWheelTimeout，持有task，本身是链式结构；包含一些cancel,expire方法3）Worker，实现Runnable，是真正干活的线程，它的run方法执行了对应tick的任务，并且让哈希轮转起来 方法：1）createWheel:初始化哈希轮，主要就是初始化HashedWheelBucket[] wheel2）newTimeout:新建一个延时任务，该方法会调用start方法让哈希轮动起来3）normalizeTicksPerWheel:令哈希轮数组的数为2的n次方，方便取余运算4）start:让哈希轮开始运转5）stop:中断worker线程并返回未处理的任务 属性：1）long startTime:哈希轮开始运转的时间，nanotime形式这里简单拓展一下nanoTime相关的知识，我们看到jdk给出的关于System.nanotime的注解是123456This method can only be used to measure elapsed time and is not related to any other notion of system or wall-clock time. The value returned representsnanoseconds since some fixed but arbitrary origin time (perhaps in the future, sovalues may be negative). The same origin is used by all invocations of this methodin an instance of a Java virtual machine; other virtual machine instances arelikely to use a different origin. 这个方法只能测量流逝的时间，和真正的时间并无对应关系，所以无法相互转换。这个返回值表示的是相对某个随意的时间点而言的纳秒值，而这个时间点还有可能是未来，所以可能返回的纳秒值是个负数。在同一个jvm中调用该方法的时间锚点（某个随意的时间点）都是同一个。2）long tickDuration: 指针每次运转的间隔时间3）Queue timeouts: 刚刚new的timeOut会被放到这里，到下个tick才会分配到指定的bucket4）HashedWheelBucket[] wheel: 哈希轮实体，是一个bucket数组5）Runnable worker: 真正执行timeOut延时任务的线程6）workerState: worker线程的状态 深入细节初始化工作从哈希轮的构造函数开始123456789101112131415161718public HashedWheelTimer( ThreadFactory threadFactory, long tickDuration, TimeUnit unit, int ticksPerWheel, boolean leakDetection) &#123; //... wheel = createWheel(ticksPerWheel); mask = wheel.length - 1; this.tickDuration = unit.toNanos(tickDuration); if (this.tickDuration &gt;= Long.MAX_VALUE / wheel.length) &#123; throw new IllegalArgumentException(String.format( "tickDuration: %d (expected: 0 &lt; tickDuration in nanos &lt; %d", tickDuration, Long.MAX_VALUE / wheel.length)); &#125; workerThread = threadFactory.newThread(worker); leak = leakDetection || !workerThread.isDaemon() ? leakDetector.open(this) : null;&#125; 大体的流程是，先进行参数的校验，接着创建哈希轮，再初始化thread，最后进行泄露检测相关的工作。这里重点关注createWheel方法123456789private static HashedWheelBucket[] createWheel(int ticksPerWheel) &#123; //... ticksPerWheel = normalizeTicksPerWheel(ticksPerWheel); HashedWheelBucket[] wheel = new HashedWheelBucket[ticksPerWheel]; for (int i = 0; i &lt; wheel.length; i ++) &#123; wheel[i] = new HashedWheelBucket(); &#125; return wheel;&#125; 先调用了normalizeTicksPerWheel(ticksPerWheel)这个方法很简单，就是将ticksPerWheel二次方化，比如传7那么这里将返回8；接着对wheel进行了初始化，至此初始化工作就结束了，接着看看哈希轮如何运转的。 哈希轮的运转哈希轮的使用套路一般是，先new一个哈希轮，接着调用下面这个方法就ok了，所以这里就是哈希轮运转的入口1234567891011121314151617public Timeout newTimeout(TimerTask task, long delay, TimeUnit unit) &#123; if (task == null) &#123; throw new NullPointerException("task"); &#125; if (unit == null) &#123; throw new NullPointerException("unit"); &#125; start(); // Add the timeout to the timeout queue which will be processed on the next tick. // During processing all the queued HashedWheelTimeouts will be added to the correct HashedWheelBucket. //这里要注意，deadline是相对startTime需要经历的时间长短 long deadline = System.nanoTime() + unit.toNanos(delay) - startTime; HashedWheelTimeout timeout = new HashedWheelTimeout(this, task, deadline); timeouts.add(timeout); return timeout;&#125; 这个方法先是进行了必要参数的校验，接着调用了start方法，最后new了一个timeOut并放到了Queue&lt;HashedWheelTimeout&gt;timeouts中。那么为啥在这里调用start方法？这也是netty将优化做到丧心病狂的体现，因为如果没有任何任务，哈希轮『空转』是没有任何意义的，所以至少有一个timeout，哈希轮才转起来。我们接下来就先看看start方法123456789101112131415161718192021222324public void start() &#123; switch (WORKER_STATE_UPDATER.get(this)) &#123; case WORKER_STATE_INIT: if (WORKER_STATE_UPDATER.compareAndSet(this, WORKER_STATE_INIT, WORKER_STATE_STARTED)) &#123; workerThread.start(); &#125; break; case WORKER_STATE_STARTED: break; case WORKER_STATE_SHUTDOWN: throw new IllegalStateException("cannot be started once stopped"); default: throw new Error("Invalid WorkerState"); &#125; // Wait until the startTime is initialized by the worker. while (startTime == 0) &#123; try &#123; startTimeInitialized.await(); &#125; catch (InterruptedException ignore) &#123; // Ignore - it will be ready very soon. &#125; &#125;&#125; 首先通过CAS更新了workerState的状态（无锁化的体现），接着异步调用了workerThread.run()，我们先不去看这个方法，先看后续的流程，代码最后是一个while循环，循环结束的条件是startTime!=0，这里用这个条件表示start成功，接下来具体到worker中再看。1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556public void run() &#123; // Initialize the startTime. startTime = System.nanoTime(); //严谨的一批，真不知道nanoTime返回0的概率是多少，我觉得比国战100连胜还要难 //这里startTime有可能返回0，但是我们用0表示并未初始化，所以这里要换一个值 if (startTime == 0) &#123; // We use 0 as an indicator for the uninitialized value here, so make sure it's not 0 when initialized. startTime = 1; &#125; // Notify the other threads waiting for the initialization at start(). // 这里是一个countDownLatch，当这里countDown之后， // start方法中while循环中的await就可以被唤醒了，进而newTimeout也被唤醒， // 可以计算delayTime了 startTimeInitialized.countDown(); //哈希轮开始转 do &#123; //这个方法可能会sleep一段时间，之后返回的是系统当前的时刻，而这个当前时刻=指针移动的时刻 //本文后续会分析这个方法 final long deadline = waitForNextTick(); if (deadline &gt; 0) &#123; //tick取余，定位到index int idx = (int) (tick &amp; mask); //处理被cancel的任务 processCancelledTasks(); //根据index定位到bucket HashedWheelBucket bucket = wheel[idx]; //把timeouts queue中的任务 放到属于它的bucket中 每次取100000个 transferTimeoutsToBuckets(); //重点方法，执行当前bucket中到期的任务 bucket.expireTimeouts(deadline); tick++; &#125; &#125; while (WORKER_STATE_UPDATER.get(HashedWheelTimer.this) == WORKER_STATE_STARTED); //代码执行到这里，哈希轮已经不转了~ // Fill the unprocessedTimeouts so we can return them from stop() method. //这里是从每个bucket中移除未被处理的任务 for (HashedWheelBucket bucket: wheel) &#123; bucket.clearTimeouts(unprocessedTimeouts); &#125; //这里是从timeouts queue中移除未被处理的任务 for (;;) &#123; HashedWheelTimeout timeout = timeouts.poll(); if (timeout == null) &#123; break; &#125; if (!timeout.isCancelled()) &#123; unprocessedTimeouts.add(timeout); &#125; &#125; processCancelledTasks();&#125; 按照上面代码的顺序，逐一去看每个方法12345678910111213141516171819202122232425262728293031323334//Worker.classprivate long waitForNextTick() &#123; //相对startTime来讲,下次指针移动还要过多久 long deadline = tickDuration * (tick + 1); for (;;) &#123; //当前时间相对startTime，已经开始了多久 final long currentTime = System.nanoTime() - startTime; //距离指针开动还有多久ms，假如是deadline-currentTime= 2000002，通过+一个999999，此时sleepTimeMs = 3 long sleepTimeMs = (deadline - currentTime + 999999) / 1000000; //sleep之后，代码会进入到这里,正常情况return的就是当前时间 if (sleepTimeMs &lt;= 0) &#123; if (currentTime == Long.MIN_VALUE) &#123; return -Long.MAX_VALUE; &#125; else &#123; return currentTime; &#125; &#125; //这里是说windows系统可能会有些bug，要把sleep的时间转为10的倍数 if (PlatformDependent.isWindows()) &#123; sleepTimeMs = sleepTimeMs / 10 * 10; &#125; //开始沉睡，直到哈希轮指针可以移动 try &#123; Thread.sleep(sleepTimeMs); &#125; catch (InterruptedException ignored) &#123; if (WORKER_STATE_UPDATER.get(HashedWheelTimer.this) == WORKER_STATE_SHUTDOWN) &#123; return Long.MIN_VALUE; &#125; &#125; &#125;&#125; 下面去看expireTimeouts123456789101112131415161718192021222324252627282930313233public void expireTimeouts(long deadline) &#123; HashedWheelTimeout timeout = head; // process all timeouts //从头到尾去遍历链表 while (timeout != null) &#123; boolean remove = false; //这里round是回合的意思，一个新的任务会定位到，几圈零几个格子。这里几圈就是round if (timeout.remainingRounds &lt;= 0) &#123; //its time to execute task if (timeout.deadline &lt;= deadline) &#123; //稍后看看这个方法 timeout.expire(); &#125; else &#123; // The timeout was placed into a wrong slot. This should never happen. throw new IllegalStateException(String.format( "timeout.deadline (%d) &gt; deadline (%d)", timeout.deadline, deadline)); &#125; remove = true; &#125; else if (timeout.isCancelled()) &#123; remove = true; &#125; else &#123; timeout.remainingRounds --; &#125; // store reference to next as we may null out timeout.next in the remove block. HashedWheelTimeout next = timeout.next; //将任务从链表中移除 if (remove) &#123; remove(timeout); &#125; timeout = next; &#125;&#125; 终于到了，任务执行的地方了123456789101112131415//HashedWheelTimeout.classpublic void expire() &#123; if (!compareAndSetState(ST_INIT, ST_EXPIRED)) &#123; return; &#125; try &#123; //这个task是用户传递进来的 task.run(this); &#125; catch (Throwable t) &#123; if (logger.isWarnEnabled()) &#123; logger.warn("An exception was thrown by " + TimerTask.class.getSimpleName() + '.', t); &#125; &#125;&#125; 最后再来看看，stop方法12345678910111213141516171819202122232425262728293031323334353637383940414243public Set&lt;Timeout&gt; stop() &#123; //确保当前线程不是任务线程，防止恶意任务使得哈希轮停止 if (Thread.currentThread() == workerThread) &#123; throw new IllegalStateException( HashedWheelTimer.class.getSimpleName() + ".stop() cannot be called from " + TimerTask.class.getSimpleName()); &#125; //这里的CAS操作成功后，run方法内的while循环便会停止，接着去收集那些未被处理的任务 if (!WORKER_STATE_UPDATER.compareAndSet(this, WORKER_STATE_STARTED, WORKER_STATE_SHUTDOWN)) &#123; // workerState can be 0 or 2 at this moment - let it always be 2. WORKER_STATE_UPDATER.set(this, WORKER_STATE_SHUTDOWN); if (leak != null) &#123; leak.close(); &#125; return Collections.emptySet(); &#125; //中断worker线程 boolean interrupted = false; //这里while循环的目的是，让worker线程执行完收集未处理的任务=!workerThread.isAlive() while (workerThread.isAlive()) &#123; //todo 这里要研究下线程的中断 见文章末尾 //这里执行调用中断方法的目的是，使得处于休眠状态（等待下次tick动）的worker线程醒过来（waitForNextTick）然后去收集未处理的任务 workerThread.interrupt(); try &#123; workerThread.join(100); &#125; catch (InterruptedException ignored) &#123; interrupted = true; &#125; &#125; if (interrupted) &#123; Thread.currentThread().interrupt(); &#125; if (leak != null) &#123; leak.close(); &#125; //返回未被处理的任务们 return worker.unprocessedTimeouts();&#125; 至此，Netty中大名鼎鼎的哈希轮就分析完毕了，还是有很多值得借鉴的地方的~ 最最后，简单看下Thread.interrupt()方法，其实这个方法是没啥用的一个方法，为啥这么说呢。举个例子，有线程t1和线程t2，如果t1调用t2.interrupt()方法，t2线程不会立即中断并且根本无视t1线程的这次调用（甚至有点想吃黄焖鸡米饭）只有一种情况可以让其中断，比如t2调用了sleep方法，这时候如果t1调用t2.interrupt()，t2的sleep方法会抛出异常，注意如果此时t2还未执行到sleep方法，那么当其执行到的时候亦会抛出异常。 没有任何语言方面的需求一个被中断的线程应该终止。中断一个线程只是为了引起该线程的注意，被中断线程可以决定如何应对中断。某些线程非常重要，以至于它们应该不理会中断，而是在处理完抛出的异常之后继续执行。 总结大体分为两个步骤一、new HashWheelTimer(xx) createWheel，初始化数组 threadFactory.newThread(worker)，初始化workerThread 二、timer.newTimeout(xx) 执行start方法让哈希轮开转（只有第一个任务会执行成功，其他直接return） 执行start方法后，计算任务的预计执行时间 初始化当前哈希轮任务 将任务存放到timeout queue中，等待调度 话分两边，再来看看哈希轮转起来之后干了啥 先是初始化starttime（=哈希轮开始运转的nanotime） sleep until 执行下次tick移动 处理被cancel的任务 分配两次tick移动之间，放到timeout queue中的任务到指定的bucket中 处理当前tick指向的bucket中到期的任务 不断执行上述过程，直到哈希轮状态不为STARTED]]></content>
      <categories>
        <category>Netty源码分析</category>
      </categories>
      <tags>
        <tag>Netty</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Netty源码分析(5)-WriteAndFlush]]></title>
    <url>%2F2018%2F11%2F02%2FNetty-6%2F</url>
    <content type="text"><![CDATA[WriteAndFlushWriteAndFlush这里分为两个步骤，Write,Flush Write回忆上文追到的unsafe的write方法，这个方法就是本文的入口。123456789101112131415161718192021222324public final void write(Object msg, ChannelPromise promise) &#123; assertEventLoop(); ChannelOutboundBuffer outboundBuffer = this.outboundBuffer; if (outboundBuffer == null) &#123; safeSetFailure(promise, WRITE_CLOSED_CHANNEL_EXCEPTION); ReferenceCountUtil.release(msg); return; &#125; int size; try &#123; msg = filterOutboundMessage(msg); size = pipeline.estimatorHandle().size(msg); if (size &lt; 0) &#123; size = 0; &#125; &#125; catch (Throwable t) &#123; safeSetFailure(promise, t); ReferenceCountUtil.release(msg); return; &#125; outboundBuffer.addMessage(msg, size, promise);&#125; 一共分为四个步骤（1）确认当前是reactor线程（2）过滤msg1234567891011121314151617protected final Object filterOutboundMessage(Object msg) &#123; if (msg instanceof ByteBuf) &#123; ByteBuf buf = (ByteBuf) msg; if (buf.isDirect()) &#123; return msg; &#125; return newDirectBuffer(buf); &#125; if (msg instanceof FileRegion) &#123; return msg; &#125; throw new UnsupportedOperationException( "unsupported message type: " + StringUtil.simpleClassName(msg) + EXPECTED_TYPES);&#125; 如果msg既不是ByteBuf类型也不是FileRegion类型的，那么直接抛出异常。这里还有一个值得注意的，方法会将所有非直接内存转换成直接内存DirectBuffer（3）估算msg的size（4）调用outboundBuffer.addMessage(msg, size, promise)12345678910111213141516//ChannelOutboundBufferpublic void addMessage(Object msg, int size, ChannelPromise promise) &#123; Entry entry = Entry.newInstance(msg, size, total(msg), promise); if (tailEntry == null) &#123; flushedEntry = null; tailEntry = entry; &#125; else &#123; Entry tail = tailEntry; tail.next = entry; tailEntry = entry; &#125; if (unflushedEntry == null) &#123; unflushedEntry = entry; &#125; incrementPendingOutboundBytes(size, false);&#125; 这里涉及几个指针，tailEntry,flushedEntry,unFlushedEntry，在执行N次上述方法后，指针之间如下图所示 Flush在pipeline上调用的flush最终都会落到head节点上1234//HeadContextpublic void flush(ChannelHandlerContext ctx) throws Exception &#123; unsafe.flush();&#125; 123456789101112//AbstractUnsafepublic final void flush() &#123; assertEventLoop(); ChannelOutboundBuffer outboundBuffer = this.outboundBuffer; if (outboundBuffer == null) &#123; return; &#125; outboundBuffer.addFlush(); flush0();&#125; 这里主要看addFlush和flush0方法123456789101112131415161718public void addFlush() &#123; Entry entry = unflushedEntry; if (entry != null) &#123; if (flushedEntry == null) &#123; flushedEntry = entry; &#125; do &#123; flushed ++; if (!entry.promise.setUncancellable()) &#123; int pending = entry.cancel(); decrementPendingOutboundBytes(pending, false, true); &#125; entry = entry.next; &#125; while (entry != null); unflushedEntry = null; &#125;&#125; 结合上面那个图，我们知道该方法执行完毕后，unFlushedEntry和flushedEntry位置对调了。接着去看flush0123protected void flush0() &#123; doWrite(outboundBuffer);&#125; 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051protected void doWrite(ChannelOutboundBuffer in) throws Exception &#123; int writeSpinCount = -1; boolean setOpWrite = false; for (;;) &#123; Object msg = in.current(); if (msg instanceof ByteBuf) &#123; ByteBuf buf = (ByteBuf) msg; int readableBytes = buf.readableBytes(); if (readableBytes == 0) &#123; in.remove(); continue; &#125; boolean done = false; long flushedAmount = 0; if (writeSpinCount == -1) &#123; writeSpinCount = config().getWriteSpinCount(); &#125; for (int i = writeSpinCount - 1; i &gt;= 0; i --) &#123; int localFlushedAmount = doWriteBytes(buf); if (localFlushedAmount == 0) &#123; setOpWrite = true; break; &#125; flushedAmount += localFlushedAmount; if (!buf.isReadable()) &#123; done = true; break; &#125; &#125; in.progress(flushedAmount); if (done) &#123; in.remove(); &#125; else &#123; break; &#125; &#125; if (done) &#123; in.remove(); &#125; else &#123; break; &#125; &#125; else &#123; throw new Error(); &#125; &#125; incompleteWrite(setOpWrite); &#125; （1）通过current方法拿到第一个需要flush的节点1234567public Object current() &#123; Entry entry = flushedEntry; if (entry == null) &#123; return null; &#125; return entry.msg;&#125; (2)获取自旋的次数，后文会提到为啥要自旋(3)调用doWriteBytes(buf)1234protected int doWriteBytes(ByteBuf buf) throws Exception &#123; final int expectedWrittenBytes = buf.readableBytes(); return buf.readBytes(javaChannel(), expectedWrittenBytes);&#125; 这里解释下为啥要自旋，因为doWriteBytes并不保证一次会将entry的数据读取完毕，所以需要不断循环直到!buf.isReadable()。我们看到这里调用了ByteBuf的readBytes方法将数据写到对应的channel中，官方文档如是说，Transfers this buffer’s data to the specified stream starting at the current。（4）删除节点，就是普通的链表删除头节点的套路1234567891011private void removeEntry(Entry e) &#123; if (-- flushed == 0) &#123; flushedEntry = null; if (e == tailEntry) &#123; tailEntry = null; unflushedEntry = null; &#125; &#125; else &#123; flushedEntry = e.next; &#125;&#125; 几个值得注意的点：1.netty中的缓冲区中的byteBuf是DirectByteBuf2.调用write方法实际是把数据写到了单向链表中3.调用flush才是真正的把数据写到socket缓冲区 参考链接：https://www.jianshu.com/p/feaeaab2ce56]]></content>
      <categories>
        <category>Netty源码分析</category>
      </categories>
      <tags>
        <tag>Netty</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Netty源码分析(4)-Pipeline]]></title>
    <url>%2F2018%2F11%2F01%2FNetty-5%2F</url>
    <content type="text"><![CDATA[Netty中的pipeline pipeline的实例化回忆下Netty系列的第二篇文章，在创建channel的过程中有几个副产品1234567// AbstractChannelprotected AbstractChannel(Channel parent) &#123; this.parent = parent; id = newId(); unsafe = newUnsafe(); pipeline = newChannelPipeline();&#125; 这里的pipeline就是本文研究的重点，我们进入到newChannelPipeline中一探究竟12345678910111213141516// AbstractChannelprotected DefaultChannelPipeline newChannelPipeline() &#123; return new DefaultChannelPipeline(this);&#125;//DefaultChannelPipelineprotected DefaultChannelPipeline(Channel channel) &#123; this.channel = ObjectUtil.checkNotNull(channel, "channel"); succeededFuture = new SucceededChannelFuture(channel, null); voidPromise = new VoidChannelPromise(channel, true); tail = new TailContext(this); head = new HeadContext(this); head.next = tail; tail.prev = head;&#125; 在构造函数中，pipeline持有channel，并且实例化了pipeline中首尾两个阀门：AbstractChannelHandlerContext类型的head和tail，并构造了双向链表。其中head和tail是双向链表的两个哨兵，这样编码时候就可以保证当前节点前后都不为空，不需要一些无谓的判断 pipeline#addLast123456789101112131415161718192021222324252627282930313233343536373839404142434445464748//DefaultChannelPipelinepublic final ChannelPipeline addLast(ChannelHandler... handlers) &#123; return addLast(null, handlers);&#125;public final ChannelPipeline addLast(EventExecutorGroup executor, ChannelHandler... handlers) &#123; if (handlers == null) &#123; throw new NullPointerException("handlers"); &#125; for (ChannelHandler h: handlers) &#123; if (h == null) &#123; break; &#125; addLast(executor, null, h); &#125; return this;&#125;public final ChannelPipeline addLast(EventExecutorGroup group, String name, ChannelHandler handler) &#123; final AbstractChannelHandlerContext newCtx; synchronized (this) &#123; checkMultiplicity(handler); newCtx = newContext(group, filterName(name, handler), handler); addLast0(newCtx); if (!registered) &#123; newCtx.setAddPending(); callHandlerCallbackLater(newCtx, true); return this; &#125; EventExecutor executor = newCtx.executor(); if (!executor.inEventLoop()) &#123; newCtx.setAddPending(); executor.execute(new Runnable() &#123; @Override public void run() &#123; callHandlerAdded0(newCtx); &#125; &#125;); return this; &#125; &#125; callHandlerAdded0(newCtx); return this;&#125; 我们只关心最后的方法内容，分成了四个步骤（1）检查handler是否重复，如果handler并未配sharable注解且被重复使用那么要抛出错误（2）新建节点，为handler取一个独一无二的名字，并设置handlerContext的属性初值，包括name,pipeline,executor,inbound,outbound（3）添加节点，就是普通的双向列表插入节点的套路1234567private void addLast0(AbstractChannelHandlerContext newCtx) &#123; AbstractChannelHandlerContext prev = tail.prev; newCtx.prev = prev; newCtx.next = tail; prev.next = newCtx; tail.prev = newCtx;&#125; （4）执行回调，如果不在reactor线程中，那么将回调抽象成一次任务，放到reactor线程的任务池中；反之，直接调用callHandlerAdded0方法1234private void callHandlerAdded0(final AbstractChannelHandlerContext ctx) &#123; ctx.handler().handlerAdded(ctx); ctx.setAddComplete();&#125; pipeline#remove1234567891011121314151617181920212223242526272829@Overridepublic final ChannelPipeline remove(ChannelHandler handler) &#123; remove(getContextOrDie(handler)); return this;&#125;private AbstractChannelHandlerContext remove(final AbstractChannelHandlerContext ctx) &#123; assert ctx != head &amp;&amp; ctx != tail; synchronized (this) &#123; remove0(ctx); if (!registered) &#123; callHandlerCallbackLater(ctx, false); return ctx; &#125; EventExecutor executor = ctx.executor(); if (!executor.inEventLoop()) &#123; executor.execute(new Runnable() &#123; @Override public void run() &#123; callHandlerRemoved0(ctx); &#125; &#125;); return ctx; &#125; &#125; callHandlerRemoved0(ctx); return ctx;&#125; 和添加节点的套路很像，但是简单了很多，只有两个步骤。（1）移除节点123456private static void remove0(AbstractChannelHandlerContext ctx) &#123; AbstractChannelHandlerContext prev = ctx.prev; AbstractChannelHandlerContext next = ctx.next; prev.next = next; next.prev = prev;&#125; （2）执行回调handler#handlerRemoved，和添加类似如果不在reactor线程也要添加到任务池 fire pipelineintBound回忆下服务端启动过程后，eventLoop不断循环做三件事情，其中第二件事情负责监听IO事件12345678910private void processSelectedKey(SelectionKey k, AbstractNioChannel ch) &#123; //... if ((readyOps &amp; (SelectionKey.OP_READ | SelectionKey.OP_ACCEPT)) != 0 || readyOps == 0) &#123; unsafe.read(); if (!ch.isOpen()) &#123; // Connection already closed - no need to handle write. return; &#125; &#125;&#125; 我们只关注READ事件，继续跟进unsafe中123456//AbstractNioByteUnsafepublic final void read() &#123; //... allocHandle.lastBytesRead(doReadBytes(byteBuf)); pipeline.fireChannelRead(byteBuf);&#125; 终于fire了，继续跟进1234567//DefaultChannelPipeline@Overridepublic final ChannelPipeline fireChannelRead(Object msg) &#123; //channel维度的事件传递会贯穿整个链表 AbstractChannelHandlerContext.invokeChannelRead(head, msg); return this;&#125; 注意上面代码中的head。继续跟进1234567891011121314//AbstractChannelHandlerContextstatic void invokeChannelRead(final AbstractChannelHandlerContext next, Object msg) &#123; final Object m = next.pipeline.touch(ObjectUtil.checkNotNull(msg, "msg"), next); EventExecutor executor = next.executor(); if (executor.inEventLoop()) &#123; next.invokeChannelRead(m); &#125; //... &#125;private void invokeChannelRead(Object msg) &#123; //... ((ChannelInboundHandler) handler()).channelRead(this,msg); //...&#125; 这里要回忆下，this表示的是head，而msg表示的是读取到的byteBuf，接下来看看head的channelRead方法1234//DefaultChannelPipeline.HeadContextpublic void channelRead(ChannelHandlerContext ctx, Object msg) throws Exception &#123; ctx.fireChannelRead(msg);&#125; 12345//AbstractChannelHandlerContextpublic ChannelHandlerContext fireChannelRead(final Object msg) &#123; invokeChannelRead(findContextInbound(), msg); return this;&#125; 重点看下findContextInbound方法，看名字就知道是寻找下一个Inbound类型的handler Ps：因为现在是读事件，所以走的是Inbound这条路显然这样调用下去，最后一个被调用的是tail。1234//DefaultChannelPipeline.TailContextpublic void channelRead(ChannelHandlerContext ctx, Object msg) throws Exception &#123; onUnhandledInboundMessage(msg);&#125; 这里值得注意的是1）InBoundHandler一般为被动响应事件，消息传递方向为链表的正向，from head2）OutBoundHandler一般为主动调用，消息传递方向为链表的反向，from tail3）一般针对channel or pipeline 维度的事件传递都是从头or从尾开始的，会贯穿整个链表4）而针对单个context的事件传播都是从当前节点开始的 outBound一般在push系统中都会主动调用channel.writeAndFlush(msg)方法，我们的入口也是这里。12345678//AbstractChannelpublic ChannelFuture writeAndFlush(Object msg) &#123; return pipeline.writeAndFlush(msg);&#125;//DefaultChannelPipelinepublic final ChannelFuture writeAndFlush(Object msg) &#123; return tail.writeAndFlush(msg);&#125; 注意这里的入口是tail。12345678910111213141516//AbstractChannelHandlerContextpublic ChannelFuture writeAndFlush(Object msg, ChannelPromise promise) &#123; //... write(msg, true, promise); //...&#125;private void write(Object msg, boolean flush, ChannelPromise promise) &#123; AbstractChannelHandlerContext next = findContextOutbound(); final Object m = pipeline.touch(msg, next); if (flush) &#123; next.invokeWriteAndFlush(m, promise); &#125; else &#123; next.invokeWrite(m, promise); &#125;&#125; 因为我们调用的是writeAndFlush所以这里传入的flush参数为true。可以看到方法中的findContextOutbound方法，和上面的findContextInbound方法类似，这个方法用来寻找Outbound类型的handler。接着调用下面这个方法12345678private void invokeWriteAndFlush(Object msg, ChannelPromise promise) &#123; if (invokeHandler()) &#123; invokeWrite0(msg, promise); invokeFlush0(); &#125; else &#123; writeAndFlush(msg, promise); &#125;&#125; 这里我们研究的是事件的传播，所以重点关注invokeWrite0方法1234567private void invokeWrite0(Object msg, ChannelPromise promise) &#123; try &#123; ((ChannelOutboundHandler) handler()).write(this, msg, promise); &#125; catch (Throwable t) &#123; notifyOutboundHandlerException(t, promise); &#125;&#125; 和inBound类似，OutBound也会不断调用handler的write方法，直到到达head，下面看看head做了啥1234//DefaultChannelPipeline.HeadContextpublic void write(ChannelHandlerContext ctx, Object msg, ChannelPromise promise) throws Exception &#123; unsafe.write(msg, promise);&#125; 123456789101112131415161718192021222324public final void write(Object msg, ChannelPromise promise) &#123; assertEventLoop(); ChannelOutboundBuffer outboundBuffer = this.outboundBuffer; if (outboundBuffer == null) &#123; safeSetFailure(promise, WRITE_CLOSED_CHANNEL_EXCEPTION); ReferenceCountUtil.release(msg); return; &#125; int size; try &#123; msg = filterOutboundMessage(msg); size = pipeline.estimatorHandle().size(msg); if (size &lt; 0) &#123; size = 0; &#125; &#125; catch (Throwable t) &#123; safeSetFailure(promise, t); ReferenceCountUtil.release(msg); return; &#125; outboundBuffer.addMessage(msg, size, promise);&#125; 我们就追到这里，后续的放到下篇文章去分析。 总结，read,write,flush等操作都是先执行当前handler覆写的方法，如果没有覆写那么默认调用父类ChannelOutHandlerAdapter,ChannelInHandlerAdapter的对应方法，而父类中的默认方法就是简单地将事件进行传递 pipeline exceptioninBound12345678//AbstractChannelHandlerContextprivate void invokeChannelRead(Object msg) &#123; try &#123; ((ChannelInboundHandler) handler()).channelRead(this, msg); &#125; catch (Throwable t) &#123; notifyHandlerException(t); &#125;&#125; 我们看catch代码块中的方法1234567//AbstractChannelHandlerContextprivate void notifyHandlerException(Throwable cause) &#123; invokeExceptionCaught(cause);&#125;private void invokeExceptionCaught(final Throwable cause) &#123; handler().exceptionCaught(this, cause); &#125; 这里可以看到都是直接调用的handler的exceptionCaught方法，如果当前handler没有覆写这个方法，那么会去调用下面这个方法12345//ChannelHandlerAdapter @Overridepublic void exceptionCaught(ChannelHandlerContext ctx, Throwable cause) throws Exception &#123; ctx.fireExceptionCaught(cause);&#125; 进到这个方法那就是一直向后推了，直到到了某个覆写了exceptionCaught方法的handler，这也是一般将解决Exc的handler配置在tail之前的一个原因。 Outbound看过了Inbound，再去看Outbound就很简单了，我们发现即使Outbound是从tail向前传递，但是也一样是调用notifyHandlerException方法，那么问题就很简单了，无论在哪个handler抛出异常，异常都是正向传递的且会忽略handler本身的类型。比如IA IB IC OA OB OC其中IB抛出了异常，那么会遍历IB IC OA OB OC如果异常一直被向后传递，最终会fall到tail节点，该节点会将异常打印到控制台 参考链接：https://www.jianshu.com/p/6efa9c5fa702https://www.jianshu.com/p/087b7e9a27a2]]></content>
      <categories>
        <category>Netty源码分析</category>
      </categories>
      <tags>
        <tag>Netty</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Netty源码分析(3)-新连接接入]]></title>
    <url>%2F2018%2F10%2F25%2FNetty-3%2F</url>
    <content type="text"><![CDATA[上文介绍了服务端启动的过程，这篇文章介绍接收新连接的过程。 (BOSS)EventLoop的启动过程上回书说到在注册channel过程中有这样的一段代码，当时说是BossEventLoop reactor线程启动的入口，我们从这里去分析本文。1234567891011121314public final void register(EventLoop eventLoop, final ChannelPromise promise) &#123; //... AbstractChannel.this.eventLoop = eventLoop; if (eventLoop.inEventLoop()) &#123; register0(promise); &#125; else &#123; eventLoop.execute(new Runnable() &#123; @Override public void run() &#123; register0(promise); &#125; &#125;); &#125;&#125; 这段代码就是将channel的注册行为抽象成了一次异步的任务，而eventLoop在执行execute方法时候也同样启动了。我们这里的channel.eventLoop指的是当前channel对应的单个EventLoop(我们这里分析的是BossEventLoop)，而SingleThreadEventExecutor是EventLoop的父类，所以我们根据eventLoop的execute方法追溯到了这里12345678910111213141516//SingleThreadEventExecutor.class@Overridepublic void execute(Runnable task) &#123; //... boolean inEventLoop = inEventLoop(); if (inEventLoop) &#123; addTask(task); &#125; else &#123; startThread(); addTask(task); if (isShutdown() &amp;&amp; removeTask(task)) &#123; reject(); &#125; &#125; //...&#125; 简单地调用栈是这样的startThread()-&gt;doStartThread()-&gt;run()-&gt;NioEventLoop#runNioEventLoop的run方法是一个很重要的方法详情见Netty源码分析(2)-Reactor线程模型，这是NioEventLoop启动的地方，在这里进行无限循环。执行NioEventLoop的三板斧：轮询注册在selector上的IO事件、处理IO事件、同步执行异步task。而新连接的接入过程就位于第二步骤（处理IO事件），我们接下来去分析。 (BOSS)EventLoop#processSelectedKeys()从无参的processSelectedKeys()追到了processSelectedKeys(SelectionKey k, AbstractNioChannel ch)中1234567private void processSelectedKey(SelectionKey k, AbstractNioChannel ch) &#123; //... if ((readyOps &amp; (SelectionKey.OP_READ | SelectionKey.OP_ACCEPT)) != 0 || readyOps == 0) &#123; unsafe.read(); &#125; //...&#125; 这里有一个疑问，和当前channel绑定的eventLoop是如何确定的？答：在channel注册过程中，通过next()方法从bossGroup中随机选择的一个eventLoop 我们看到此时当EventLoop（BossGroup中的一员）轮询到了ACCEPT事件会调用unsafe.read()123456789101112131415161718192021public void read() &#123; assert eventLoop().inEventLoop(); final ChannelPipeline pipeline = pipeline(); final RecvByteBufAllocator.Handle allocHandle = unsafe().recvBufAllocHandle(); do &#123; int localRead = doReadMessages(readBuf); if (localRead == 0) &#123; break; &#125; if (localRead &lt; 0) &#123; closed = true; break; &#125; &#125; while (allocHandle.continueReading()); int size = readBuf.size(); for (int i = 0; i &lt; size; i ++) &#123; pipeline.fireChannelRead(readBuf.get(i)); &#125; readBuf.clear(); pipeline.fireChannelReadComplete();&#125; 这里重点关注doReadMessages和fireChannelRead方法 doReadMessages123456789101112131415161718//NioServerSocketChannel.classprotected int doReadMessages(List&lt;Object&gt; buf) throws Exception &#123; SocketChannel ch = javaChannel().accept(); try &#123; if (ch != null) &#123; buf.add(new NioSocketChannel(this, ch)); return 1; &#125; &#125; catch (Throwable t) &#123; logger.warn("Failed to create a new channel from an accepted socket.", t); try &#123; ch.close(); &#125; catch (Throwable t2) &#123; logger.warn("Failed to close a socket.", t2); &#125; &#125; return 0;&#125; 第一行代码就是jdk nio接收新连接的代码，因为是监听到了ACCEPT事件所以这里会立刻返回一个SocketChannel，这个也是nio中的channle。我们可以看到buf是一个list容器，再将socketChannel放到容器之前，还要对其进行包装（包装为netty中的channel类型）。下面去看看具体咋包装的1234public NioSocketChannel(Channel parent, SocketChannel socket) &#123; super(parent, socket); config = new NioSocketChannelConfig(this, socket.socket());&#125; 诶等一等，这个代码看着有点眼熟啊，这就是Netty源码分析(1)-服务端启动过程中new Channel那段代码，所以就不去看了，大概创建了这样几个东西 channel, channelConfig, 保存了变量到成员变量, channelId,unsafe,pipeline 不过这里还是有一些不同的，这里注册的事件是SelectionKey.OP_READ，而服务端channel注册的是SelectionKey.OP_ACCEPT事件；而且这里的channel是NioScoketChannel fireChannelRead将readBuf填充满NioSocketChannel后接下来轮询这个list去调用下面这个方法pipeline.fireChannelRead(readBuf.get(i));我们当前pipeline中除了head和tail还包含一个特殊的ChannelInboundHandler-ServerBootstrapAcceptor，我们暂且忽略在handler链前方的handler直接看ServerBootstrapAcceptor，而ServerBootstrapAcceptor我们在上篇文章已经分析过了，这里再回忆下12345678910111213141516171819202122232425262728public void channelRead(ChannelHandlerContext ctx, Object msg) &#123; final Channel child = (Channel) msg; child.pipeline().addLast(childHandler); for (Entry&lt;ChannelOption&lt;?&gt;, Object&gt; e: childOptions) &#123; try &#123; if (!child.config().setOption((ChannelOption&lt;Object&gt;) e.getKey(), e.getValue())) &#123; logger.warn("Unknown channel option: " + e); &#125; &#125; catch (Throwable t) &#123; logger.warn("Failed to set a channel option: " + child, t); &#125; &#125; for (Entry&lt;AttributeKey&lt;?&gt;, Object&gt; e: childAttrs) &#123; child.attr((AttributeKey&lt;Object&gt;) e.getKey()).set(e.getValue()); &#125; try &#123; childGroup.register(child).addListener(new ChannelFutureListener() &#123; @Override public void operationComplete(ChannelFuture future) throws Exception &#123; if (!future.isSuccess()) &#123; forceClose(child, future.cause()); &#125; &#125; &#125;); &#125; catch (Throwable t) &#123; forceClose(child, t); &#125;&#125; 首先Acceptor接收到的信息强制转换为channel，接着执行childChannel初始化的三板斧，最后将获取到的channel注册到childGroup上，其实到这里这整个流程已经串起来了。 Ps这里的channel实际上是NioSocketChannel（1）Server端调用bind，执行NioServerSocketChannel的实例化、初始化和注册（注册到BossGroup,这个时候触发了boss eventLoop的启动，开始无限循环执行那三件事情），在初始化过程中塞进了一个特殊的handler-ServerBootstrapAcceptor；接着执行bind方法和地址绑定到一起（2）当有新连接到来的时候，会被eventLoop捕获到。捕获到的channel最开始还是原生的nio channel需要包装成netty中的channel，接着针对接收到的每个channel调用pipeline上的handler中对应的方法，这个时候执行到了ServerBootstrapAcceptor的channelRead方法，而在这个方法中将channel注册到了childGroup（注册到WorkerGroup,这个时候触发了worker eventLoop的启动，开始无限循环执行那三件事情）Ps:接收到的channel以后就托管给childGroup了，对应事件的轮询也要由childGroup中的eventLoop来执行 不过为了『追求卓越』我们还是深入看下这里的register方法我们追着代码走到了这里12345//MultithreadEventLoopGroup.class @Overridepublic ChannelFuture register(Channel channel) &#123; return next().register(channel);&#125; 先来看看这个next()1234@Overridepublic EventExecutor next() &#123; return chooser.next();&#125; 这里的chooser是一个EventExecutorChooser类，它的next方法从eventGroup中选择一个eventLoop，找到了eventLoop之后就是普通的channel注册过程了，和上篇文章中讲到的注册过程完全相同，最终都会执行selectionKey = javaChannel().register(eventLoop().selector, 0, this);这段代码是经典的nio注册channel的代码，可见netty其实说白了就是对jdk nio的封装，使他更为好用罢了Ps可见无论是通过channel向eventLoop注册or eventLoop注册channel最终都是执行上面那段code 极其重要！！！Worker的reactor线程在注册NioSocketChannel过程启动（之后就去无限循环它的那三件事情），以后该channel的读写事件就可以由worker的reactor线程来处理了 在register的过程中会执行很多用户定义的回调方法，比如invokeHandlerAddedIfNeeded()会调用pipeline上handler的handlerAdded方法，注意这个时候已经是childGroup中了，所以根据用户初始时候的输入12345.childHandler(new ChannelInitializer&lt;SocketChannel&gt;() &#123; @Override public void initChannel(SocketChannel ch) throws Exception &#123; &#125;&#125;) 此时要执行ChannelInitializer的handlerAdded方法，而该方法会调用我们组装过程中定义的initChannel方法，这个方法调用完成后ChannelInitializer会将自己从pipeline中删除。 小结：1.new server channel2.init server channel(3 steps)— 2.1 in the 3rd step add Acceptor to bossGroup pipeline as a handler— Ps:Acceptor#channelRead init newChannel which just connects,and register it on childGroup3.register server channel to bossGroup— 3.1 boss reactor thread start(do 3 things)— 3.2 fireManyMethods such as invokeHandlerAddedIfNeeded,fireChannelRegistered4.bind address to server channel5.new connect come6.bossGroup catch new channel and package it as a Netty Channel7.fire server channel pipeline8.run into Acceptor#channelRead and register new channel to childGroup(in this process child reactor thread start) 参考链接：https://www.jianshu.com/p/0242b1d4dd21]]></content>
      <categories>
        <category>Netty源码分析</category>
      </categories>
      <tags>
        <tag>Netty</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Netty源码分析(2)-Reactor线程模型]]></title>
    <url>%2F2018%2F10%2F24%2FNetty-4%2F</url>
    <content type="text"><![CDATA[正式进入Netty核心组件：Reactor线程模型看完本文应该明白这些问题：1.为什么channel的操作是线程安全的？2.NioEventLoop都干了啥？3.如何解决jdk nio的空轮询的bug？ 主流程在开始之前先贴张图关于NioEventLoop,EventLoopGroup,Selector之间的关系如图1所示。EventLoopGroup:NioEventLoop:Thread:Selector = 1:n:n:n下面开始分析源码，还记得在上篇文章提到过，当服务端执行channel的注册时会执行这行代码channel.eventLoop().execute(xxx)，而这行代码是NioEventLoop的入口。12345678910111213141516//SingleThreadEventExecutor.class@Overridepublic void execute(Runnable task) &#123; //... boolean inEventLoop = inEventLoop(); if (inEventLoop) &#123; addTask(task); &#125; else &#123; startThread(); addTask(task); if (isShutdown() &amp;&amp; removeTask(task)) &#123; reject(); &#125; &#125; //...&#125; 这里先判断当前线程是否在EventLoop绑定的线程中（就是简单地进行Thread的比对），在服务端刚启动的时候代码会执行到else部分，我们看下startThread方法12345678//SingleThreadEventExecutor.classprivate void startThread() &#123; if (STATE_UPDATER.get(this) == ST_NOT_STARTED) &#123; if (STATE_UPDATER.compareAndSet(this, ST_NOT_STARTED, ST_STARTED)) &#123; doStartThread(); &#125; &#125;&#125; 这里也是Netty高性能之无锁化的一个体现，通过CAS保证即使在多线程竞争的时候也是线程安全。12345678910111213//SingleThreadEventExecutor.classprivate void doStartThread() &#123; //... executor.execute(new Runnable() &#123; @Override public void run() &#123; thread = Thread.currentThread(); SingleThreadEventExecutor.this.run(); success = true; &#125; &#125; //...&#125; 这里可以看到线程（我们称为Reactor线程）是由run方法创建的，这里就是简单地将当前线程设置到SingleThreadEventExecutor中，值得注意的是这个当前线程指的是由线程池调度分配给执行run方法的线程。继续跟进run方法，这时候到了一个及其核心的地方。123456789101112131415161718@Overrideprotected void run() &#123; for (;;) &#123; try &#123; switch (selectStrategy.calculateStrategy(selectNowSupplier, hasTasks())) &#123; case SelectStrategy.CONTINUE: continue; case SelectStrategy.SELECT: select(wakenUp.getAndSet(false)); if (wakenUp.get()) &#123; selector.wakeup(); &#125; default: // fallthrough &#125; processSelectedKeys(); runAllTasks(ioTime * (100 - ioRatio) / ioRatio); &#125; It’s time to show the 2rd picture.如图2所示，reactor线程一直在循环做三件事情 select process selected keys run tasks select1234select(wakenUp.getAndSet(false));if (wakenUp.get()) &#123; selector.wakeup();&#125; 在进行select阻塞操作之前，先将wakenUp置为false。下面那段代码Netty源码注解写的天花乱坠，但我还是没理解。。。这里留个问题吧，会有多个线程调用selector.select()操作吗？如果有这种场景就能理解了。接着去看看select方法，这个方法很长，这里逐步去分析12345678910111213141516private void select(boolean oldWakenUp) throws IOException &#123; Selector selector = this.selector; try &#123; int selectCnt = 0; long currentTimeNanos = System.nanoTime(); long selectDeadLineNanos = currentTimeNanos + delayNanos(currentTimeNanos); for (;;) &#123; long timeoutMillis = (selectDeadLineNanos - currentTimeNanos + 500000L) / 1000000L; if (timeoutMillis &lt;= 0) &#123; if (selectCnt == 0) &#123; selector.selectNow(); selectCnt = 1; &#125; break; &#125; //... 这段代码，还没进入真正的select操作，首先记录了当前时间以及定时任务中到期时间最近任务的执行时间，接着代码进入了无限循环中，首先判断如果当前时间已经过了最近任务执行时间0.5ms以上，如果这时候还没执行过select or selectNow操作这时候执行一次立刻返回的selectNow操作，之后退出无限循环。12345678if (hasTasks() &amp;&amp; wakenUp.compareAndSet(false, true)) &#123; selector.selectNow(); selectCnt = 1; break;&#125;int selectedKeys = selector.select(timeoutMillis);selectCnt ++; hasTask方法判断TaskQueue是否为空，如果不为空且CAS操作成功，那么立即执行一次selectNow并跳出无限循环。如果不满足上述条件，代码来到了selector.select(timeout)方法。这是jdk的nio中的方法，带有超时时间，而且当其他线程调用selector.wakeUp时候会唤醒阻塞。执行完select操作后会对selectCnt+1。 自问自答这里之前有些疑惑，为何要+0.5ms ？后来想了下，如果没有这个0.5ms，那么select操作在没有定时任务的情况，可能会立刻停止阻塞甚至没有捕获任何事件。 再考虑另外一个极端情况，定时任务的delayNanos(currentTimeNanos)相当大，那么select操作岂不是要阻塞很久，那整个netty可能都玩不转了？这里看了下源码，当select操作阻塞的时候是可以通过wakeup操作唤醒的，而添加任务的代码如下，这里是会唤醒select操作的123if (!addTaskWakesUp &amp;&amp; wakesUpForTask(task)) &#123; wakeup(inEventLoop);&#125; 1234567if (selectedKeys != 0 || oldWakenUp || wakenUp.get() || hasTasks() || hasScheduledTasks()) &#123; // - Selected something, // - waken up by user, or // - the task queue has a pending task. // - a scheduled task is ready for processing break;&#125; 这里注解写的很清晰，当轮询到IO事件（select something）、被用户所唤醒、旧的wakenUp为true、任务队列有任务、定时任务要被执行，这时候要执行break操作，退出无限循环。1234567891011121314151617181920212223long time = System.nanoTime();if (time - TimeUnit.MILLISECONDS.toNanos(timeoutMillis) &gt;= currentTimeNanos) &#123; // timeoutMillis elapsed without anything selected. selectCnt = 1;&#125; else if (SELECTOR_AUTO_REBUILD_THRESHOLD &gt; 0 &amp;&amp; selectCnt &gt;= SELECTOR_AUTO_REBUILD_THRESHOLD) &#123; // The selector returned prematurely many times in a row. // Rebuild the selector to work around the problem. logger.warn( "Selector.select() returned prematurely &#123;&#125; times in a row; rebuilding Selector &#123;&#125;.", selectCnt, selector); rebuildSelector(); selector = this.selector; // Select again to populate selectedKeys. selector.selectNow(); selectCnt = 1; break;&#125;currentTimeNanos = time;&#125; // end loop 这段代码就是专门为了解决jdk nio bug而写的。简单查了下这个bug，就是说当selector.select阻塞的时候，即使没有轮询到io事件也没有其他线程调用wakeup也会退出阻塞，将会无法退出循环导致cpu使用率一直是100%。Ps：啥事儿没发生还不阻塞让cpu空转 不断执行无意义的循环 异常途径提前结束阻塞达到一定次数视为触发了bug代码最开始就是检测上次的select操作是否阻塞了timeoutMills这么长，如果是那就能够说明没触发那个bug，所以将selectCnt置为1。相反阻塞时间不超过timeoutMills的次数达到了SELECTOR_AUTO_REBUILD_THRESHOLD次，那么这时候视为触发了bug，解决的方法很简单就是rebuild一个selector，将attachment(也就是channel),interestOps一起转移到新selector上。 简单总结下select操作1）先去判断是否有快到期的定时任务，如果有那么selectNow并break2）判断taskQueue中是否有任务，如果有那么selectNow()并break3）执行selector.select(timeout)，这里timeout表示定时任务的到期时间4）最后通过巧妙的方式绕过了jdk的bug processSelectedKeys1234567private void processSelectedKeys() &#123; if (selectedKeys != null) &#123; processSelectedKeysOptimized(selectedKeys.flip()); &#125; else &#123; processSelectedKeysPlain(selector.selectedKeys()); &#125;&#125; 这里Netty对selectedKeys进行了优化，原生的是一个hashSet，这里将其存到了数组中，方便访问与遍历。123456789101112131415161718192021222324252627282930313233private void processSelectedKeysOptimized(SelectionKey[] selectedKeys) &#123; for (int i = 0;; i ++) &#123; final SelectionKey k = selectedKeys[i]; if (k == null) &#123; break; &#125; selectedKeys[i] = null; final Object a = k.attachment(); if (a instanceof AbstractNioChannel) &#123; processSelectedKey(k, (AbstractNioChannel) a); &#125; else &#123; @SuppressWarnings("unchecked") NioTask&lt;SelectableChannel&gt; task = (NioTask&lt;SelectableChannel&gt;) a; processSelectedKey(k, task); &#125; if (needsToSelectAgain) &#123; for (;;) &#123; i++; if (selectedKeys[i] == null) &#123; break; &#125; selectedKeys[i] = null; &#125; selectAgain(); selectedKeys = this.selectedKeys.flip(); i = -1; &#125; &#125;&#125; 遍历selectedKeys，找到key和对应的channel，之后传入到processSelectedKey(k, (AbstractNioChannel) a);中处理，最后判断是否需要重新select，如果需要那么执行selectAgain()先来看processSelectedKey12345678910111213141516171819202122private void processSelectedKey(SelectionKey k, AbstractNioChannel ch) &#123; try &#123; int readyOps = k.readyOps(); if ((readyOps &amp; SelectionKey.OP_CONNECT) != 0) &#123; int ops = k.interestOps(); ops &amp;= ~SelectionKey.OP_CONNECT; k.interestOps(ops); unsafe.finishConnect(); &#125; if ((readyOps &amp; SelectionKey.OP_WRITE) != 0) &#123; ch.unsafe().forceFlush(); &#125; if ((readyOps &amp; (SelectionKey.OP_READ | SelectionKey.OP_ACCEPT)) != 0 || readyOps == 0) &#123; unsafe.read(); if (!ch.isOpen()) &#123; return; &#125; &#125; &#125;&#125; 可以看到这段代码就是根据轮询到的不同IO事件执行不同的操作。unsafe.read()会在后面的文章中分析。接着判断needsToSelectAgain，对于每个NioEventLoop而言，每隔256个channel从selector上移除的时候，就标记 needsToSelectAgain 为true，执行一次『洗牌』。这里就不进行具体的分析了。 runAllTasks12345678910111213141516171819202122232425262728293031 protected boolean runAllTasks(long timeoutNanos) &#123; fetchFromScheduledTaskQueue(); Runnable task = pollTask(); if (task == null) &#123; afterRunningAllTasks(); return false; &#125; final long deadline = ScheduledFutureTask.nanoTime() + timeoutNanos; long runTasks = 0; long lastExecutionTime; for (;;) &#123; safeExecute(task); runTasks ++; if ((runTasks &amp; 0x3F) == 0) &#123; lastExecutionTime = ScheduledFutureTask.nanoTime(); if (lastExecutionTime &gt;= deadline) &#123; break; &#125; &#125; task = pollTask(); if (task == null) &#123; lastExecutionTime = ScheduledFutureTask.nanoTime(); break; &#125; &#125; afterRunningAllTasks(); this.lastExecutionTime = lastExecutionTime; return true;&#125; (1)将到期的定时任务从一个优先队列（PriorityQueue）转移到MPSC（多生产者单消费者,Thread-Safe BlockingQueue）队列中(2)从taskQueue中获取一个task(3)计算本次任务循环的截止时间，因为需要留时间给select操作和process操作(4)进入循环(5)执行任务，如果被执行的任务到了64个（nanotime的获取是一个相对昂贵的操作，所以不是每次循环都去判断），此时去判断是否已经timeout，如果是的话那么直接break(6)如果不符合上述要求，那么继续从taskQueue中拉取任务并执行 总结，Netty中的任务机制是其线程安全的一大保证。当非reactor线程执行一些并发操作时，都会把这次操作抽象为一个任务并将任务放到reactor线程的任务队列中等待reactor线程去执行。比如：channel的write操作、定时任务的优先队列任务的添加等操作都会被封装为一次普通的task。 参考链接：https://www.jianshu.com/p/0d0eece6d467]]></content>
      <categories>
        <category>Netty源码分析</category>
      </categories>
      <tags>
        <tag>Netty</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Netty源码分析(1)-服务端启动过程]]></title>
    <url>%2F2018%2F10%2F23%2FNetty-2%2F</url>
    <content type="text"><![CDATA[GoodLuck Demo1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556public class DiscardServer &#123; private int port; public DiscardServer(int port) &#123; this.port = port; &#125; public void run() throws Exception &#123; EventLoopGroup bossGroup = new NioEventLoopGroup(); // (1) EventLoopGroup workerGroup = new NioEventLoopGroup(); try &#123; ServerBootstrap serverBootStrap = new ServerBootstrap(); // (2) serverBootStrap.group(bossGroup, workerGroup) .channel(NioServerSocketChannel.class) // (3) .handler(new SimpleServerHandler()) .childHandler(new ChannelInitializer&lt;SocketChannel&gt;() &#123; // (4) @Override public void initChannel(SocketChannel ch) throws Exception &#123; &#125; &#125;) .option(ChannelOption.SO_BACKLOG, 128) // (5) .childOption(ChannelOption.SO_KEEPALIVE, true); // (6) // Bind and start to accept incoming connections. ChannelFuture f = serverBootStrap.bind(port).sync(); // (7) // shut down your server. f.channel().closeFuture().sync(); &#125; finally &#123; workerGroup.shutdownGracefully(); bossGroup.shutdownGracefully(); &#125; &#125; public static void main(String[] args) throws Exception &#123; new DiscardServer(8080).run(); &#125; &#125; private static class SimpleServerHandler extends ChannelInboundHandlerAdapter &#123; @Override public void channelActive(ChannelHandlerContext ctx) throws Exception &#123; System.out.println("channelActive"); &#125; @Override public void channelRegistered(ChannelHandlerContext ctx) throws Exception &#123; System.out.println("channelRegistered"); &#125; @Override public void handlerAdded(ChannelHandlerContext ctx) throws Exception &#123; System.out.println("handlerAdded"); &#125; &#125; NioEventLoopGroup是一个处理IO操作的多线程事件循环。其中EventLoop是一个死循环，主要做三件事：检测IO事件，处理IO事件，执行任务 ServerBootstrap是一个辅助类，仅仅用来将组件拼装到一起 group(bossGroup, workerGroup) 我们需要两种类型的人干活，一个是老板，一个是工人，老板负责从外面接活，接到的活分配给工人干，放到这里，bossGroup的作用就是不断地accept到新的连接，将新的连接丢给workerGroup来处理。bossGroup一般用来接收传入的连接；workerGroup负责处理连接的IO事件。这里的workerGroup即源码中的childGroup channel表示一个连接 option一般用来配置TCP参数 childOption用来配置父channel接收到的channel，本例子中父channel是NioServerSocketChannel ch.pipeline().addLast(new SimpleServerHandler())这里使用了管道模式，将自定义的handler放到pipeline中当做『拦路虎』 .handler这里是father的handler，也就是bossGroup中channel使用的handler .childHandler注意这里是child的handler，也就是workerGroup中的channel使用的handler handlerChannelInitializer这个类是一个特殊的handler，当回调执行handlerAdded方法时候会调用initChannel方法 上面代码在本地执行之后，最终控制台输出为（注意这时候还没有客户端仅仅是服务端启动就触发的事件）：123handlerAddedchannelRegisteredchannelActive 分析源码上面例子中，serverBootStrap.bind(port).sync()就是我们的入口点。1234567public ChannelFuture bind(SocketAddress localAddress) &#123; validate(); if (localAddress == null) &#123; throw new NullPointerException("localAddress"); &#125; return doBind(localAddress);&#125; 进入到doBind中123456private ChannelFuture doBind(final SocketAddress localAddress) &#123; //... final ChannelFuture regFuture = initAndRegister(); //... doBind0(regFuture, channel, localAddress, promise);&#125; 省略了很多细节，重点关注两个方法initAndRegister（初始化并注册channel）和doBind0（绑定channel监听端口），至此分道扬镳~ initAndRegister先看省略后的骨干代码12345678910final ChannelFuture initAndRegister() &#123; //... final Channel channel = channelFactory().newChannel(); //... init(channel); //... ChannelFuture regFuture = group().register(channel); //... return regFuture;&#125; 我们还是专注于核心代码，抛开边角料，我们看到 initAndRegister() 做了几件事情1.new channel2.init channel3.register channel 我们逐步分析这三件事情 new channelchnnel是由channelFactory生产出来的，那么channelFactory是什么时候被初始化的呢，我们层层回溯，发现是在.channel(NioServerSocketChannel.class)方法的时候创建的123456public B channel(Class&lt;? extends C&gt; channelClass) &#123; if (channelClass == null) &#123; throw new NullPointerException("channelClass"); &#125; return channelFactory(new ReflectiveChannelFactory&lt;C&gt;(channelClass));&#125; 我们进入ReflectiveChannelFactory#newChannel方法看看12345678@Overridepublic T newChannel() &#123; try &#123; return clazz.newInstance(); &#125; catch (Throwable t) &#123; throw new ChannelException("Unable to create Channel from class " + clazz, t); &#125;&#125; 原来是通过反射的方式来创建我们传入的NioServerSocketChannel，换言之通过了默认构造函数new出了一个NioServerSocketChannel。接下来进入到NioServerSocketChannel的构造函数去看看12345678private static final SelectorProvider DEFAULT_SELECTOR_PROVIDER = SelectorProvider.provider();public NioServerSocketChannel() &#123; this(newSocket(DEFAULT_SELECTOR_PROVIDER));&#125;private static ServerSocketChannel newSocket(SelectorProvider provider) &#123; //... return provider.openServerSocketChannel();&#125; 这里通过SelectorProvider.openServerSocketChannel方法创建了一个channel1234public NioServerSocketChannel(ServerSocketChannel channel) &#123; super(null, channel, SelectionKey.OP_ACCEPT); config = new NioServerSocketChannelConfig(this, javaChannel().socket());&#125; 我们发现这里new了一个NioServerSocketChannelConfig，顶层接口为ChannelConfig，它表示channel的配置属性集合。我们再来看上面代码部分的第一行，追溯到父类AbstractNioChannel中12345678protected AbstractNioChannel(Channel parent, SelectableChannel ch, int readInterestOp) &#123; super(parent); this.ch = ch; this.readInterestOp = readInterestOp; //... ch.configureBlocking(false); //...&#125; 将创建出来的ServerSocketChannel保存到成员变量；将channel设置为非阻塞模式；将传入的SelectionKey.OP_ACCEPT保存到成员变量； 接下来重点看super(parent)1234567// AbstractChannel.javaprotected AbstractChannel(Channel parent) &#123; this.parent = parent; id = newId(); unsafe = newUnsafe(); pipeline = newChannelPipeline();&#125; 这里new出了三个大件，分别为1) id表示channel的唯一标识2）Unsafe是在实际进行数据传输时候使用的类，相当于被channel封装了3）pipeline表示处理输入输出事件的管道（类似双向链表） 总结，new一个channel的过程，就是通过channelFactory的反射机制创建的channel，而channelFactory是在调用.channel时候初始化的；接下来通过自己的构造函数和三个父类的构造函数分别创建了： channel, channelConfig, 保存了变量到成员变量, channelId,unsafe,pipeline init channel1234@Overridevoid init(Channel channel) throws Exception &#123; //....&#125; init内代码太长，接下来逐步分析123456789101112131415161718final Map&lt;ChannelOption&lt;?&gt;, Object&gt; options = options();synchronized (options) &#123; channel.config().setOptions(options);&#125;final Map&lt;AttributeKey&lt;?&gt;, Object&gt; attrs = attrs();synchronized (attrs) &#123; for (Entry&lt;AttributeKey&lt;?&gt;, Object&gt; e: attrs.entrySet()) &#123; @SuppressWarnings("unchecked") AttributeKey&lt;Object&gt; key = (AttributeKey&lt;Object&gt;) e.getKey(); channel.attr(key).set(e.getValue()); &#125;&#125;ChannelPipeline p = channel.pipeline();if (handler() != null) &#123; p.addLast(handler());&#125; 这里面的option()/attrs()/handler()方法都是提取Bootstrap组装时传入的参数。我们看这段代码其实就是把用户输入的那些属性配置到channel中。注意这里的channel是NioServerSocketChannelPs channel初始化三板斧：option/attr/pipeline.addHandler123456789101112131415161718final EventLoopGroup currentChildGroup = childGroup;final ChannelHandler currentChildHandler = childHandler;final Entry&lt;ChannelOption&lt;?&gt;, Object&gt;[] currentChildOptions;final Entry&lt;AttributeKey&lt;?&gt;, Object&gt;[] currentChildAttrs;synchronized (childOptions) &#123; currentChildOptions = childOptions.entrySet().toArray(newOptionArray(childOptions.size()));&#125;synchronized (childAttrs) &#123; currentChildAttrs = childAttrs.entrySet().toArray(newAttrArray(childAttrs.size()));&#125;p.addLast(new ChannelInitializer&lt;Channel&gt;() &#123; @Override public void initChannel(Channel ch) throws Exception &#123; ch.pipeline().addLast(new ServerBootstrapAcceptor( currentChildGroup, currentChildHandler, currentChildOptions, currentChildAttrs)); &#125;&#125;); 这段代码简单讲就是在NioServerSocketChannel中配置了最后一个『写死』的handler，我们进入到new ServerBootstrapAcceptor看看细节Ps这里的childGroup就是我们demo中传入的workGroupPs值得注意的是，这里还并未执行initChannel方法，该方法是在执行回调方法handlerAdded后调用的12345678910111213141516private static class ServerBootstrapAcceptor extends ChannelInboundHandlerAdapter &#123; private final EventLoopGroup childGroup; private final ChannelHandler childHandler; private final Entry&lt;ChannelOption&lt;?&gt;, Object&gt;[] childOptions; private final Entry&lt;AttributeKey&lt;?&gt;, Object&gt;[] childAttrs; ServerBootstrapAcceptor( EventLoopGroup childGroup, ChannelHandler childHandler, Entry&lt;ChannelOption&lt;?&gt;, Object&gt;[] childOptions, Entry&lt;AttributeKey&lt;?&gt;, Object&gt;[] childAttrs) &#123; this.childGroup = childGroup; this.childHandler = childHandler; this.childOptions = childOptions; this.childAttrs = childAttrs; &#125;//...&#125; 我们看这个类很简单，它是ServerBootStrap的一个内部类。它持有很多child开头的属性，这里的构造方法就是简单地将值赋给成员变量。下面看一个关键的方法12345678910111213141516171819202122232425262728public void channelRead(ChannelHandlerContext ctx, Object msg) &#123; final Channel child = (Channel) msg; child.pipeline().addLast(childHandler); for (Entry&lt;ChannelOption&lt;?&gt;, Object&gt; e: childOptions) &#123; try &#123; if (!child.config().setOption((ChannelOption&lt;Object&gt;) e.getKey(), e.getValue())) &#123; logger.warn("Unknown channel option: " + e); &#125; &#125; catch (Throwable t) &#123; logger.warn("Failed to set a channel option: " + child, t); &#125; &#125; for (Entry&lt;AttributeKey&lt;?&gt;, Object&gt; e: childAttrs) &#123; child.attr((AttributeKey&lt;Object&gt;) e.getKey()).set(e.getValue()); &#125; try &#123; childGroup.register(child).addListener(new ChannelFutureListener() &#123; @Override public void operationComplete(ChannelFuture future) throws Exception &#123; if (!future.isSuccess()) &#123; forceClose(child, future.cause()); &#125; &#125; &#125;); &#125; catch (Throwable t) &#123; forceClose(child, t); &#125;&#125; 这个方法是NioServerSocketChannel的读事件就绪时候触发的，可以看到此时的msg是一个channel，而这个channel是要传递给workGroup的，我们看到首先对这个channel进行了『初始化三板斧』，接下来将这个channel注册到了childGroup也就是workerGroup，这里就是Boss将任务分配给Worker的地方。 为何这里可以直接将msg强制转为Channel?答：这个追溯到eventLoop的第二板斧中，在那里显示通过jdk原生的javaChannel().accept()获得了SocketChannel，接下来再将这个原生channel封装成NioSocketChannel，最后将封装好的channel传递到pipeline中 至此channel初始化工作也完成了，又到了总结时间。首先对NioServerSocketChannel执行初始化三板斧（attr/option/handler），在最后一板斧中将一个特殊的handler-ServerBootStrapAcceptor塞到了pipeline中，这个handler的read方法比较特殊，它将接收到的channel注册到了workGroup 这里对表示连接的channel进行宏观概览 register channelChannelFuture regFuture = config().group().register(channel);，这里的config表示的是bootStrap的config，而这里的group表示的是bossGroup1234567891011121314151617//MultithreadEventLoopGroup.class//这个next其实就是从group中选择一个eventLoop@Overridepublic ChannelFuture register(Channel channel) &#123; return next().register(channel);&#125;//SingleThreadEventLoop.classpublic ChannelFuture register(Channel channel) &#123; return this.register((ChannelPromise)(new DefaultChannelPromise(channel, this)));&#125;public ChannelFuture register(ChannelPromise promise) &#123; ObjectUtil.checkNotNull(promise, "promise"); promise.channel().unsafe().register(this, promise); return promise;&#125; 之前代码追到这里就停了，导致忽略了很重要的点。我们继续向下追1234567891011121314public final void register(EventLoop eventLoop, final ChannelPromise promise) &#123; //... AbstractChannel.this.eventLoop = eventLoop; if (eventLoop.inEventLoop()) &#123; register0(promise); &#125; else &#123; eventLoop.execute(new Runnable() &#123; @Override public void run() &#123; register0(promise); &#125; &#125;); &#125;&#125; 由于我们处于main线程中，所以这里会执行else的逻辑，注意这里的evenLoop是我们自己new的EvenLoopGroup随机挑选的一个，在这里赋给了当前的channel。这里会执行eventLoop.execute，而这行代码是EvenLoop也就是reactor线程的触发点，执行之后开启reactor线程。我们继续跟进注册过程1234567891011121314private void register0(ChannelPromise promise) &#123; //... doRegister(); //... pipeline.invokeHandlerAddedIfNeeded(); pipeline.fireChannelRegistered(); if (isActive()) &#123; if (firstRegistration) &#123; pipeline.fireChannelActive(); &#125; else if (config().isAutoRead()) &#123; beginRead(); &#125; &#125;&#125; 1234567891011121314151617@Overrideprotected void doRegister() throws Exception &#123; boolean selected = false; for (;;) &#123; try &#123; selectionKey = javaChannel().register(eventLoop().selector, 0, this); return; &#125; catch (CancelledKeyException e) &#123; if (!selected) &#123; eventLoop().selectNow(); selected = true; &#125; else &#123; throw e; &#125; &#125; &#125;&#125; 最后跟到了这行代码selectionKey = javaChannel().register(eventLoop().selector, 0, this);这里就是经典的jdk注册channel的方式。至此注册行为结束，接下来会触发一些回调方法pipeline.invokeHandlerAddedIfNeeded()触发了最开始我们定义的SimpleServerHandler中的handlerAdded方法，此时控制台输出”handlerAdded”。在这里会执行ChannelInitializer#handlerAdded方法，该方法会回调我们自行重写的initChannel方法，一般这个方法用于将handler塞到pipeline中pipeline.fireChannelRegistered()触发了最开始我们定义的SimpleServerHandler中的channelRegistered方法，此时控制台输出”channelRegistered”isActive()我们进到isActive()方法，发现该方法表示channel对应的socket是否已经处于绑定状态，显然这时候还没有绑定。所以此时还不能回调channelActive方法 finally，终于分析完了第一步骤。三件事 1.new一个channel new一个channel的过程，就是通过channelFactory的反射机制创建的channel，而channelFactory是在调用.channel时候初始化的；接下来通过自己的构造函数和三个父类的构造函数分别创建了：channelchannelConfig,保存了变量到成员变量,channelId,unsafe,pipeline 2.init这个channel 首先对NioServerSocketChannel执行初始化三板斧（attr/option/handler），在最后一板斧中将一个特殊的handler-ServerBootStrapAcceptor塞到了pipeline中，这个handler的read方法比较特殊，它将接收到的channel注册到了workGroup 3.将这个channel register到某个对象（Boss的Reactor线程在这一步骤中启动） 将NioServerSocketChannel注册到了bossGroup的selector上;回调handlerAdd;回调channelRegister doBind012345678910111213141516private static void doBind0( final ChannelFuture regFuture, final Channel channel, final SocketAddress localAddress, final ChannelPromise promise) &#123; // This method is invoked before channelRegistered() is triggered. Give user handlers a chance to set up // the pipeline in its channelRegistered() implementation. channel.eventLoop().execute(new Runnable() &#123; @Override public void run() &#123; if (regFuture.isSuccess()) &#123; channel.bind(localAddress, promise).addListener(ChannelFutureListener.CLOSE_ON_FAILURE); &#125; else &#123; promise.setFailure(regFuture.cause()); &#125; &#125; &#125;);&#125; 注意这里的execute方法已经不会再触发reactor线程的运转了（通过CAS保证了只会触发一次），这里只是单纯地新增了一个task供Reacotr线程执行。我们发现这里将绑定的具体行为，封装成了一个异步的任务放到了eventLoop中执行。 Ps 这样的话能够保证优先执行异步封装的register任务再执行bind任务我们重点去看channel.bind这里并不是eventLoop启动的地方！！！1234@Overridepublic ChannelFuture bind(SocketAddress localAddress, ChannelPromise promise) &#123; return pipeline.bind(localAddress, promise);&#125; 层层深入我们跟到了这里，发现调用的是pipeline的bind，这里我也妥协了打了断点跟到了下面这段代码12345678910111213141516//AbstractChannel.classpublic final void bind(final SocketAddress localAddress, final ChannelPromise promise) &#123; assertEventLoop(); //... boolean wasActive = isActive(); doBind(localAddress); //... if (!wasActive &amp;&amp; isActive()) &#123; invokeLater(new Runnable() &#123; @Override public void run() &#123; pipeline.fireChannelActive(); &#125; &#125;); &#125;&#125; 大概两件事1）doBind 2）pipeline.fireChannelActive。先来看第一件事情doBind123456789//NioServerSocketChannel@Overrideprotected void doBind(SocketAddress localAddress) throws Exception &#123; if (PlatformDependent.javaVersion() &gt;= 7) &#123; javaChannel().bind(localAddress, config.getBacklog()); &#125; else &#123; javaChannel().socket().bind(localAddress, config.getBacklog()); &#125;&#125; 我们可以看到就是简单地jdk形式的绑定地址。绑定过之后，再执行isActive()返回的就是true了，此时会触发channelActive事件，console打印”channelActive” Finally终于分析完了，自认还算比较清晰，而且将参考文章中一些没有的地方也提到了。 参考链接：https://www.jianshu.com/p/c5068caab217]]></content>
      <categories>
        <category>Netty源码分析</category>
      </categories>
      <tags>
        <tag>Netty</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[netty外层]]></title>
    <url>%2F2018%2F10%2F15%2Fnetty-1%2F</url>
    <content type="text"><![CDATA[总有一天我会把netty彻底搞清楚和线程池一样 ByteBuf原生的缺点先说nio提供的ByteBuffer的缺点（结合例子）12345678910111213private static ByteBuffer bb = ByteBuffer.allocate(1024);SocketChannel sc = (SocketChannel)sk.channel();bb.clear();while (sc.read(bb) &gt; 0)&#123; bb.flip(); while (bb.hasRemaining())&#123; System.out.print((char)bb.get()); &#125; System.out.println(); bb.clear();&#125; （1）ByteBuffer长度固定，一旦分配完成容量不能动态扩展和收缩（2）只有一个标识位置的指针position，读写的时候要手工调用flip() ByteBuf的好处（1）提供了readerIndex和writerIndex两个指针，写的时候writerIndex不断增加，读的时候readerIndex不断增加（但是不能超过writerIndex）（2）屏蔽扩展细节，实现动态扩展（3）discardReadBytes操作，将0-readerIndex范围的数据回收并续到capacity后面（目的是尽可能地重用缓冲区，但是因为改操作会发生字节数组的内存复制所以会影响性能） Channel功能说明（1）channel是netty网络操作抽象类，包括网络的读、写，客户端发起连接，主动关闭连接，获取通信双方的网络地址等。（2）channel需要注册到EventLoop的多路复用器上，用于处理IO事件，通过channel的eventLoop方法可以获取注册的EventLoop（3）EventLoop的本质就是处理网络读写事件的Reactor线程（4）在netty中每个channel对应一个物理连接，每个连接都由自己的tcp参数配置（tcp缓冲区大小、tcp超时时间等），通过metadata()可以获取当前channel的tcp参数设置 ChannelPipeline和ChannelHandler（1）ChannelPipeline和ChannelHandler类似于Servlet和Filter，都是责任链模式的一种变形（2）ChannelPipeline是Channel的数据管道的抽象，消息在ChannelPipeline中流动（3）ChannelPipeline持有io事件拦截器ChannelHandler的链表，由ChannelHandler对IO事件进行拦截和处理，可以方便地新增和删除ChannelHandler（4）这里关注下ChannelHandlerAdapter，因为ChannelHandler接口中的方法太多，所以如果自定义的handler直接继承该接口要实现很多用不到的方法，所以这里使用了适配器模式，简单地讲就是一个对接口中的方法进行了空实现的抽象类（5）ChannelHandler用ChannelHandlerContext包裹着，有prev和next节点，可以获取前后ChannelHandler，read时从ChannelPipeline的head执行到tail，write时从tail执行到head，所以head既是read事件的起点也是write事件的终点，与io交互最紧密 Reactor模型单线程模型指的是所有的io操作都在同一个nio线程上面完成，nio线程的职责如下（1）接收客户端的TCP连接（2）读取通信对端的请求或者应答（3）向通信对端发送消息请求或者应答这种模型，理论上可以处理所有的io相关操作。不过只适用于一些小容量的应用场景，对于高负载、大并发的应用不合适（1）性能问题。一个nio线程处理成百上千个请求，性能无法支撑，哪怕CPU利用率100%（2）可靠性问题。一旦nio线程意外进入死循环，那么会导致整个系统通信不可用 多线程模型为了解决上述单线程模型的问题，引入了多线程模型。和单线程模型最大的区别是，有一组nio线程处理io操作。（1）有一个专门的nio线程—-Acceptor线程用于监听服务端，接收客户端的tcp连接请求（2）网络io操作—-读、写等由一个nio线程池负责，线程池负责消息的读取、编解码和发送（3）一个nio线程可以同时处理n条链路，但是1个链路只对应1个nio线程，防止发生并发操作问题 主从模型大多数情况，多线程模型都能满足需求。但是在及其特殊的场景中，一个nio线程负责监听和处理所有的客户端连接可能还是会有问题。例如百万级别的客户端连接，单独一个Acceptor线程可能会存在性能不足问题。为此，引出Reactor多线程模型（1）服务端用于接收客户端连接的不再是一个单独的nio线程，而是一个独立的nio线程池。（2）Acceptor接收到客户端tcp连接请求处理完成后，将新创建的SocketChannel注册到io线程池（sub reactor线程池）的某个io线程上。由它负责socketChannel的读写和编码工作。（3）Acceptor线程池只用于客户端的连接，一旦链路建立成功，就将链路注册到subReactor线程池的io线程上，由io线程负责后续的io操作。]]></content>
      <categories>
        <category>I/O</category>
      </categories>
      <tags>
        <tag>I/O</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[网络协议(1)]]></title>
    <url>%2F2018%2F10%2F11%2Fprotocal%2F</url>
    <content type="text"><![CDATA[关于网络的一些基础概念最近看阮大神博客发现两篇好文： http://www.ruanyifeng.com/blog/2012/05/internet_protocol_suite_part_i.html http://www.ruanyifeng.com/blog/2012/06/internet_protocol_suite_part_ii.html 看过之后很多概念逐渐清晰遂记录下 ARP协议：通过IP地址去拿MAC地址，只适用于同一个子网络下，通过广播将目标机器的IP地址广播到所有的子网络内的机器，目标机器收到广播后（判断IP地址是否相同）返回自己的MAC地址 路由：网关之间的通信 MAC地址：由机器的网卡唯一指定 网关：子网络的看门大爷，和外界打交道需要通过网关 “传输层”的功能，就是建立”端口到端口”的通信。 “网络层”的功能是建立”主机到主机”的通信。只要确定主机和端口，我们就能实现程序之间的交流。 “应用层”的作用，就是规定应用程序的数据格式。 同一个子网络发送数据需要知道目标机器的MAC地址 非一个子网络发送数据需要知道目标机器的IP地址和网关的MAC地址 IP地址可变，而MAC地址不可变，通过IP地址能够较快的找到目标机器的子网络 例子： 假设网络上要将一个数据包（名为PAC）由北京的一台主机（名称为A，IP地址为IP_A，MAC地址为MAC_A） 发送到华盛顿的一台主机（名称为B，IP地址为IP_B，MAC地址为MAC_B）。 这两台主机之间不可能是直接连接起来的，因而数据包在传递时必然要经过许多中间节点（如路由器，服务器等等）， 我们假定在传输过程中要经过C1、C2、C3（其MAC地址分别为M1，M2，M3）三个节点。 A在将PAC发出之前，先发送一个ARP请求，找到其要到达IP_B所必须经历的第一个中间节点C1的MAC地址M1， 然后在其数据包中封装（Encapsulation）这些地址：IP_A、IP_B，MAC_A和M1。当PAC传到C1后， 再由ARP根据其目的IP地址IP_B，找到其要经历的第二个中间节点C2的MAC地址M2，然后再将带有M2的数据包传送到C2。 如此类推，直到最后找到带有IP地址为IP_B的B主机的地址MAC_B，最终传送给主机B。 在传输过程中，IP_A、IP_B和MAC_A不变，而中间节点的MAC地址通过ARP在不断改变（M1，M2，M3），直至目的地址MAC_B。 电脑上网必须有四个参数 1）本机IP地址 2）子网掩码 3）网关IP地址 4）DNS服务器IP地址 本机访问Google全过程 1）将域名发送到DNS服务器，DNS服务器返回域名对应的IP地址（Google的IP地址） 2）将当前服务的端口和目标服务器程序的端口写到TCP头中 3）将当前主机的IP地址和目标服务器端IP地址写到IP头中 4）通过子网掩码判断Google的IP地址和本机的IP地址是否在同一个子网络中 5）如果在同一个子网络中，那么通过ARP协议去获取Google的MAC地址，然后写到以太协议的Header中 end 如果不在同一个子网络中，那么通过ARP协议获取网关的MAC地址并写入以太协议的Header中 6）本机所在网络的网关通过路由将数据发送到Google所在子网络的网关 7）Google的网关将数据发到Google的服务器上（在这之前要先找到Google的MAC地址） 使用wireshark软件抓到的一次HTTP请求，很多东西更清晰了，值得注意点几个点 1）可以清楚地看到三次握手（这里抓到了两次请求） 2）仔细观察收到的HTTP包，可以清楚地看到层级结构，以太协议-IP-TCP-HTTP-数据包 3）数据传输阶段ACK = SEQ+TCP SEGMENT 4）152是对151的回应，表示收到了你的消息；同理154是对153的回应表示收到了你的消息]]></content>
      <categories>
        <category>网络协议</category>
      </categories>
      <tags>
        <tag>网络协议</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MQ-入门]]></title>
    <url>%2F2018%2F09%2F27%2FMQ%2F</url>
    <content type="text"><![CDATA[Outline1.使用场景2.如何保证消息可靠到达3.如何保证幂等4.如何削峰填谷 使用场景不适用的场景：1）调用方实时依赖结果的业务场景 适用的场景：1）有执行顺序的job，可以通过cron执行第一个job，当第一个job执行成功后通过mq通知第二个job，依次类推2）上游不关心执行结果，以预约场景为例，用户预约成功后对于后续的一些处理过程（比如发送消息通知商户等）并不关心，这种情况很适合采用mq和预约行为代码解耦。而且当需要增加一些其他后处理行为的时候，不需要修改上游代码。 MQ核心架构发送方：1）业务调用方2）MQ-client-sender（核心api：sendMsg(),sendCallback()） MQ集群：1）MQ-server2）zk3）db4）管理后台web 接收方：1）业务接收方2）MQ-client-reciever（核心api：revcCallback(),sendAck()） MQ运转流程MQ消息投递上半场，MQ-client-sender到MQ-server流程见上图： （1）MQ-client将消息发送给MQ-server（此时业务方调用的是API：SendMsg） （2）MQ-server将消息落地，落地后即为发送成功 （3）MQ-server将应答发送给MQ-client（此时回调业务方是API：SendCallback） MQ消息投递下半场，MQ-server到MQ-client-receiver流程见上图4-6： （4）MQ-server将消息发送给MQ-client（此时回调业务方是API：RecvCallback） （5）MQ-client回复应答给MQ-server（此时业务方主动调用API：SendAck） （6）MQ-server收到ack，将之前已经落地的消息删除，完成消息的可靠投递 如何保证消息可靠到达机制1）消息落地2）消息超时重传、消息确认 具体措施上半场的超时与重传 MQ上半场的1或者2或者3如果丢失或者超时，MQ-client-sender内的timer会重发消息，直到期望收到3，如果重传N次后还未收到，则SendCallback回调发送失败，需要注意的是，这个过程中MQ-server可能会收到同一条消息的多次重发。 下半场的超时与重传 MQ下半场的4或者5或者6如果丢失或者超时，MQ-server内的timer会重发消息，直到收到5并且成功执行6，这个过程可能会重发很多次消息，一般采用指数退避的策略，先隔x秒重发，2x秒重发，4x秒重发，以此类推，需要注意的是，这个过程中MQ-client-receiver也可能会收到同一条消息的多次重发。 如何保证幂等 上半场幂等的设计此时重发是MQ-client发起的，消息的处理是MQ-server，为了避免步骤2落地重复的消息，对每条消息，MQ系统内部必须生成一个inner-msg-id(该id在MQ-client端生成，当需要重试的时候使用同样的消息Id，而不要在server端自动生成消息)，作为去重和幂等的依据，这个内部消息ID的特性是： （1）全局唯一 （2）MQ生成，具备业务无关性，对消息发送方和消息接收方屏蔽 Ps：上半场的幂等不涉及到调用方的工作，主要保证不会落地重复 下半场幂等的设计此时重发是MQ-server发起的，消息的处理是消息消费业务方，消息重发势必导致业务方重复消费（上例中的一次付款，重复扣减库存），为了保证业务幂等性，业务消息体中，必须有一个biz-id，作为去重和幂等的依据，这个业务ID的特性是： （1）对于同一个业务场景，全局唯一 （2）由业务消息发送方生成，业务相关，对MQ透明 （3）由业务消息消费方负责判重，以保证幂等 比如，代码中对biz-id进行判断，如果处理过了直接return，以保证幂等 Ps：下半场的幂等涉及到消费方的业务逻辑的改动 MQ削峰填谷传统的rpc框架，服务提供方会有限流以及鉴权措施对于通过mq传递引用的场景，如何防止流量过大造成下游服务雪崩？ 1）原来，MQ-client-reciver除了推送模式，还有一种拉模式，reciver每隔一定时间从server拉取一定的消息，以实现流量控制，保护自身。这个是MQ提供的通用功能，无须上下游修改代码。 2）于此同时，对于批量拉取的消息，消费放最好能够做到批量消费，这需要对原有的1对1的代码进行修改，以实现批量处理 如何保证业务操作和消息发送的一致性正向：1）业务将消息投递到消息中间件2）中间件收到消息后，将消息存入db，标记消息状态为待处理3）中间件返回消息处理的结果（入库结果）4）业务收到入库结果，如果成功执行业务操作；反之，放弃业务处理，结束5）业务操作完成，将业务操作结果发送到中间件6）如果业务成功，更新消息状态为待发送，并进行消息投递；如果失败，删除db中的消息，结束 问题：可能出现的问题，全流程执行后（可能有fail）消息仍然处于待处理状态，此时需要反向操作 反向：1）中间件主动询问业务执行结果2）检查业务执行结果3）如果成功，那么更新消息为待发送；反之删除 原文链接：https://mp.weixin.qq.com/s/CIPosICgva9haqstMDIHag]]></content>
      <categories>
        <category>Java中间件</category>
      </categories>
      <tags>
        <tag>好文收集</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[读写锁]]></title>
    <url>%2F2018%2F09%2F27%2FReentrantReadWriteLock%2F</url>
    <content type="text"><![CDATA[今天看到一个词，『锁降级』，于是就Google了下，然后顺带着把之前没搞懂的一个知识点搞清楚了，在这里记录下。简单地讲，就是获取写锁后对数据进行变更，再获取读锁，再释放写锁，平滑地完成锁降级，以实现同一个线程的写读一致先上例子（不得不说，官方的例子真是简洁优雅没任何废话）1234567891011121314151617181920212223242526272829303132class CachedData &#123; Object data; volatile boolean cacheValid; final ReentrantReadWriteLock rwl = new ReentrantReadWriteLock(); void processCachedData() &#123; rwl.readLock().lock(); if (!cacheValid) &#123; // Must release read lock before acquiring write lock rwl.readLock().unlock(); rwl.writeLock().lock(); try &#123; // Recheck state because another thread might have // acquired write lock and changed state before we did. if (!cacheValid) &#123; data = ... cacheValid = true; &#125; // Downgrade by acquiring read lock before releasing write lock rwl.readLock().lock(); &#125; finally &#123; rwl.writeLock().unlock(); // Unlock write, still hold read &#125; &#125; try &#123; use(data); &#125; finally &#123; rwl.readLock().unlock(); &#125; &#125; &#125; 几个值得注意的点1）当存在读锁时，获取写锁的线程会被阻塞（防止读的时候数据被修改）2）读锁是共享锁，所以当存在读锁时，获取读锁的线程不会被阻塞（极大提升了系统吞吐量）3）锁降级的目的，保证了一个『原子性』，写-读原子性（如果先放弃写锁再获取读锁，可能会被其他线程抢占写锁导致数据变更）;主要是为了保证当前线程，写读的一致性4）当前线程持有写锁时，可以获取读锁；而当前线程持有读锁时，无法获取写锁。很容易理解，当前线程持有写锁时，其他线程不会获取到读/写锁，所以可以直接获取读锁，不会对其他执行读的线程产生影响；而当前线程持有读锁时，其他线程可能也持有读锁，所以在获取写锁时要先放弃读锁]]></content>
      <categories>
        <category>多线程</category>
      </categories>
      <tags>
        <tag>多线程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Top-K]]></title>
    <url>%2F2018%2F09%2F26%2Ftop-k%2F</url>
    <content type="text"><![CDATA[Outline讨论了几种解决top-k问题的方法前言：之前觉着一旦掌握了某些通性通法就不用去单独刷什么算法题，殊不知很多最优秀的解题思路单凭自己闭门造车是很难相出来的，所以还是要去多刷题（和高三一样），刷题-思考-答案-消化-思考-提升题目：int[] arr = new int[] {5,3,7,1,8,2,9,4,7,2,6,6}，从中找出top5的数字 排序解决利用排序算法，比如快排，对arr进行排序，之后找到前5的数字。这种方法是最先想到的方法，时间复杂度为O(N*logN)伪代码:12quickSort(arr);return arr[0,4]; 时间复杂度O(NlogN) 初级排序解决再次审题，我们只需要top5的数字，无须对所有数组中的元素进行排序，所以可以使用初级排序算法，比如冒泡排序、选择排序伪代码：1234for(int i=0;i&lt;4;i++)&#123; findMaxAndSwap(arr);&#125;return arr[0,4]; 时间复杂度O(k*N) 堆排序解决数组堆解决这个问题，先令arr的前5个元素堆有序，解析来遍历后面的元素，如果大于堆中的最小值（根节点）进行swap，之后再使堆变得有序伪代码12345678heap[4] = make_heap(arr[0, 4]);for(i=5 to n)&#123; if(arr[i] &gt; arr[0]) &#123; swap(arr,0,i); adjust_heap(heep[4]); &#125; &#125;return heap[4]; 时间复杂度O(N*logK) 随机选择解决说白了，还是利用快排解决这个问题伪代码123456789int RS(arr, low, high, k)&#123; if(low== high) return arr[low]; i= partition(arr, low, high); temp= i-low; //数组前半部分元素个数 if(temp&gt;=k) return RS(arr, low, i-1, k); //求前半部分第k大 else return RS(arr, i+1, high, k-i); //求后半部分第k-i大&#125; 时间复杂度O(N) 题外话：分治法，每个分支“都要”递归，例如：快速排序，O(n*lg(n))减治法，“只要”递归一个分支，例如：二分查找O(lg(n))，随机选择O(n) bitMap解决空间换时间5,3,7,1,8,2,9,4,7,2,6,61234567891011121314int max = findMax(arr);int[] res = new int[max+1];for(int i = 0;i&lt;arr.length;i++) &#123; res[arr[i]]++;&#125;int cur = 0;for(int j=res.length-1;j&gt;0;j--) &#123; for(int i= 0;i&lt;res[j];i++) &#123; arr[cur++] = j-1; if(cur &gt;= 3) break; &#125; if(cur &gt;= 3) break;&#125;return arr[k];]]></content>
      <categories>
        <category>Algorithm</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[spi机制与策略模式]]></title>
    <url>%2F2018%2F09%2F14%2Fspi%2F</url>
    <content type="text"><![CDATA[Outline一、啥是SPI？二、啥是策略模式？三、SPI和策略模式有啥关系？ 一、啥是SPI？入职初期经常听大佬说SPI，也不知道是干啥的，最近终于有所领悟。Java程序员应该对『面向接口』编程不陌生，我们要说的SPI和面向接口编程是紧密关联的。 简单地讲，如果一个接口由调用方来定义，而接口的实现由提供方来实现，这个就是SPI。而如果接口的定义和实现都由提供方来完成，就是我们常说的API。这样说可能还是比较抽象，不如来看一个例子。src.zip/rt-jar内定义了一个接口Driver，在这个例子中src包扮演的角色是调用方12345public interface Driver &#123; Connection connect(String url, java.util.Properties info) throws SQLException; ...省略&#125; 既然src定义了接口，那么他肯定要用啊，具体细节忽略我们只关心下面这行代码123456789101112public class DriverManager &#123; ... ServiceLoader&lt;Driver&gt; loadedDrivers = ServiceLoader.load(Driver.class); .... try&#123; while(driversIterator.hasNext()) &#123; driversIterator.next(); &#125; &#125; catch(Throwable t) &#123; // Do nothing &#125;&#125; 小结： 调用方提供了接口Driver 调用方面向Driver接口进行了编程 下面看看提供方做了啥首先接口提供方mysql-connector-java登场，我们重点关注两个地方1）接口的具体实现1234567891011public class Driver extends NonRegisteringDriver implements java.sql.Driver &#123; public Driver() throws SQLException &#123; &#125; static &#123; try &#123; DriverManager.registerDriver(new Driver()); &#125; catch (SQLException var1) &#123; throw new RuntimeException("Can't register driver!"); &#125; &#125;&#125; 2）名为java.sql.Driver的配置文件1com.mysql.jdbc.Driver 小结： 实现了src中定义的接口 定义了一个不知道干啥的配置文件 我们再回到方法调用方src中（注意src是需要引入mysql-connector-java这个包的），有这样一行代码ServiceLoader.load(Driver.class);ServiceLoader读取mysql-connector-java包中的那个配置文件中定义的实现类的名字，通过反射获取对应类实例~这样我们的接口就被指定具体的实现了~接下来我们来看策略模式 二、啥是策略模式？在阎宏博士的《JAVA与模式》一书中开头是这样描述策略（Strategy）模式的：策略模式属于对象的行为模式。其用意是针对一组算法，将每一个算法封装到具有共同接口的独立的类中，从而使得它们可以相互替换。策略模式使得算法可以在不影响到客户端的情况下发生变化。 我们还是通过一个例子来看1234567891011121314151617181920212223242526272829interface Calculator &#123; int doExecute(int a, int b);&#125;class AddCalculator implements Calculator&#123; public int doExecute(int a, int b)&#123; return a+b; &#125;&#125;class MinusCalculator implements Calculator&#123; public int doExecute(int a, int b)&#123; return a-b; &#125;&#125;public class Test &#123; Calculator cal; public void setCal(Calculator cal)&#123; this.cal = cal; &#125; public void doCalculate(int a, int b)&#123; cal.doExecute(a, b); &#125; public static void main(String[] args) &#123; Test t = new Test(); t.setCal(new AddCalculator()); t.doCalculator(1,2); t.setCal(new MinusCalculator()); t.doCalculator(2,1); &#125;&#125; 在Test类的doCalculate方法中并不关心接口的具体实现，这就是一『面向接口』编程的例子，我们看到接口的具体实现在main方法也就是接口的调用处来指定实现，这个思想和IOC的思想又是类似的~那么看我们的最后一个问题，SPI和策略模式有啥关系 三、SPI和策略模式有啥关系？聪明的小伙伴应该已经看出端倪SPI模式和策略模式都是面向接口编程的典范，而且接口的具体实现由调用方来指定。策略模式的例子如上，对于第一个SPI例子来说，假如我们在开发一个java项目，使用的数据库是oracle，那么我们引入oracle的jar包即可，显然该jar包中一定包含一个Driver的实现类以及执行实现类的配置文件，这个时候我们的java程序就可以『无痕』地使用oracle提供的Driver类了这里再多说一句，所有的java项目都会依赖src包的~]]></content>
      <categories>
        <category>Java 进阶</category>
      </categories>
      <tags>
        <tag>Java 进阶</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[内存数据库-Redis]]></title>
    <url>%2F2018%2F09%2F13%2FRedis%2F</url>
    <content type="text"><![CDATA[Outline1.Redis常用的数据结构2.Redis的持久化3.Redis主从复制4.Redis集群5.Redis应用场景6.redis为啥这么快 Redis常用的数据结构这里都是针对value来讲的 StringString 是 redis 最基本的类型，你可以理解成与 Memcached 一模一样的类型，一个 key 对应一个 value。（Ps:redis相比memcached新增了很多类型）String 可以包含任何数据。比如jpg图片或者序列化的对象。String 类型是 Redis 最基本的数据类型，String 类型的值最大能存储 512MB。Ps：value较小、模型简单的 value可以使用String类型存储，对于一些特殊的数据结构，比如List、Set等，建议采用相应的下面介绍的List和Set数据结构进行存储，这样不仅可以节省存储空间还可以提高操作效率。 ListList类型是按照插入顺序排序的字符串链表。和数据结构中的普通链表一样，可以在其头部(left)和尾部(right)添加新的元素。在插入时，如果该键并不存在，Redis将为该键创建一个新的链表。与此相反，如果链表中所有的元素均被移除，那么该键也将会被从数据库中删除。 SetSet是String类型的无序集合。集合是通过哈希表实现的，所以添加，删除，查找的复杂度都是O(1)。 zsetRedis zset 和 set 一样也是String类型元素的集合,且不允许重复的成员。不同的是每个元素都会关联一个double类型的分数。redis正是通过分数来为集合中的成员进行从小到大的排序。zset的成员是唯一的,但分数(score)却可以重复。底层数据结构是跳表（快速插入、查询、删除，且支持范围查询） Redis的持久化（单机）RDB &amp; AOF 简介1）RDB 持久化机制，会在一段时间内生成指定时间点的数据集快照(snapshot)2）AOF 持久化机制(Ps：类似数据库日志，全量日志)，记录 server 端收到的每一条写命令，当 server 重启时会以此来重建之前的数据集。3）如果仅使用 Redis 作为缓存加速访问，可以关闭这两个持久化设置4）也可以同时开启这两个持久化设置，但是在这种情况下，Redis 重启时会使用 AOF 文件来重建数据集，因为 AOF 文件保存的数据往往更加完整 详解RDBRDB 创建与载入Redis 提供了 SAVE 和 BGSAVE 两个命令来生成 RDB 文件，区别是前者是阻塞的，后者是后台 fork 子进程进行不会阻塞主进程处理命令请求。载入 RDB 文件不需要手工运行，而是 server 端自动进行，只要启动时检测到 RDB 文件存在 server 端便会载入 RDB 文件重建数据集。当然上面简介中已经提到，如果 同时存在 AOF 的话会优先使用 AOF 重建数据集因为其保存的数据更完整。 RDB 相关配置SAVE POINT配置：save seconds changesxx秒内发生了xx次变化，那么执行一次快照保存 RDB 的优点1）RDB文件是一个很简洁的单文件，它保存了某个时间点的 Redis 数据集，很适合用于做备份。你可以设定一个时间点对 RDB 文件进行归档（备份保存），这样就能在需要的时候很轻易的把数据恢复到不同的版本。2）基于上面所描述的特性，RDB 文件很适合用于灾备，因为单文件可以很方便地传输到另外的数据中心。3）RDB的性能很好，需要进行持久化时，主进程会 fork 一个子进程出来，然后把持久化的工作交给子进程，自己不会有相关的 I/O 操作。 RDB 的缺点1）RDB容易造成数据的丢失，当你希望在 Redis 停止工作时尽量减少数据丢失的话，那 RDB 不适用。假设每5分钟保存一次快照，如果Redis因为某些原因不能正常工作，那么从上次产生快照到 Redis 出现问题这段时间的数据就会丢失了。2）RDB 使用 fork 子进程进行数据的持久化，如果数据比较大的话可能就会花费点时间，造成 Redis 停止服务几毫秒。3）在 Linux 系统中，fork 会拷贝进程的 page table。随着进程占用的内存越大，进程的 page table 也会越大，那么 fork 也会占用更多的时间。 如果 Redis 占用的内存很大 (例如 20 GB)，那么在 fork 子进程时，会出现明显的停顿现象（无法处理 client 的请求）4）Linux fork 子进程采用的是 copy-on-write 的方式。在 Redis 执行 RDB 持久化期间，如果 client 写入数据很频繁，那么将增加 Redis 占用的内存 详解 AOFAOF 实现和 RDB 持久化数据库键值对来记录数据库状态不同，AOF 是通过保存对数据库的写命令集来记录数据库状态的。AOF 持久化实现可以分为命令追加(append)、文件写入(write)、文件同步(fsync) 三个步骤。Ps 和RDB不同的是 AOF并非异步的 而是和处理客户端请求的进程是同一个进程Append 追加命令到aof_buf 缓冲区 ，Write 将缓冲区的内容写入到AOF文件缓冲区，Fsync 将程序缓冲区的内容写入到文件。 命令追加当 AOF 持久化功能打开时，server 端每执行完一个写命令，会以协议格式将被执行的写命令追加到 server 端 redisServer 结构体中的 aof_buf 缓冲区末尾。 文件写入与同步Redis server 进程是一个事件循环(event loop)，server 每结束一个事件循环之前都会调用 flushAppendOnlyFile 函数，考虑是否将 aof_buf 缓冲区中的内容吸入和保存到 AOF 文件，而 flushAppendOnlyFile 函数的行为由 appendfsync 选项来控制 appendfsync的值 flushAppendOnlyFile行为 everysec 每个事件循环都将 aof_buf 缓冲区中的内容写入 AOF 文件，Redis 还会每秒在子线程中执行一次 fsync()。在实践中，推荐使用这种设置，一定程度上可以保证数据持久性，又不会明显降低 Redis 性能 no 每个事件循环都将 aof_buf 缓冲区中的内容写入 AOF 文件，但不对其进行同步，何时同步至磁盘会让操作系统决定。这种模式下 AOF 的写入速度最快，不过因其会在系统缓存中积累一段时间的数据，所以同步时间为三者最长。一旦宕机将会丢失自上一次同步 AOF 文件起所有的数据 always 每个事件循环都将 aof_buf 缓冲区中的内容写入 AOF 文件，并且调用 fsync() 将其同步到磁盘。这可以保证最好的数据持久性，但却会给系统带来极大的开销，其效率是三者中最慢的，但同时安全性也是最高的，即使宕机也只丢失一个事件循环中的数据 Ps：文件的写入和同步（fsync）为了提高文件的写入效率， 在现代操作系统中， 当用户调用 write 函数， 将一些数据写入到文件的时候， 操作系统通常会将写入数据暂时保存在一个内存缓冲区里面， 等到缓冲区的空间被填满、或者超过了指定的时限之后， 才真正地将缓冲区中的数据写入到磁盘里面。这种做法虽然提高了效率， 但也为写入数据带来了安全问题， 因为如果计算机发生停机， 那么保存在内存缓冲区里面的写入数据将会丢失。为此， 系统提供了 fsync 和 fdatasync 两个同步函数， 它们可以强制让操作系统立即将缓冲区中的数据写入到硬盘里面， 从而确保写入数据的安全性。 上图可以看出，appendfsync的值会很大的影响redis的性能，append、写入、同步到磁盘都是同步的 AOF 重写AOF 持久化并不是没有缺点的，Redis 会不断将接收到的写命令追加到 AOF 文件中，导致 AOF 文件越来越大。过大的 AOF 文件会消耗磁盘空间，并且导致 Redis 重启时更加缓慢。为了解决这个问题，在适当情况下，Redis 会对 AOF 文件进行重写，去除文件中冗余的命令，以减小 AOF 文件的体积（Ps：比如对某个key执行了两次写操作，第二次会覆盖第一次，那么AOF再保存第一次的写操作其实意义并不大）。 AOF的重写会执行大量的写入操作，Redis是单线程的，所以如果有服务器直接调用重写，服务器就不能处理其他命令了，因此Redis服务器新起了单独一个进程来执行AOF重写。 在子进程执行AOF重写时，服务端接收到客户端的命令之后，先执行客户端发来的命令，然后将执行后的写命令追加到AOF缓冲区中，同时将执行后的写命令追加到AOF重写缓冲区中。 等到子进程完成了重写工作后(Ps都是写到重写缓冲区)，会发一个完成的信号给服务器，服务器就将AOF重写缓冲区中的所有内容追加到AOF文件中，然后原子性地覆盖现有的AOF文件（Ps注意这里强调的原子性）。 题外话：提到IO经常要提到缓存，这个缓存是干啥的？进程执行I/O操作，就是向操作系统发出请求，让它要么把缓冲区的数据排干（写），要么用数据把缓冲区填满（读）。进程使用这一机制处理所有数据进出操作。简单地说，数据在内核空间和用户空间『穿梭』成本较高，所以无论是读还是写都希望能够凑齐一波一起去执行，这样可以均摊『跨空间』的成本。而在凑齐一波之前数据在的区域就是缓冲区 AOF优点1）比RDB可靠。你可以制定不同的 fsync 策略：no、everysec 和 always。默认是 everysec。这意味着你最多丢失一秒钟的数据。2）AOF日志文件是一个纯追加的文件。就算是遇到突然停电的情况，也不会出现日志的定位或者损坏问题。3）当AOF文件太大时，Redis 会自动在后台进行重写。重写很安全（Ps：原子性），因为重写是在一个新的文件上进行，同时 Redis 会继续往旧的文件追加数据。新文件上会写入能重建当前数据集的最小操作命令的集合（Ps：AOF瘦身）。当新文件重写完，Redis 会把新旧文件进行切换，然后开始把数据写到新文件上（Ps：原子性）。4）AOF 把操作命令以简单易懂的格式一条接一条的保存在文件里，很容易导出来用于恢复数据。例如我们不小心用 FLUSHALL 命令把所有数据刷掉了，只要文件没有被重写，我们可以把服务停掉，把最后那条命令删掉，然后重启服务，这样就能把被刷掉的数据恢复回来。 AOF 的缺点1）在相同的数据集下，AOF 文件的大小一般会比 RDB 文件大。 2）在某些 fsync 策略下，AOF 的速度会比 RDB 慢(非异步)。通常 fsync 设置为每秒一次就能获得比较高的性能，而在禁止 fsync 的情况下速度可以达到 RDB 的水平。 小结：RDB和AOF的优缺点 RDB AOF 优点 简单的备份单文件，适合灾备和数据恢复不同版本；性能较好会fork一个新进程完成IO操作 最好的情况下，最多丢失某次的更新数据；AOF文件过大时候会执行重写（瘦身），而且新旧AOF的替换操作是原子性的 缺点 容易造成数据丢失；如果数据较大，fork子进程会占用较多cpu时间，甚至造成redis停止服务几毫秒；fork是基于写时拷贝，如果客户端频繁的写，那么将增加redis占用的内存 相同数据集下，AOF文件一般比RDB大；AOF的速度一般比RDB慢（同步） Redis主从复制简介Redis 作为单机数据库使用时，使用场景有限且存在单点宕机问题，无法维持高可用。因此 Redis 允许通过 SLAVEOF 命令或者 slaveof 配置项来让一个 Redis server 复制另一个 Redis server 的数据集和状态，我们称之为主从复制。主服务器下文称 master，从服务器下文称 slave，Redis 采用异步的复制机制。复制机制的运行依靠三个特性： 1）当一个 master 和一个 slave 连接正常时，master 会发送一连串的命令流来保持对 slave 的更新，以便于将自身数据集的变更复制给 slave ：包括客户端的写入、key 的过期或被逐出等 2）当 master 和 slave 之间的连接断开后（断开的原因可能是网络问题或者连接超时） slave 重连上 master 并尝试进行部分重同步，这意味着它只会尝试获取在断开连接期间内丢失的命令流 3）当无法进行部分重同步时， slave 会请求进行全量重同步。这会涉及到一个更复杂的过程，例如 master 需要创建所有数据的快照，将之发送给 slave ，之后在数据集更改时持续发送命令流到 slave 优点1）master 可以关闭持久化机制，减少不必要的 IO 操作且降低延迟，对于以性能著称的组件来说极为重要 2）slave 虽然不能处理写请求，但是可以处理读请求，从而增加读取操作的吞吐量。但由于复制机制的原因，主从数据存在不一致的时间窗口（Ps：作为缓冲中间件来讲，主从不一致影响并不大） 3）使得 Redis 可以告别单机版本的单点风险，采用副本形式提高可用性，在 master 宕机时可以将 slave 提升为 master 继续向外提供服务 Ps：常说的主从复制和集群的区别是什么？主从复制一般包含一个master和若干slave，slave 在复制机制的场景下，可以提供故障恢复、分担读流量和数据备份的功能。 集群机制一大特点是，不同集群中存放的数据不同。集群机制的使用意味着你的数据量较大，数据会根据 Key 计算出的 slot 值自动在多个分片上进行分区(Partitioning)，客户端对某个 Key 的请求会被转发到持有那个 Key 的分片上。分片由一个 master 和若干个 slave 组成，二者间通过复制机制同步数据。因此总结来看，集群模式更像分区和复制机制的组合 原理主从复制过程可分为三个阶段：复制初始化、数据同步和命令传播。1）复制初始化：slave和master之间建立tcp连接2）数据同步：slave向master发送psync命令，master判断是进行增量同步还是全量同步（该阶段一般未全量同步）3）命令传播：完成数据同步后，master和slave需要通过心跳包来确认对方是否在线，master向slave发送PING命令，slave向master发送REPLCONF ACK命令，该命令还包含slave保存数据的复制偏移量（master会比对偏移量向slave发送未同步的命令） Redis集群简介大规模数据存储系统都会面临的一个问题就是如何横向拓展。当你的数据集越来越大，一主多从的模式已经无法支撑这么大量的数据存储，于是你首先考虑将多个主从模式结合在一起对外提供服务 Redis Cluster 简介常见的三种方案1）将Proxy放到客户端：分发压力放到客户端，缓解了服务器压力；不能平滑扩容or下线，运维成本高2）将Proxy放到服务端：对客户端透明；服务端压力增大，可能会影响性能，难以做到平滑扩容or下线3）官方方案Redis Cluster：无中心节点，数据按照 slot 存储分布在多个 Redis 实例上，平滑的进行扩容/缩容节点，自动故障转移（节点之间通过 Gossip 协议交换状态信息,进行投票机制完成 slave 到 master 角色的提升）降低运维成本，提高了系统的可扩展性和高可用性。一般将同一分片的master和slave部署在不同机房，以增加容灾能力客户端可以和任意redis相连接，如果当前key不位于当前分片，那么会收到一个move请求，之后再去请求新的分片 Redis应用场景1）缓存2）消息队列：list的一个双向链表，可以通过lpush(left)将消息放到左侧，通过rpop(right)将右侧的消息取出3）分布式锁：setnx+expire+del三个操作实现4）倒排索引：搜索beijing想达到和输入『北京』效果相同，可以考虑将beijing作为key，北京作为value Redis为啥这么快？Redis采用的是基于内存的采用的是单进程单线程模型的 KV 数据库，由C语言编写，官方提供的数据是可以达到100000+的QPS（每秒内查询次数）1、完全基于内存，绝大部分请求是纯粹的内存操作，非常快速。数据存在内存中，类似于HashMap，HashMap的优势就是查找和操作的时间复杂度都是O(1)； 2、数据结构简单，对数据操作也简单，Redis中的数据结构是专门进行设计的； 3、采用单线程，避免了不必要的上下文切换和竞争条件，也不存在多进程或者多线程导致的切换而消耗 CPU，不用去考虑各种锁的问题，不存在加锁释放锁操作，没有因为可能出现死锁而导致的性能消耗； 4、使用多路I/O复用模型，非阻塞IO；(nio单线程模型的为数不多的应用) Ps1）作为一个内存数据库，所需要的一切数据都在内存中显然单线程就是最快的2）当需要使用下层存储的时候（比如磁盘）这个时候推荐使用多线程方案]]></content>
      <categories>
        <category>Java中间件</category>
      </categories>
      <tags>
        <tag>Java中间件</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[tcp window]]></title>
    <url>%2F2018%2F09%2F12%2Ftcp-window%2F</url>
    <content type="text"><![CDATA[Outline1.对比了拥塞控制和流量控制2.拥塞控制的细节 tcp概念TCP四元组：[源ip，源端口号，目的ip，目的端口号] TCP（Transmission Control Protocol 传输控制协议）是一种面向连接的、可靠的、基于字节流的传输层通信协议。 三个概念：面向连接：必须在建立连接前确认对方链路可达并准备好才开始通信，建立后对状态的保持。（udp直接通信，不会询问接收端是否可达并准备接收数据） 字节流：tcp传输的是byte类型数据。 可靠：保证数据报能够到达接收端（超时重传机制、丢包重传及数据顺序一致） 拥塞控制和流量控制对比 拥塞控制 流量控制 窗口 拥塞窗口(cwnd) 滑动窗口(rwnd) 窗口大小定义者 发送方 接收方 目的 为了在网络通畅的时候多发送一些数据，在网络堵塞的时候少发一些数据 为了防止发送端发送数据过快（字节/时间），导致数据丢失 控制范围 是一个全局性的过程，涉及到所有的主机、路由器，以及与降低网络传输性能有关的所有因素 点对点通信量的控制，是端到端的问题 单位 报文段 字节 最早先有滑动窗口，接收方告知发送方当前可以接收的量，这在局域网一般没问题，但是一旦到网络中可能因为每个连接发送的数据量过大而导致出现堵塞的状况，而且如果再有重试机制可能会导致雪崩。为此引入了拥塞窗口，发送端根据当前的网络状况决定发送数据的多少。 Ps滑动窗口和拥塞窗口接收发送两端都有 拥塞控制的一些细节拥塞控制的几种算法：慢开始( slow-start )、拥塞避免( congestion avoidance )、快重传( fast retransmit )和快恢复( fast recovery )最开始指数增长阶段就是慢开始算法、而线性增长阶段是拥塞避免算法Ps:更新后的ssthresh值变为出现超时时的拥塞窗口数值的一半 快重传算法1）首先要求接收方每收到一个失序的报文段后就立即发出重复确认（为的是使发送方及早知道有报文段没有到达对方）而不要等到自己发送数据时才进行捎带确认。 2）算法还规定，发送方只要一连收到三个重复确认就应当立即重传对方尚未收到的报文段Mx，而不必继续等待Mx设置的重传计时器到期。由于发送方尽早重传未被确认的报文段，因此采用快重传后可以使整个网络吞吐量提高约20%。 与快重传配合使用的还有快恢复算法1）当发送方连续收到三个重复确认，就执行“乘法减小”算法，把慢开始门限ssthresh减半（当前cwind的一半）。这是为了预防网络发生拥塞（这里想想就清楚了，因为接下来要进行重传再加上正常的传输可能会发送拥塞）。请注意：接下去不执行慢开始算法。 2）由于发送方现在认为网络很可能没有发生拥塞，因此与慢开始不同之处是现在不执行慢开始算法（即拥塞窗口cwnd现在不设置为1），而是把cwnd值设置为慢开始门限ssthresh减半后的数值，然后开始执行拥塞避免算法（“加法增大”），使拥塞窗口缓慢地线性增大。 总结发送方窗口的上限值 = Min [ rwnd, cwnd ] 当rwnd &lt; cwnd 时，是接收方的接收能力限制发送方窗口的最大值。 当cwnd &lt; rwnd 时，则是网络的拥塞限制发送方窗口的最大值。 参考链接：https://blog.csdn.net/yechaodechuntian/article/details/25429143]]></content>
      <categories>
        <category>I/O</category>
      </categories>
      <tags>
        <tag>I/O</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[maven基础知识]]></title>
    <url>%2F2018%2F09%2F11%2Fmaven%2F</url>
    <content type="text"><![CDATA[Outline1.import 和 parent2.dependencyManagement和dependency3.maven plugin4.maven scope import 和 parent子model可以使用parent继承某个父model但是，问题也出现了：单继承：maven的继承跟java一样，单继承，也就是说子model中只能出现一个parent标签；parent模块中，dependencyManagement中预定义太多的依赖，造成pom文件过长，而且很乱；如何让这些依赖可以分类并清晰的管理？问题解决：import scope依赖 注意：scope=import只能用在dependencyManagement里面,且仅用于type=pom的dependency总结：pom的多重继承通过将多个欲继承的model放到同一个model中来实现，而为了让父model中的依赖们更清晰的管理引入了import(将另一个pom文件以import的方式引入而不是平铺在当前model中) dependencyManagement和dependency在maven多模块项目中，为了保持模块间依赖的统一，常规做法是在parent model中，使用dependencyManagement预定义所有模块需要用到的dependency(依赖)dependencyManagement只会影响现有依赖的配置，但不会引入依赖，即子model不会继承parent中dependencyManagement所有预定义的dependency，只引入需要的依赖即可，简单说就是“按需引入依赖”或者“按需继承”在parent中严禁直接使用dependencies预定义依赖，因为子model会自动继承dependencies中所有预定义依赖 dependenciesManagement标签中声明的依赖并不会被子项目直接继承，在子项目中需要声明，但是只需要声明groupId和artifactId dependencies标签中的依赖会被子项目完全继承,即父项目中有的依赖都会出现在子项目中 maven plugin maven-resources-plugin: copy resources not in src/main/resources to target/classes maven-surefire-plugin: maven compatible junit maven-war-plugin: 打war包时候的配置项，这里主要是指定包的名字 maven-eclipse-plugin: The Maven Eclipse Plugin is used to generate Eclipse IDE files (.classpath, .project, *.wtpmodules and the .settings folder) for use with a project. maven-deploy-plugin: to add your artifact(s) to a remote repository maven scope compile：默认值 他表示被依赖项目需要参与当前项目的编译，还有后续的测试，运行周期也参与其中，是一个比较强的依赖。打包的时候通常需要包含进去 test：依赖项目仅仅参与测试相关的工作，包括测试代码的编译和执行，不会被打包，例如：junit runtime：表示被依赖项目无需参与项目的编译，不过后期的测试和运行周期需要其参与。与compile相比，跳过了编译而已。例如JDBC驱动，适用运行和测试阶段 provided：打包的时候可以不用包进去，别的设施会提供。事实上该依赖理论上可以参与编译，测试，运行等周期。相当于compile，但是打包阶段做了exclude操作]]></content>
      <categories>
        <category>Java Web</category>
      </categories>
      <tags>
        <tag>Java Web</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[关于Tomcat]]></title>
    <url>%2F2018%2F09%2F05%2Ftomcat-all%2F</url>
    <content type="text"><![CDATA[Tomcat真是不能言说的伤，看了好几遍了总是再门外徘徊本来想着以后再也不看了，但是前阵子在掘金上又看了一篇讲解的特别清晰的文章自己也想把Tomcat的内容总结下（搬运下） 一、Tomcat顶层架构一图胜千言（1）Tomcat中只有一个Server（代表整个服务器），一个Server包含多个Service，一个Service可以有多个Connector和一个Container（2）Connector用于接收请求并将请求封装成Request和Response来具体处理；最底层使用Socket实现，而request和response是按HTTP协议封装的，所以Connector要同时支持TCP/IP和HTTP两种协议（3）Container用于封装和管理Servlet，以及具体处理request请求 二、Connector架构分析Connector就是使用ProtocolHandler来处理请求的，不同的ProtocolHandler代表不同的连接类型，比如：Http11Protocol使用的是普通Socket来连接的，Http11NioProtocol使用的是NioSocket来连接的。 其中ProtocolHandler由包含了三个部件：Endpoint、Processor、Adapter。 （1）Endpoint用来处理底层Socket的网络连接，Processor用于将Endpoint接收到的Socket封装成Request，Adapter用于将Request交给Container进行具体的处理。 （2）Endpoint由于是处理底层的Socket网络连接，因此Endpoint是用来实现TCP/IP协议的，而Processor用来实现HTTP协议的，Adapter将请求适配到Servlet容器进行具体的处理。 （3）Endpoint的抽象实现AbstractEndpoint里面定义的Acceptor和AsyncTimeout两个内部类和一个Handler接口。Acceptor用于监听请求，AsyncTimeout用于检查异步Request的超时，Handler用于处理接收到的Socket，在内部调用Processor进行处理。 Ps Container一般通过HTTP协议和外界交流，AJP仅仅适用于部分apache项目比如apache服务器 三、Container架构分析4个子容器的作用分别是： （1）Engine：引擎，用来管理多个站点，一个Service最多只能有一个Engine；（2）Host：代表一个站点，也可以叫虚拟主机，通过配置Host就可以添加站点；（3）Context：代表一个应用程序，对应着平时开发的一套程序，或者一个WEB-INF目录以及下面的web.xml文件；（4）Wrapper：每一Wrapper封装着一个Servlet； pipeline模式在Tomcat中的应用每个Pipeline都有特定的Valve，而且是在管道的最后一个执行，这个Valve叫做BaseValve，BaseValve是不可删除的当执行到StandardWrapperValve的时候，会在StandardWrapperValve中创建FilterChain，并调用其doFilter方法来处理请求，这个FilterChain包含着我们配置的与请求相匹配的Filter和Servlet，其doFilter方法会依次调用所有的Filter的doFilter方法和Servlet的service方法，这样请求就得到了处理！ 四、Tomcat和类加载器之前看JVM一直对类加载器不甚理解，最近看了另外一篇文章有了一定的认识有一点需要认识到，类加载器是有作用范围的，一般某个特定的类加载器只能加载位于特定目录下的类 首先，我们来问个问题：Tomcat 如果使用默认的类加载机制行不行？我们思考一下：Tomcat是个web容器， 那么它要解决什么问题： 一个web容器可能需要部署两个应用程序，不同的应用程序可能会依赖同一个第三方类库的不同版本，不能要求同一个类库在同一个服务器只有一份，因此要保证每个应用程序的类库都是独立的，保证相互隔离。 部署在同一个web容器中相同的类库相同的版本可以共享。否则，如果服务器有10个应用程序，那么要有10份相同的类库加载进虚拟机，这是扯淡的。 web容器也有自己依赖的类库，不能于应用程序的类库混淆。基于安全考虑，应该让容器的类库和程序的类库隔离开来。 web容器要支持jsp的修改，我们知道，jsp 文件最终也是要编译成class文件才能在虚拟机中运行，但程序运行后修改jsp已经是司空见惯的事情，否则要你何用？ 所以，web容器需要支持 jsp 修改后不用重启。 再看看我们的问题：Tomcat 如果使用默认的类加载机制行不行？答案是不行的。为什么？我们看，第一个问题，如果使用默认的类加载器机制，那么是无法加载两个相同类库的不同版本的，默认的类加载器是不管你是什么版本的，只在乎你的全限定类名，并且只有一份。第二个问题，默认的类加载器是能够实现的，因为他的职责就是保证唯一性。第三个问题和第一个问题一样。我们再看第四个问题，我们想我们要怎么实现jsp文件的热修改（楼主起的名字），jsp 文件其实也就是class文件，那么如果修改了，但类名还是一样，类加载器会直接取方法区中已经存在的，修改后的jsp是不会重新加载的。那么怎么办呢？我们可以直接卸载掉这jsp文件的类加载器，所以你应该想到了，每个jsp文件对应一个唯一的类加载器，当一个jsp文件修改了，就直接卸载这个jsp类加载器。重新创建类加载器，重新加载jsp文件。 Tomcat的类加载器我们看到，前面3个类加载和默认的一致，CommonClassLoader、CatalinaClassLoader、SharedClassLoader和WebappClassLoader则是Tomcat自己定义的类加载器，它们分别加载/common/、/server/、/shared/（在tomcat 6之后已经合并到根目录下的lib目录下）和/WebApp/WEB-INF/中的Java类库。其中WebApp类加载器和Jsp类加载器通常会存在多个实例，每一个Web应用程序对应一个WebApp类加载器，每一个JSP文件对应一个Jsp类加载器。 WebApp可以视为一个个web应用程序使用的加载器，Jsp类加载器视为一个个Jsp页面使用的加载器 Common ClassLoader能加载的类都可以被Catalina ClassLoader和SharedClassLoader使用，从而实现了公有类库的共用，而CatalinaClass Loader和Shared ClassLoader自己能加载的类则与对方相互隔离。WebApp ClassLoader可以使用Shared ClassLoader加载到的类，但各个WebApp ClassLoader实例之间相互隔离。而JasperLoader的加载范围仅仅是这个JSP文件所编译出来的那一个.Class文件，它出现的目的就是为了被丢弃：当Web容器检测到JSP文件被修改时，会替换掉目前的JasperLoader的实例，并通过再建立一个新的Jsp类加载器来实现JSP文件的HotSwap功能。 后记：其实破坏双亲委派模型很简单，只要覆写父类中的loadClass()方法就行了，把委派的逻辑去掉改为自己去加载类。至于破坏的目的，拿tomcat来讲，主要是隔离（类的不同版本）、热插拔 参考链接：https://blog.csdn.net/xlgen157387/article/details/79006434https://blog.csdn.net/qq_38182963/article/details/78660779]]></content>
      <categories>
        <category>Java Web</category>
      </categories>
      <tags>
        <tag>Java Web</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring-IOC骨架]]></title>
    <url>%2F2018%2F09%2F01%2FSpring-IOC-Skeleton%2F</url>
    <content type="text"><![CDATA[试着把自己看源码对一些理解写下来从相对宏观的角度去分析整个过程码还是从经典的例子开始入手123456789101112private void applicationEntrance()&#123; ApplicationContext context = new FileSystemXmlApplicationContext("classpath:applicationContext.xml"); context.getBean("account");&#125;public ClassPathXmlApplicationContext(String[] configLocations, boolean refresh, ApplicationContext parent)&#123; super(parent); setConfigLocations(configLocations); if (refresh) &#123; refresh(); &#125;&#125; 要开车了，坐稳了，我们逐步进行分析首先调用new FileSystemXmlApplicationContext(xml)方法，最后发现方法来到了refresh()方法，这个可以一个大名鼎鼎的方法，凡是看过或者尝试看过spring源码对同学应该都对这个方法不陌生，这是一个标准的模板方法模式，由父类指定整个代码的执行逻辑，由子类去决定每个方法的具体实现。额，还是上代码吧重要的地方会写注释123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081refresh() //applicaitonContext其实就是高配版本的Beanfactory所以很多工作还是要BeanFactory来做 //去初始化beanFactory,包括(1）读取xml (2）将xml中的bean转为beanDefination (3）并注册到容器中 1)obtainFreshBeanFactory() refreshBeanFactory() //创建BeanFactory createBeanFactory() loadBeanDefinitions(beanFactory) loadBeanDefinitions(demo.xml) 1.读取配置文件 //将配置文件抽象为resource resourceLoader.load(String confLocation) 容器读取resource 2.将配置文件中的bean转为spring中的格式BeanDefination并注册到容器中 //将resource转为document Document doc = doLoadDocument(inputSource, resource) //获取xml的验证模式 getValidationModeForResource(resource) 判断是DTD or XSD（根据是否具有DOCTYPE） //对xml进行解析获取document（根据验证模式） this.documentLoader.loadDocument(inputSource, getEntityResolver(), this.errorHandler,getValidationModeForResource(resource),isNamespaceAware()); //注册beanDefination registerBeanDefinitions(doc, resource) 将document转为beanDefination 根据element进行构造 对beanDefination进行校验 //beanName-BeanDefinition存放在Map&lt;String, BeanDefinition&gt; beanDefinitionMap this.beanDefinitionMap.put(beanName, beanDefinition); //大佬在这里被调用，唯一可以插手容器启动阶段的一个bean，可以对beanDefinition进行修改 //对该类型bean会提前执行BeanFactory.getBean() 2)invokeBeanFactoryPostProcessors() //注册BeanPostProcessor //真正干活的还是beanFactory = =! 3)registerBeanPostProcessors(beanFactory) //实例化所有单例非懒加载beans 4)finishBeanFactoryInitialization(beanFactory) 遍历 beanDefinitionMap找到单例非抽象非懒加载的bean调用BeanFactory的getBean(beanName) 单例bean先从缓冲中读取，如果没有才去创建；反之直接返回 获取depend-on到bean先去对这些bean调用getBean() //以单例bean为例子 createBean() //这里要和postProcessBeforeInitialization区分 InstantiationAwareBeanPostProcessor#postProcessBeforeInstantiation,如果这里返回的bean非空那么直接return（短路了） //核心类 AbstractAutowireCapableBeanFactory //核心方法，创建bean-&gt;填充-&gt;初始化 doCreateBean(beanName,beanDefination,args) //创建bean的实例并用BeanWrapper包裹 1、createBeanInstance(beanName, mbd, args) instantiateBean(beanName, mbd) //策略模式决定如何实例化bean cglib or reflect //反射策略居然是cglib策略的父类= =！ //对于look-up之类的功能需要cglib来支持 getInstantiationStrategy().instantiate(mbd, beanName, parent) BeanWrapper bw = new BeanWrapperImpl(beanInstance); initBeanWrapper(bw); return bw //对BeanWrapper进行属性填充 2、populateBean(beanName, mbd, instanceWrapper) 先去从BeanDefination中获取property属性放到pv中 InstantiationAwareBeanPostProcessor#postProcessAfterInstantiation 代码来到了byName byType //注意beanDefination中的pv仅仅表示配置的属性，而非全属性 以byName为例，先去遍历已经实例化的beanWrapper中的set/is开头的方法 再去判断这个属性是否是一个bean，如果是将对应的bean放到pv中稍后一起填充 InstantiationAwareBeanPostProcessor#postProcessPropertyValues //属性填充 applyPropertyValues(beanName, mbd, bw, pvs) //初始化bean 3、initializeBean(beanName, exposedObject, mbd) //这里对实现各种aware结尾接口的相关属性进行注入 invokeAwareMethods(beanName, bean) //beanPostProcessor的前置方法 applyBeanPostProcessorsBeforeInitialization //调用bean的初始化方法 invokeInitMethods(beanName, wrappedBean, mbd) 先执行InitializingBean类型的bean都afterPropertiesSet()方法 再执行&lt;bean init-method&gt; //beanPostProcessor的后置方法 applyBeanPostProcessorsAfterInitialization //注册DisposableBean和&lt;destroy-bean&gt; 4、registerDisposableBeanIfNecessary AbstractAutowireCapableBeanFactory这个类是很重要的一个类]]></content>
      <categories>
        <category>Spring</category>
      </categories>
      <tags>
        <tag>Spring</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JVM]]></title>
    <url>%2F2018%2F08%2F20%2FJVM-1%2F</url>
    <content type="text"><![CDATA[Outline0.最近在看《深入理解JVM》对全书做了一个思维导图，也方便以后的回顾（提纲挈领嘛），看不清楚的小伙伴可以download到本地，图片还是很清晰的1.读书过程中的一些想法与体会也顺带记录了下，欢迎大家批评指正 一个*.java到真正执行都经历了什么？ 首先.java文件通过javac编译器生成字节码文件.classPs：Java号称一次编写，到处运行，这里的.class是和平台无关的，这个时候需要不同平台的JVM对同一份.class文件进行翻译了在生成了class文件后，虽然可以根据class文件的内容去阅读具体的细节，但是比较『反人类』，这个时候可以使用javap去『去伪存真』，比如一些语法糖，让我们可以阅读到离机器最近的code，有时候方便排查一些『莫名其妙』的问题（乱用成语的习惯。。。）javap -verbose xx.class 除了代码外还可以查看常量池的信息 加载理论上来讲类的加载是讲究时机的，具体可以参考导图中初始化的触发时机（初始化发生前一定会先执行加载操作）。当类被加载的时候，会做三件事情1）通过类的全限定名获取此类的二进制流2）将这个字节流所代表的静态存储结构转换为方法区运行时的数据结构，包括class中的常量池保存到JVM内存的运行时常量池、Class&amp;Method&amp;Field的信息保存到方法区Ps：简单的说类的全部信息都保存在方法区中，包括类的版本、字段、方法等的描述信息。再看看一些细节的东西将class文件中保存的一些内容存储到真正的运行时的内存中，比如class文件中的常量池，包括字面量和符号引用，要存储到JVM运行时的常量池中；将类的版本信息、方法、字段等的描述信息，存储到方法区中。3）在堆中生成这个类的class对象，作为访问方法区中该类信息的入口（通过反射去拿类的信息） 验证这步没啥好说的，就是对class文件进行校验，防止class会损害虚拟机 准备两件事，第一，为类变量分配内存并赋默认值；第二，为final修饰的变量进行赋值操作（非默认值） 解析严格意义上讲，解析可以在初始化之前也可以在其之后。这步骤的工作就是将常量池中的符号引用转为直接引用比如上面的object（符号引用）转为堆上的直接引用对于初始化前的方法解析主要针对一些运行期间不可变的方法比如final、private、static等而对于初始化后的方法解析主要针对一些运行期绑定的方法（指的是真正运行时候，而不是说初始化之后立刻去解析）还是举个例子吧 123private void fun(Father f) &#123; f.fun1();&#125; 假设调用上面这个方法，而入参为Father的子类Son且Son也覆写了fun1方法，那么程序最终执行的父类的方法还是子类的方法是不得而知的，只有在程序真正运行时候才能确定，称之为运行时绑定 初始化执行clinit方法，该方法包含类变量的赋值操作（非final）和static语句块中的操作 至此该类已经可以正常使用了 内存回收实战这里是笔者公司内部的一个case，已经隐去了具体的涉密信息，展示一个框架给大家看，但是也能说明问题了。123456789for(int id : idList) &#123; ExecutorService threadPool = Executors.newSingleThreadExucutor(); threadPool.execute(new Runnable()&#123; @Override public void run() &#123; doBusiness(id); &#125; &#125;);&#125; 这段代码会引起频繁的YGC，和OOM（具体OOM原因视JVM配置参数而定） 显然是因为在for循环了创建了太多线程池，但是又没有执行shutdown操作，导致有很多活跃的核心线程，最终导致了栈的OOM（也可能是堆OOM视配置而定），而YGC是因为线程池内部的一些对象没有被回收，看过线程池内部原理的同学应该知道Worker之类的对象 内存分配的策略1.对象优先分配到Eden区，Eden区满了会触发一次Young GC(新生代对象朝生夕死 Young GC非常频繁并且回收速度较快 Old GC速度一般是前者的10倍)2.大对象（阈值可配）直接进入老年代，避免在Eden去和Survivor区间进行大量的copy3.长期存活的对象进入老年代，Survivor区每熬过一次Young GC年龄就+1，增加到一定数值，就会晋升到老年带中4.发生Young GC时，虚拟机如果发现之前每次晋升到老年代的平均大小大于老年代的剩余空间，就执行一次Full GC；如果小于查看是否允许担保失败，如果允许那么久执行Young GC；反之执行Full GC5.担保机制：让Survivor中无法容纳的对象直接晋升老年带 总结：Young GC经常发生 而且速度很快 触发条件是Eden区满Full GC发生条件：1.System.gc() 此方法建议执行full gc 但是未必会执行 一般禁用2.发生Young GC时如果发现当前老年代剩余空间小于之前每次晋升对象的平均大小那么执行Full GC3.发生Young GC时如果空间分配担保机制不允许失败那么执行Full GC JAVA内存泄露定义：对象已经没有程序使用了但是gc无法移除，就是内存泄露eg1.顺序构造链表时候，可能会使用哨兵节点，而最后返回的是哨兵.next，此时哨兵就是内存泄露2.LinkedList移除节点时候会将双向链表都断开也是为了释放内存3.数组实现stack出栈操作 return element[–size] 此时element[size]内存泄露4.循环中创建线程池 忘记调用shutdown5.长生命周期对象（v）持有短生命周期的对象（o）123456Vector v = new Vector(10);for (int i = 0; i &lt; 100; i++) &#123; Object o = new Object(); v.add(o); o = null;&#125; o = null本意是释放对象，但是由于v并未释放所以导致内存泄露 不同内存区域的OOM1.栈无限递归2.堆死循环向容器内塞大对象3.方法区3.1 常量池String.valueOf(i++).intern 死循环执行这行代码3.2 方法区运行时产生大量的类去填充方法区 Ps： cglib or reflect 结合例子来看.class文件的常量池123456789101112public class TestJavap &#123; private int n = 1; private String mstring = "123"; public int c; public void f() &#123;&#125; private final int a = 4; public static void main(String[] args) &#123; String d = "string"; System.out.println(d); &#125;&#125; javap反编译后结果如下123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124Classfile /Users/luyu/git/tech-lab/target/classes/com/Test/TestJavap.class Last modified 2018-9-17; size 799 bytes MD5 checksum bb7043f39135f0e9f9cfc397f6d96a65 Compiled from "TestJavap.java"public class com.Test.TestJavap minor version: 0 major version: 49 flags: ACC_PUBLIC, ACC_SUPERConstant pool: #1 = Methodref #10.#34 // java/lang/Object."&lt;init&gt;":()V #2 = Fieldref #9.#35 // com/Test/TestJavap.n:I #3 = String #36 // 123 #4 = Fieldref #9.#37 // com/Test/TestJavap.mstring:Ljava/lang/String; #5 = Fieldref #9.#38 // com/Test/TestJavap.a:I #6 = String #39 // string #7 = Fieldref #40.#41 // java/lang/System.out:Ljava/io/PrintStream; #8 = Methodref #42.#43 // java/io/PrintStream.println:(Ljava/lang/String;)V #9 = Class #44 // com/Test/TestJavap #10 = Class #45 // java/lang/Object #11 = Utf8 n #12 = Utf8 I #13 = Utf8 mstring #14 = Utf8 Ljava/lang/String; #15 = Utf8 c #16 = Utf8 a #17 = Utf8 ConstantValue #18 = Integer 4 #19 = Utf8 &lt;init&gt; #20 = Utf8 ()V #21 = Utf8 Code #22 = Utf8 LineNumberTable #23 = Utf8 LocalVariableTable #24 = Utf8 this #25 = Utf8 Lcom/Test/TestJavap; #26 = Utf8 f #27 = Utf8 main #28 = Utf8 ([Ljava/lang/String;)V #29 = Utf8 args #30 = Utf8 [Ljava/lang/String; #31 = Utf8 d #32 = Utf8 SourceFile #33 = Utf8 TestJavap.java #34 = NameAndType #19:#20 // "&lt;init&gt;":()V #35 = NameAndType #11:#12 // n:I #36 = Utf8 123 #37 = NameAndType #13:#14 // mstring:Ljava/lang/String; #38 = NameAndType #16:#12 // a:I #39 = Utf8 string #40 = Class #46 // java/lang/System #41 = NameAndType #47:#48 // out:Ljava/io/PrintStream; #42 = Class #49 // java/io/PrintStream #43 = NameAndType #50:#51 // println:(Ljava/lang/String;)V #44 = Utf8 com/Test/TestJavap #45 = Utf8 java/lang/Object #46 = Utf8 java/lang/System #47 = Utf8 out #48 = Utf8 Ljava/io/PrintStream; #49 = Utf8 java/io/PrintStream #50 = Utf8 println #51 = Utf8 (Ljava/lang/String;)V&#123; public int c; descriptor: I flags: ACC_PUBLIC public com.Test.TestJavap(); descriptor: ()V flags: ACC_PUBLIC Code: stack=2, locals=1, args_size=1 0: aload_0 1: invokespecial #1 // Method java/lang/Object."&lt;init&gt;":()V 4: aload_0 5: iconst_1 6: putfield #2 // Field n:I 9: aload_0 10: ldc #3 // String 123 12: putfield #4 // Field mstring:Ljava/lang/String; 15: aload_0 16: iconst_4 17: putfield #5 // Field a:I 20: return LineNumberTable: line 6: 0 line 7: 4 line 8: 9 line 11: 15 LocalVariableTable: Start Length Slot Name Signature 0 21 0 this Lcom/Test/TestJavap; public void f(); descriptor: ()V flags: ACC_PUBLIC Code: stack=0, locals=1, args_size=1 0: return LineNumberTable: line 10: 0 LocalVariableTable: Start Length Slot Name Signature 0 1 0 this Lcom/Test/TestJavap; public static void main(java.lang.String[]); descriptor: ([Ljava/lang/String;)V flags: ACC_PUBLIC, ACC_STATIC Code: stack=2, locals=2, args_size=1 0: ldc #6 // String string 2: astore_1 3: getstatic #7 // Field java/lang/System.out:Ljava/io/PrintStream; 6: aload_1 7: invokevirtual #8 // Method java/io/PrintStream.println:(Ljava/lang/String;)V 10: return LineNumberTable: line 14: 0 line 15: 3 line 16: 10 LocalVariableTable: Start Length Slot Name Signature 0 11 0 args [Ljava/lang/String; 3 8 1 d Ljava/lang/String;&#125;SourceFile: "TestJavap.java" 1）final修饰的a，它的值4也在常量池，4就是所谓的字面量2）字符串常量”123”也在常量池，”123”也是所谓的字面量3）基本上类中出现的符号都会在常量池中，比如n、c、a、mstring、d也都在常量池中4）除此之外还有域的类型，方法的返回值类型、入参类型、入参名字5）方法内部的”string”并不在常量池中6）我们可以理解为类（父类信息）、域（修饰符等）、方法（修饰符等）的实际信息都存放在方法区中，而那些符号和字面量存在常量池。终于搞懂了字面量和符号引用的意思，实践是检验整理的唯一标准，古人诚不欺我 再来宏观看下jvm运行时的数据区重点关注方法区（jdk8改为metaspace），包括1）运行时常量池(Runtime Constant Pool)，包括class文件中的常量池和运行时的string.intern()2）类信息(Class &amp; Field &amp; Method data)3）即时编译器编译后的代码(Code)等等 什么是动态连接如何理解动态连接？我们知道Class文件的常量池中存有大量的符号引用，在加载过程中会被原样的拷贝到内存里先放着，到真正使用的时候就会被解析为直接引用 (直接引用包含：直接指向目标的指针、相对偏移量、能间接定位到目标的句柄等)。有些符号引用会在类的加载阶段或者第一次使用的时候转化为直接引用，这种转化称为静态解析(private/final)，而有的将在运行期间转化为直接引用，这部分称为动态连接(多态)。 常用的垃圾收集器 适用周期 名称 算法 特征描述 新生代 Serial 复制 1.垃圾收集时STW2.只有一个活跃的线程去进行GC3.可以和老年代的CMS一起使用 新生代 ParNew 复制 1.Serial的多线程版本2.垃圾收集时STW3.多个活跃的线程进行GC4.可以和老年代的CMS一起使用 新生代 Parallel Scavenge 复制 1.多个活跃的线程进行GC2.吞吐量=运行用户代码时间/（用户代码时间+GC时间），关注点比较奇特是吞吐量，而不是用户线程的停顿时间3.停顿时间短用户体验较好，而吞吐量高可以最高效率利用CPU时间，适合在后台运算而不许太多交互的任务4.STW 老年代 Serial Old 标记-整理 1.Serial老年代版本2.只有一个活跃的线程去进行GC3.垃圾收集时STW 老年代 Parallel Old 标记-整理 1.Parallel Scavenge老年代版本2.多个活跃的线程进行GC3.与Parallel Scavenge一起形成了吞吐量优先的闭环4.STW 老年代 CMS(Concurrent Mark Sweep) 标记-清除 1.以获取最短回收停顿时间为目标2.分为四个步骤，初始标记（STW耗时较短）、并发标记（并发标记和用户线程并行，耗时较长）、重新标记（STW耗时较短）、并发清除（并发清除和用户线程并行，耗时较长）3.耗时较长的过程用户线程都能工作，所以用户体验较好4.缺点：CPU敏感，可能很多CPU用来处理GC；无法处理浮动垃圾（清理过程，工作线程产生的垃圾）；标记清除算法的缺点—-空间碎片 老年代 G1 标记-整理 1.非常精确地控制停顿，指定xx毫秒停顿xx毫秒2.极力避免全区域垃圾收集，将堆划分为多个独立区域，优先回收垃圾最多的区域（G1=Garbage First） GC触发条件Minor GC：当Eden区满的时候，就会执行Minor GCFull GC：1）大对象直接进入老年代，如果此时老年代空间不够那么执行Full GC2）调用System.gc时，系统建议执行Full GC，但是不必然执行3）执行了jmap -histo:live pid命令 //这个会立即触发fullgc4）在执行minor gc的时候进行的一系列检查如果开启空间担保机制，则JVM会检查老年代中最大连续可用空间是否大于了历次晋升到老年代中的平均大小，如果小于则执行改为执行Full GC。5）由Eden区、From Space区向To Space区复制时，对象大小大于To Space可用内存，则把该对象转存到老年代（空间担保机制），且老年代的可用内存小于该对象大小 Eden/From/ToFrom和To是动态的在GC开始的时候，对象只会存在于Eden区和名为“From”的Survivor区，Survivor区“To”是空的。紧接着进行GC，Eden区中所有存活的对象都会被复制到“To”，而在“From”区中，仍存活的对象会根据他们的年龄值来决定去向。年龄达到一定值(年龄阈值，可以通过-XX:MaxTenuringThreshold来设置)的对象会被移动到年老代中，没有达到阈值的对象会被复制到“To”区域。经过这次GC后，Eden区和From区已经被清空。这个时候，“From”和“To”会交换他们的角色，也就是新的“To”就是上次GC前的“From”，新的“From”就是上次GC前的“To”。不管怎样，都会保证名为To的Survivor区域是空的。Minor GC会一直重复这样的过程，直到“To”区被填满，“To”区被填满之后，会将所有对象移动到年老代中。 代码的执行顺序1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586package com.Test;/** * Created by luyu on 2018/9/26. */public class OrderTest &#123; static final int c = init2(); private static int init2() &#123; System.out.println("father static final variable"); return 0; &#125; int d = init3(); private int init3()&#123; System.out.println("father normal variable"); return 0; &#125; static int a = init(); private static int init() &#123; System.out.println("father static variable"); return 1; &#125; static &#123; System.out.println("father static &#123;&#125;"); &#125; final int b = init1(); private static int init1() &#123; System.out.println("father final variable"); return 0; &#125; public OrderTest() &#123; System.out.println("father constructor"); &#125;&#125;class OrderTestSon extends OrderTest &#123; static final int c = init2(); private static int init2() &#123; System.out.println("son staic final variable"); return 0; &#125; static &#123; System.out.println("son static &#123;&#125;"); &#125; static int a = init(); private static int init() &#123; System.out.println("son static variable"); return 1; &#125; final int b = init1(); private static int init1() &#123; System.out.println("son final variable"); return 0; &#125; int d = init3(); private int init3()&#123; System.out.println("son normal variable"); return 0; &#125; public OrderTestSon() &#123; System.out.println("son constructor"); &#125; public static void main(String[] args) &#123; new OrderTestSon(); &#125;&#125; 输出结果：father static final variablefather static variablefather static {}son staic final variableson static {}son static variablefather normal variablefather final variablefather constructorson final variableson normal variableson constructor 代码执行顺序为：父类clinit方法（包括类变量的赋值+静态代码块）、子类clinit方法、父类普通成员初始化（普通代码块）、父类构造函数、子类普通成员初始化（普通代码块）、子类构造函数注意点：final static修饰的变量会优先赋值，前提是”=”右侧是常量，如果通过方法赋值那么就是普通的clinit 《深入理解JVM》思维导图]]></content>
      <categories>
        <category>JVM</category>
      </categories>
      <tags>
        <tag>JVM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据库事务]]></title>
    <url>%2F2018%2F08%2F14%2Ftransaction%2F</url>
    <content type="text"><![CDATA[Outline1.ACID2.CAP3.BASE4.一致性协议一直都对数据库事务不甚清楚，趁着前段时间对数据库事务级别进行了总结，一起对分布式事务进行总结梳理 ACID1234start transaction;update account set money = money-100 where userId = 1;update account set money = money+100 where userId = 2;commit; A（Atomicity），原子性，一个事务内的操作，要么全成功执行，要么全部不执行。C（Consistency），一致性，事务执行的结果必须使将数据库从一种一致的状态变为另一种一致的状态，不可能出现中间状态，只有事务提交前状态和提交后状态，不存在提交中的状态。I（Isolation），隔离性，每个读写事务的对象对其他事务的操作对象相互独立，即该事务提交前对其他事务都不可见。D（Durability），持久性，一个事务一旦提交，它对数据库中对应数据的状态变更就是永久性的。 CAP单机事务很容易实现支持ACID特性的事务处理系统，但是在分布式环境中问题就变得困难许多。CAP理论告诉我们，一个分布式系统不可能同时满足一致性(C:Consistency)、可用性(A:Availability)、分区容错性(P:Partition tolerance)。最多只能同时满足其中两项。 抛弃P这个最简单，只有一台机器提供服务，肯定能保证一致性和可用性。看到这里可能有人会问了，单机怎么保证可用性？我们来看这段话，引自《NOSQL Distilled》By the usual definition of “available,” this would mean alack of availability, but this is where CAP’s special usage of “availability” gets confusing. CAP defines “availability” to mean “every request received by a non-failing node in the system must result in a response”[Lynch and Gilbert]. So a failed, unresponsive node doesn’t infer alack of CAP availability.上面那段主要说的是，CAP中的A指的是，每个被活着的节点收到的请求都能得到响应，所以如果一个节点宕了无法对请求进行响应并不代表不具备A。这样就说得通了，单机肯定满足C和A，但是不具备P，意味着放弃了系统的可扩展性。 抛弃A（机器未宕机但不提供服务）意味着分布式系统只有C和P，指当系统遇到网络分区等故障的时候，受影响的服务无法对外提供正常服务。当发生网络分区时，此时的措施是从系统中抛弃一部分分区，从而维持系统的一致性和分区容错性。 抛弃C意味着分布式系统只有C和P，指当系统遇到网络分区等故障的时候，让分离开的几个部分同时对外提供服务，这样可以保证可用性和分区容错性，但是会导致一致性受影响。 Ps 这里还有另外一种理解方式，分布式系统（可以简单理解为满足P必然是分布式系统）情况下，如果满足C那么服务器间进行数据同步时要锁住服务必然不能提供服务；如果满足A那么服务器必然不能被锁住，而这样又无法兼顾一致性。 如今大多数公司的架构都是分布式，所以分区容错性可以说是系统的一个最基本的需求，所以讨论的焦点就在如何在C和A之间寻求平衡。 BASEBASE:1）Basically Available遇到故障，服务降级，比如响应时间增加、非核心功能不可用等2）Soft state弱状态，指允许系统中的数据存在中间状态，即允许系统在不同节点的数据副本之间进行数据同步的过程存在延时3）Eventually consistent顾名思义，经过一段时间同步后，最终能够达到一个一致的状态 通过牺牲强一致性来获得可用性，并允许数据在一段时间内是不一致的，但最终达到一直状态。 一致性协议2PC1）阶段一：提交事务请求（投票阶段）1.1事务询问协调者向所有的参与者发送事务内容，询问是否可以执行事务提交操作，并开始等待各参与者响应1.2执行事务各参与者节点执行事务操作，将Undo和Redo信息写到事务日志中1.3各参与者向协调者反馈事务询问的响应如果参与者成功执行了事务操作，那么反馈给协调者yes，表示可以事务执行；反之，反馈给协调者no，表示事务不可以执行。 2）阶段二：执行事务提交2.1执行事务提交如果参与者反馈都是Yes，那么执行事务提交2.1.1发送提交请求协调者向所有参与者节点发出Commit请求2.1.2事务提交参与者接收到Commit请求后，会正式执行事务commit操作，并在完成提交之后释放整个事务执行期间占用的事务资源2.1.3反馈事务提交结果参与者在完成事务提交后，向协调者发送Ack消息2.1.4完成事务协调者接收到所有参与者反馈的Ack消息后，完成事务Or2.2中断事务如果任何一个参与者反馈为No，或者在协调者等待某个参与者响应超时后，那么就会中断事务。2.2.1发送回滚请求协调者向参与者节点发送Rollback请求2.2.2事务回滚参与者接收到Rollback请求后，利用Undo信息执行回滚操作，并在完成回滚后释放整个事务执行期间占用的资源。2.2.3反馈事务回滚结果参与者完成事务回滚后，向协调者发送Ack消息2.2.4中断事务协调者接收所有参与者反馈的Ack后，完成事务中断 缺点：1）同步阻塞在提交阶段，所有参与者都处于阻塞状态，需要等待协调者发送Commit（等待所有参与者都发送Yes到协调者）2）单点问题协调者如果出问题，整个二阶段将无法执行，参与者将一直阻塞3）数据不一致协调者向参与者发送commit请求之后，发生了局部网络异常or协调者在未发送完Commit请求前自身发生了崩溃，导致最终只有部分参与者收到了commit请求4）缺少容错机制协调者需要给每个参与者额外指定超时机制，超时后整个事务失败。（缺少容错机制） 3PC3PC是2PC的改进版，将2PC提交事务请求阶段一分为二，由CanCommit,PreCommit,DoCommit三个阶段组成 1）阶段一：CanCommit1.事务询问协调者向所有参与者发送一个包含事务内容的canCommit请求，询问是否可以执行事务提交操作，并开始等待参与者响应2.各参与者向协调者反馈事务询问的响应参与者在接收到来自协调者的canCommit后，会反馈yes or no表示能否执行事务 2）阶段二：PreCommit执行事务预提交（如果都是yes）1.发送预提交请求协调者向参与者发送preCommit请求2.事务预提交参与者接收到preCommit请求后，会执行事务操作，并将Undo和Redo信息记录到事务日志中3.各参与者向协调者反馈事务执行的响应如果参与者成功执行了事务操作，那么就反馈给协调者Ack响应，同时等待最终的指令：commit or abortor中断事务（不都是yes or 超时）1.发送中断请求协调者向参与者发送abort2.中断事务无论收到来自协调者的abort请求，还是在等待协调者请求过程中出现超时，参与者都会中断事务 3）阶段三：doCommit执行提交1.发送提交请求协调者收到了所有参与者的Ack响应，向所有参与者发送doCommit请求2.事务提交参与者接收doCommit请求后，会正式执行事务的提交，并在完成提交之后释放在整个事务期间占用的事务资源3.反馈事务提交结果参与者在完成事务提交后，向协调者发送Ack消息4.协调者接收到所有参与者反馈的Ack后，完成事务or中断事务（不都是yes or 超时）1.发送中断请求协调者向所有的参与者节点发送abort请求2.事务回滚参与者接收到abort后，会利用Undo信息来执行事务回滚操作。并在回滚后释放整个事务执行期间占用的资源。3.反馈事务回滚结果参与者在完成事务回滚后，向协调者发送Ack消息。4.中断事务协调者接收到所有参与者反馈的Ack消息后，中断事务 Ps：一旦进入阶段三，无论是协调者出现问题，还是协调者和参与者之间的网络出现问题，都会导致参与者无法及时接收到来自协调者的doCommit or abort，此时，参与者会在超时后，继续进行事务提交 优点：1.相比二阶段提交，阻塞范围变小了，如果事务不能被某个参与者执行会在阶段一便反馈给协调者2.在出现单点故障后继续达成一致，阶段二协调者故障(发送preCommit前)-所有参与者会自动中断事务，阶段三协调者故障-所有参与者会提交事务缺点：1.阶段二在参与者都收到preCommit后，如果出现了网络分区，此时参与者会执行事务但未提交，进入阶段三，这个时候协调者没有收到所有的Ack，那么会通知能和它保持联系的参与者进行回滚，但是由于网络分区隔离出去的参与者们会继续提交事务，这会导致数据的不一致]]></content>
      <categories>
        <category>数据库</category>
      </categories>
      <tags>
        <tag>数据库</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring官方文档拾遗]]></title>
    <url>%2F2018%2F08%2F11%2FSpring-3%2F</url>
    <content type="text"><![CDATA[最近在读Spring官方文档，顺路对很多不清晰的地方进行了梳理1.ApplicationContext 关于ApplicationContext都说ApplicationContext是高级版的BeanFactory，更加框架化相比BeanFactory主要多了如下几个功能1）通过MessageResource接口提供了国际化支持1234# in exception.propertiesexception = "又写bug了吧"# in exception_en_GB.propertiesexception = "find bug agian" 123&lt;bean id="messageSource" class="org.springframework.context.support.ResourceBundleMessageSource"&gt; &lt;property name="basename" value="exception"/&gt; &lt;/bean&gt; 1234567public static void main(String[] args) &#123; MessageSource resources = new ClassPathXmlApplicationContext("spring/applicationContext.xml"); String message0 = resources.getMessage("exception",null,"又写bug了把", Locale.UK); String message1 = resources.getMessage("exception",null,"又写bug了把", Locale.CHINA); System.out.println(message0); System.out.println(message1);&#125; 输出为：find bug agian又写bug了吧Ps原以为高大上的国际化其实很简单，只不过类似屠龙技不常用罢了2）通过ResourceLoader接口提供了资源访问功能一个Resource本质上是一个功能更丰富的JDK类java.net.URL版本，实际上，Resource的实现包装了一个java.net.URL的实例 ApplicationContext可以方便地加载资源3）通过ApplicationEventPublisher接口提供了事件发布功能12345678910111213public class BlackListEvent extends ApplicationEvent &#123; private final String address; private final String test; public BlackListEvent(Object source, String address, String test) &#123; super(source); this.address = address; this.test = test; &#125; // accessor and other methods...&#125; 12345678910111213141516171819202122public class EmailService implements ApplicationEventPublisherAware &#123; private List&lt;String&gt; blackList; private ApplicationEventPublisher publisher; public void setBlackList(List&lt;String&gt; blackList) &#123; this.blackList = blackList; &#125; //这里注入的实际上是ApplicationContext本身 public void setApplicationEventPublisher(ApplicationEventPublisher publisher) &#123; this.publisher = publisher; &#125; public void sendEmail(String address, String text) &#123; if (blackList.contains(address)) &#123; BlackListEvent event = new BlackListEvent(this, address, text); publisher.publishEvent(event); return; &#125; // send email... &#125;&#125; 123456789101112public class BlackListNotifier implements ApplicationListener&lt;BlackListEvent&gt; &#123; private String notificationAddress; public void setNotificationAddress(String notificationAddress) &#123; this.notificationAddress = notificationAddress; &#125; public void onApplicationEvent(BlackListEvent event) &#123; // notify appropriate parties via notificationAddress... &#125;&#125; 12345678910111213&lt;bean id="emailService" class="example.EmailService"&gt; &lt;property name="blackList"&gt; &lt;list&gt; &lt;value&gt;known.spammer@example.org&lt;/value&gt; &lt;value&gt;known.hacker@example.org&lt;/value&gt; &lt;value&gt;john.doe@example.org&lt;/value&gt; &lt;/list&gt; &lt;/property&gt;&lt;/bean&gt;&lt;bean id="blackListNotifier" class="example.BlackListNotifier"&gt; &lt;property name="notificationAddress" value="blacklist@example.org"/&gt;&lt;/bean&gt; 当调用emailService的sendEmail()时，如果有email在黑名单中，这时候会会发布BlackListEvent，而BlackListNotifier会对这个事件进行监听，进行一些处理工作Ps一个事件可以有多个监听者，多个监听者如果同时被触发，会同步地执行，这里的同步指的是一个一个的执行，而且publisher.publishEvent(event);方法也会阻塞知道监听者逐一执行完自己的逻辑 关于BeanPostProcessor1.BeanPostProcessor处理容器内所有符合条件的BeanDefinition Ps这里的符合条件与否在覆写的方法中自行判断if(beanName==xxx) 2.A bean post-processor typically checks for callback interfaces or may wrap a bean with a proxy. Some Spring AOP infrastructure classes are implemented as bean post-processors in order to provide proxy-wrapping logic.AOP就是通过BeanPostProcessor实现偷梁换柱的 3.Classes that implement the BeanPostProcessor interface are special and are treated differently by the container. All BeanPostProcessors and beans that they reference directly are instantiated on startup, as part of the special startup phase of the ApplicationContext实现该接口的bean是很特殊的，它以及它所依赖的bean需要优先启动，可以将其视为容器启动的一个阶段。而且实现了该接口的bean（或者被依赖的beans）无法对其进行动态代理，因为动态代理本身就是通过BeanPostProcessor实现的，你可以理解为”大力士举不起自己” 关于BeanFactoryPostProcessor1.PropertyPlaceholderConfigurer12345678910111213&lt;bean class="org.springframework.beans.factory.config.PropertyPlaceholderConfigurer"&gt; &lt;property name="locations" value="classpath:com/foo/jdbc.properties"/&gt;&lt;/bean&gt;or&lt;context:property-placeholder location="classpath:com/foo/jdbc.properties"/&gt;&lt;bean id="dataSource" destroy-method="close" class="org.apache.commons.dbcp.BasicDataSource"&gt; &lt;property name="driverClassName" value="$&#123;jdbc.driverClassName&#125;"/&gt; &lt;property name="url" value="$&#123;jdbc.url&#125;"/&gt; &lt;property name="username" value="$&#123;jdbc.username&#125;"/&gt; &lt;property name="password" value="$&#123;jdbc.password&#125;"/&gt;&lt;/bean&gt; 12345# jdbc.propertiesjdbc.driverClassName=org.hsqldb.jdbcDriverjdbc.url=jdbc:hsqldb:hsql://production:9002jdbc.username=sajdbc.password=root 在运行时，占位符内的内容会被替换为属性文件中的匹配的value，这个工作就是由PlaceholderConfiguer完成的，不难看出这个类应该实现了BeanFactoryPostProcessor]]></content>
      <categories>
        <category>Spring</category>
      </categories>
      <tags>
        <tag>Spring</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[排序算法]]></title>
    <url>%2F2018%2F08%2F06%2Fsort-algorithm%2F</url>
    <content type="text"><![CDATA[Outline1.回顾几种排序算法先来两个利用递归的排序算法共同点：入参相同都有lo和hi都是利用递归递归终止的条件相同都是hi&lt;=lo return不同点：快排核心操作位于两个递归前（快嘛而且入参和排序算法相同）归并核心操作位于两个递归后（入参多了一个mid） 快速排序1234567891011121314151617181920private void sort(int[] a, int lo, int hi) &#123; if(hi&lt;=lo) return; int j = partition(a, lo, hi); sort(a,lo,j-1); sort(a,j+1,hi);&#125;//目标：让a[lo]位于它该位于的地方(j)；比a[lo]小的都位于其左侧，大的位于其右侧private int partition(int[] a, int lo, int hi) &#123; int v = a[lo]; int i = lo, j = hi+1; while(true) &#123; //这里=很关键 while(a[++i] &lt;= v) if(i == hi) break; while(a[--j] &gt; v) if(j == lo) break; if(i &gt;= j) break; swap(a, i ,j); &#125; swap(a,lo,j); return j;&#125; 关于partiton1）遇到合适的元素要停在当前位置，所以要提前++和–，不然swap时候索引又发生改变了，所以初始的i和j要在真正需要遍历的元素范围之外2）外层循环跳出的条件显然要在swap和while之间3）最后的swap要和j去交换，此时需要交换一个小于arr[low]的元素算法分析时间复杂度O(NlogN)空间复杂度O(1)当a本身就有序的时候，算法的时间复杂度会退化到O(N2)将int[] a = {1,2,3,4,5,6,7,8}输入到算法中发现算法退化到sort(a,0,7)-&gt;sort(a,1,7)-&gt;sort(a,2,7)…sort(a,7,7) 归并排序12345678910111213141516171819202122private void sort(int[] a, int lo, int hi) &#123; if (hi &lt;= lo) return; int mid = lo + (hi - lo) / 2; sort(a, lo, mid); sort(a, mid + 1, hi); //这里的入参很关键 要有lo 和 hi merge(a, lo, mid, hi);&#125;private void merge(int[] a, int lo, int mid, int hi) &#123; int i = lo, j = mid + 1; int[] aux = new int[a.length]; //这里 for (int k = lo; k &lt;= hi; k++) &#123; aux[k] = a[k]; &#125; for (int k = lo; k &lt;= hi; k++) &#123; if (i &gt; mid) a[k] = aux[j++]; else if (j &gt; hi) a[k] = aux[i++]; else if (aux[i] &lt; aux[j]) a[k] = aux[i++]; else a[k] = aux[j++]; &#125;&#125; 算法分析时间复杂度O(NlogN)空间复杂度O(N) 冒泡排序1234567891011121314void bubbleSort(int[] arr, int n) &#123; for (int i = 0; i &lt; n - 1; i++) &#123; boolean isOrdered = true; for (int j = 0; j &lt; n - 1 - i; j++) &#123; if (arr[j] &gt; arr[j + 1]) &#123; int tem = arr[j + 1]; arr[j + 1] = arr[j]; arr[j] = tem; isOrdered = false; &#125; &#125; if (isOrdered) break; &#125;&#125; 算法分析时间复杂度O(N2)所谓冒泡是说每次从底部向上遍历，每层循环会将最大的元素置为数组末端而且关键的一点是每次只能和周围的元素进行比较当数组本身就有序 的时候，这种写法的冒泡排序复杂度为O(N) 选择排序1234567private void sort(int[]a) &#123; for(int i=0;i&lt;a.length-1;i++)&#123; for(int j=i+1;j&lt;a.length;j++)&#123; if(a[j]&lt;a[i]) swap(a,i,j); &#125; &#125;&#125; 123456789101112private void sort(int[]a)&#123; //多少趟数 for(int i = 0;i &lt; a.length-1;i++) &#123; //没趟多少次 //和冒泡相比每次是和最x元素比较而不是和周围比较 for(int j = 0;j&lt;a.length-1-i;j++)&#123; if(a[j]&gt;a[a.length-1-i])&#123; swap(a,j,a.length-1-i); &#125; &#125; &#125;&#125; 算法分析时间复杂度O(N2)算法思路：每次遍历不再是和周围元素比较，而是和指定位置的元素比较，和冒泡排序相比数据移动次数明显减少优化方案不必每次小于a[i]都进行交换操作，只需存储最小值的索引，内层循环结束再去交换1234567891011121314static void selectSort(int[] arr, int n) &#123; for (int i = 0; i &lt; n - 1; i++) &#123; int curMin = Integer.MAX_VALUE; int curMinIndex = -1; for (int j = n - 1; j &gt;= i; j--) &#123; if (arr[j] &lt; curMin) &#123; curMin = arr[j]; curMinIndex = j; &#125; &#125; arr[curMinIndex] = arr[i]; arr[i] = curMin; &#125;&#125; 插入排序123456789private void sort(int[]a) &#123; //牌的位置，默认已经有一张牌所以从1开始 for(int i=1;i&lt;a.length;i++)&#123; //对当前牌进行交换 for(int j=i;j&gt;=1 &amp;&amp; a[j]&lt;a[j-1];j--)&#123; swap(a,j,j-1); &#125; &#125; &#125; 算法分析时间复杂度O(N2)保证i左侧都是有序的，接下来将i放到合适的位置适合处理有序或者部分有序的数优化思路，内层循环不使用swap操作，采用错位赋值12345678910111213static void insertSort(int[] arr, int n) &#123; for (int i = 1; i &lt; n; i++) &#123; int value = arr[i]; for (int j = i - 1; j &gt;= 0; j--) &#123; if (value &lt; arr[j]) &#123; arr[j + 1] = arr[j]; &#125; else &#123; arr[j + 1] = value; break; &#125; &#125; &#125;&#125; 希尔排序12345678910111213private void sort(int[] a) &#123; int h = 1; int N = a.length; while (h &lt; N / 3) h = 3 * h + 1; while (h &gt;= 1) &#123; for (int i = h; i &lt; N; i++) &#123; for (int j = i; j &gt;= h &amp;&amp; a[j] &lt; a[j - h]; j -= h) &#123; swap(a, j, j - h); &#125; &#125; h /= 3; &#125;&#125; 算法分析和普通的插入排序相比，在进行h有序排序过程中，元素移动的位置更大，无需一个个的交换虽然最终当h=1的时候仍然是普通的插入排序，但这个时候需要需要移动的次数就不会太多了比如最小的元素位于数组末端，如果采用普通的插入排序需要交换N-1次，而希尔排序则不一样，当h很大的时候可以将位于数组末端的最小元素直接直接换到数组前端特点：编写简单，不需要额外空间并且性能也不太差 堆排序优雅的堆排序，想起来当初面试的时候被现在的老大问堆排序问到冷场，time flies~说起堆排序，先提纲挈领一发，就是利用优先队列实现的一种排序方法所谓优先队列其实就两个方法，一是插入元素，二是删除最大(小)元素1234public interface PriorityQueue&#123; public void insert(int a); public int delMax();&#125; 123456789private void sort(int[] a)&#123; PriorityQueue queue = new MyPriorityQueue(); for(int i : a)&#123; queue.insert(a); &#125; for(int i = a.length-1;i&gt;=0;i--)&#123; a[i] = queue.delMax(); &#125;&#125; 接下来问题就简单了，如何实现MypriorityQueue?这个时候就引出了”堆”这里的堆和我们认识中的stack不同，而是利用数组实现的一种数据结构二叉堆是一组能够用堆有序的完全二叉树排序的元素，并在数组中按照层级存储（不使用数组的第一个位置）堆有一个有趣的特性，位置k的结点的父结点位置为k/2向下取整；位置k的结点的两个子结点分别是2k和2k+1,注意堆的父节点是同时大于（小于）两个子节点的如何利用堆实现优先队列？1234567891011121314151617181920212223242526272829303132public class MyPriorityQueue implements PriorityQueue &#123; private int[] arr; private int N = 0; public MyPriorityQueue(int size) &#123; arr = new int[size+1]; &#125; public void insert(int a)&#123; arr[++N] = a; swim(N); &#125; public int delMax()&#123; int max = arr[1]; swap(arr,1,N--); sink(1); return max; &#125; private void swim(int k)&#123; while(k&gt;1 &amp;&amp; arr[k/2]&lt;arr[k])&#123; swap(arr,k/2,k); k/=2; &#125; &#125; private void sink(int k)&#123; while(2*k&lt;=N)&#123; int j=2*k; if(j&lt;N &amp;&amp; arr[j] &lt; arr[j+1]) j++; if(arr[k]&gt;=arr[j]) break; swap(arr,k,j); k=j; &#125; &#125;&#125; 我们已经通过实现优先队列的api完成了基于堆的排序，但是让人烦躁的是每次都需要堆无序数组逐一插入到优先队列中，能否将一个无序数组直接变成堆有序的数组？123456private void sort(int[] a)&#123; int N = a.length; for(int k = N/2;k&gt;=1;k--)&#123; sink(k); &#125;&#125; 由此联想到，直接将数组进行原地排序1.将数组置为堆有序2.堆有序的数组通过交换和下沉操作完成排序Ps需要将第一位空出来12345678910111213141516171819202122private void sort(int[]a)&#123; int N = a.length; //堆有序 for(int k=N/2;k&gt;=1;k--)&#123; sink(a,k,N); &#125; //最大元素和最小元素交换位置 //下沉最小元素，调整堆的结构 while(N&gt;1)&#123; swap(a,1,N--); sink(a,1,N); &#125;&#125;private void sink(int[] arr,int k, int size)&#123; while(2*k&lt;=size)&#123; int j=2*k; if(j&lt;size &amp;&amp; arr[j] &lt; arr[j+1]) j++; if(arr[k]&gt;=arr[j]) break; swap(arr,k,j); k=j; &#125;&#125; 无须空出第一位的写法12345678910111213141516171819202122static void sink(int[] a, int k, int N) &#123; while (2 * k + 1 &lt;= N) &#123; int j = 2 * k + 1; if (j+1 &lt;= N &amp;&amp; a[j] &lt; a[j + 1]) j++; if (a[k] &gt;= a[j]) break; TemTest.swap(a, k, j); k = j; &#125;&#125;static void sort(int[] a) &#123; int N = a.length-1; int k = N/2; while (k &gt;= 0) &#123; sink(a, k, N); k--; &#125; while (N &gt;= 1) &#123; TemTest.swap(a,0,N--); sink(a,0,N); &#125;&#125; 时间复杂度O(NlogN)，空间复杂度O(1)]]></content>
      <categories>
        <category>Algorithm</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring-AOP专题]]></title>
    <url>%2F2018%2F07%2F28%2FSpring-2%2F</url>
    <content type="text"><![CDATA[Spring AOP专题AOP解决程序开发里事务性，和核心业务无关的问题，但这些问题对于业务场景的实现是很有必要的。传统的编程方式是垂直化的，不同的模块相同的逻辑虽然可以抽象出来，但是还是会对原有的代码有一定的侵入，而且这部分相同的逻辑（下文称为统计代码）大多数情况和业务代码无关，那么能不能有一种方法可以对业务代码0侵入，而且又能实现应有的功能？ 代理模式没有什么是通过抽象不能解决的，如果有那就抽象两次–奥雷连诺-刘能在业务代码上抽象出一层代理层，持有原始类的引用，在调用处前后加入相应的统计代码。优点：1.业务代码无感知2.对同一个接口的实现类来讲只需要一个代理类缺点：1.在统计代码相同的情况下，不同接口需要开发不同的代理类2.而且统计代码和业务代码仍然存在一定程度的耦合，比如现在想把某个方法的统计功能去掉or某个没有统计功能的代码加上统计功能，那么仍然需要修改Proxy类 既然有缺点那就一条一条解决，首先解决代理类膨胀的问题，这个很简单通过动态代理就能解决 Ps这部分自己不常用，所以写一下代码 12345678910111213141516171819202122232425public class MonitorHandler implements InvocationHandler &#123; private Object obj; public MonitorHandler(Object obj)&#123; this.obj = obj; &#125; @Override public Object invoke(Object proxy, Method method, Object[] args) &#123; if(needLog())&#123; logbefore(); Object result = method.invoke(obj,args); logafter(); return result; &#125; return method.invoke(obj,args); &#125;&#125;public static void main(String[] args) &#123; InterfaceA interfaceA = new InterfaceAImpl(); InterfaceB interfaceB = new InterfaceBImpl(); InterfaceA proxyA = (InterfaceA) Proxy.newProxyInstance(MonitorHandler.class.getClassLoader(), new Class&lt;?&gt;[]&#123;InterfaceA.class&#125;,new MonitorHandler(interfaceA)); InterfaceA proxyB = (InterfaceB) Proxy.newProxyInstance(MonitorHandler.class.getClassLoader(), new Class&lt;?&gt;[]&#123;InterfaceB.class&#125;,new MonitorHandler(interfaceB)); proxyA.doWork(); proxyB.doWork();&#125; 对了这里还要提一句如果被代理的类没实现接口此时需要使用CGLIB好，这样看已经解决了，代理类膨胀的问题；那么统计代码和业务代码耦合的问题如何解决？这个时候就要引出aop了Ps Spring AOP的底层实现就是通过JDK动态代理和CGLIB动态代理技术实现的 AOP我觉得理解下面三个术语就ok了 advisoradvisor = pointcut(定位到具体方法) + advice(方法的具体织入位置和逻辑) 切点pointcut定义统计逻辑的切入位置，目前只支持方法级别的切入切点只定位到某个方法上 adviceadvice = 逻辑 + 方位统计代码写到这里，由不同类型表示目标切入方法的具体方位，包括前置、后置、异常、最终、环绕BeforeAdvice/AfterAdvice/ThrowAdvice 简单例子《Spring 3.x》，这个例子已经是aspectJ了~1234567891011121314151617181920212223242526272829public class NaiveWaiter implements Waiter&#123; public void greetTo(String name)&#123; System.out.println("greet to"+name); &#125; public void serverTo(String name)&#123; System.out.println("serve to"+name); &#125;&#125;@Aspectpublic class PreGreetingAspect&#123; @Before("execution(* greetTo(...))") public void beforeGreeting()&#123; System.out.println("How are you"); &#125;&#125;public static void main(String[] args) &#123; AspectJProxyFactory fac = new AspectJProxyFactory(); fac.setTarget(new NaiveWaiter); fac.addAspect(PreGreetingAspect.class); Waiter proxy = fac.getProxy(); proxy.greetTo("Luyu"); proxy.serverTo("Luyu");&#125;or&lt;aop:aspectj-autoproxy /&gt;&lt;bean id="waiter" class="com.luyu.NaiveWaiter" /&gt;&lt;bean class="com.luyu.PreGreetingAspect"&gt; AOP真是博大精深，各种网上的文章都只是简单地介绍如何使用，而不深入剖析内部原理（可能和相对复杂也有关系），想要理解AOP还是要深入去看 深入分析AOP原理关于aop的使用，有两种方式一种是配置的方式；另一种是注解的方式（@Aspect）配置的方式是基础，主要分析这种方式给出一个简单的基于注解的配置文件1234567891011121314&lt;bean id="testAdvisor" class=com.techlab.advisor.TestAdvisor/&gt;&lt;bean id="myTestAop" class="org.springframework.aop.ProxyFactoryBean"&gt; &lt;property name="proxyInterfaces"&gt; &lt;value&gt; com.test.AbcInterface &lt;value/&gt; &lt;/property&gt; &lt;property name="target"&gt; &lt;bean class="com.techlab.bean.TestBean"/&gt; &lt;/property&gt; &lt;property name="interceptorNames"&gt; &lt;list&gt;&lt;value&gt;testAdvisor&lt;/value&gt;&lt;/list&gt; &lt;/property&gt;&lt;/bean&gt; 从上面对配置文件易知aop的入口点应该是ProxyFactoryBean，也是通过这个bean实现了IOC和AOP的对接。ProxyFactoryBean本身也是一个FactoryBean，所以对于这种类型的bean实例化的时候，返回的对象是由其内部getObject()方法决定的，这个就是我们分析的入口点。开车~1234567891011121314151617181920212223242526272829303132333435getObject() //初始化advisor链 1.initializeAdvisorChain() //遍历每个配置的interceptorNames获取实例bean advice = this.beanFactory.getBean(name) //将advice包裹为advisor并放到链中 addAdvisorOnChainCreation(advice,name) //返回单例的代理对象 2.return getSingletonInstance() //AopProxy有两个子类实现，一个是Cglib，一个是JdkDynamicProxy //根据目标对象是否实现接口来决定采用哪种方式进行动态代理 AopProxy proxy = createAopProxy()) proxy.getProxy(this.proxyClassLoader) Proxy.newProxyInstance(classLoader, proxiedInterfaces, this); //重点看下this里面的invoke()方法，这是最核心的方法，完成了功能增强 AopProxy#invoke() //获取拦截器链(这里的拦截器和mvc中的不同，这个拦截器是advice的子类) List&lt;Object&gt; chain = this.advised.getInterceptorsAndDynamicInterceptionAdvice(method,targetClass) //根据第一步中的advisor去生成interceptor registry.getInterceptors(advisor) //将和方法匹配的拦截器放到拦截器链中 interceptorList.add(new InterceptorAndDynamicMethodMatcher(interceptor,mm)) //如果没有任何拦截器 AopUtils.invokeJoinpointUsingReflection(target,method,args) method.invoke(target,args) //如果有拦截器 //proceed方法会逐个运行拦截器的方法,判断拦截器是否和当前bean匹配 //如果运行到了链的末尾，那么直接调用目标对象的方法；否则继续执行匹配的拦截器方法 new ReflectviceMethodInvocation(proxy,target,method,args,targetClass,chain).proceed() //如果拦截器调用完毕 invokeJoinpoint() //如果当前方法和advisor的pointcut匹配 interceptor.invoke(this) //如果当前拦截器不匹配，递归调用proceed proceed() 大概整个流程就是这样了，有一些细节要提一下。之前一直想不通，假如某个方法既有前置增强又有后置增强，如果遍历拦截器（对advisor的包裹）怎么实现的先执行前置再执行方法本身最后执行后置的？看了拦截器的源码就懂了（结合proceed方法）前置advisor与后置advisor配置的先后顺序也不会影响最终的执行结果1234567891011121314151617181920212223@Overridepublic Object proceed() throws Throwable &#123; //如果拦截器遍历完毕，那么调用方法本身的方法 if (this.currentInterceptorIndex == this.interceptorsAndDynamicMethodMatchers.size() - 1) &#123; return invokeJoinpoint(); &#125; //注意这里的++ Object interceptorOrInterceptionAdvice = this.interceptorsAndDynamicMethodMatchers.get(++this.currentInterceptorIndex); if (interceptorOrInterceptionAdvice instanceof InterceptorAndDynamicMethodMatcher) &#123; InterceptorAndDynamicMethodMatcher dm = (InterceptorAndDynamicMethodMatcher) interceptorOrInterceptionAdvice; //如果拦截器和当前方法匹配，那么调用拦截器的方法 if (dm.methodMatcher.matches(this.method, this.targetClass, this.arguments)) &#123; return dm.interceptor.invoke(this); &#125; //如果拦截器不匹配，那么递归调用proceed方法 else &#123; return proceed(); &#125; &#125; ...省略&#125; 12345678910public class AfterReturningAdviceInterceptor implements MethodInterceptor, AfterAdvice, Serializable &#123; ...省略 @Override public Object invoke(MethodInvocation mi) throws Throwable &#123; //先去执行其他拦截器的方法or本身的方法 Object retVal = mi.proceed(); this.advice.afterReturning(retVal, mi.getMethod(), mi.getArguments(), mi.getThis()); return retVal; &#125;&#125; 123456789public class MethodBeforeAdviceInterceptor implements MethodInterceptor, Serializable &#123; ...省略 @Override public Object invoke(MethodInvocation mi) throws Throwable &#123; //先去执行advice的前置方法 this.advice.before(mi.getMethod(), mi.getArguments(), mi.getThis() ); return mi.proceed(); &#125;&#125; 小结：终于撸完了一个简单的AOP全过程，之前如果有人问题AOP的原理，我就只能说动态代理和CGLIB现在我知道了ProxyFactoryBean将IOC和AOP连接在一起以及为啥前置&gt;方法&gt;后置，通过不同advice的interceptor实现 简单地说，就是对每个目标对象创建一个BeanFactory的实例，通过getObject对实现进行改造，该方法内部通过动态代理 or CGLIB的方式对当前方法进行代理，而代理的invoke方法中会先去遍历当前target配置的advisor，然后找到和当前method匹配的advisor，之后再将匹配的advisor和method融合到一起，实现aop~ 这种配置的方法其实很蛋疼，每一个需要被代理的类都要配置为一个proxyFactoryBean那么如果不这么配置的话咋实现对每个可能的目标类的代理呢？我们想到了Spring预留的BeanPostProcessor接口，该方法可以做到对每个符合代理要求的bean进行偷梁换柱具体逻辑位于postProcessAfterInitialization中，也就是执行过初始化方法之后，对符合pointCut的类进行代理，返回给getBean的调用处完成『偷梁换柱』 关于AOP的几问AOP的底层是通过什么实现的？Spring AOP底层使用JDK动态代理和CGLIB来实现的这里多提一句JDK动态代理相当于new了一个代理所实现接口的实例，接下来再对其方法进行覆写,写到这里引出一个问题，如果代理类本身就有的一些方法能否同样被代理？哈哈，一下就暴露了对动态代理的不熟悉,(TargetInterface)Proxy.newProxyInstance,注意前面那个强制转换，此时是无法调用非接口方法的。缺点： JDK动态代理需要代理类实现至少一个接口 且只能对接口中的方法进行代理 当代理类未实现任何接口或者想要代理目标的所有方法而不止是实现自接口的方法，此时需要使用CGLIB代理，CGLIB相当于new了一个代理类的子类，对子类的方法进行覆写以支持代理功能缺点： 但是这样有一个问题就是无法覆写代理类本身生命为final的方法 CGLib在创建代理对象时所花费的时间却比JDK多得多 关于JDK和CGLIB的总结 如果目标对象实现了接口，默认情况使用JDK的动态代理实现AOP 如果目标对象实现了接口，可以强制使用CGLIB，设置方式aop:aspectj-autoproxy proxy-target-class=”true” 如果目标对象没有实现接口，必须采用CGLIB库 基于aspectJ注解的动态代理是如何偷梁换柱的？这里描述一些源码细节，AOP代理对象是通过AnnotationAwareAspectJAutoProxyCreator创建的，而该类实现了BeanPostProcessor方法，在该接口的postProcessAfterInitialization方法中实现了偷梁换柱，这部分可以参考下Bean的生命周期 https://luyu05.github.io/2018/07/26/Spring-1/ AOP的应用？最常用的一个地方就是数据库事务，除了本身的业务逻辑代码只需一个注解就ok。建立在 AOP 的基础之上的。其本质是对方法前后进行拦截，然后在目标方法开始之前创建或者加入一个事务，在执行完目标方法之后根据执行情况提交或者回滚事务。]]></content>
      <categories>
        <category>Spring</category>
      </categories>
      <tags>
        <tag>Spring</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring-MVC]]></title>
    <url>%2F2018%2F07%2F14%2FSpring-MVC%2F</url>
    <content type="text"><![CDATA[Spring-MVC框架阅读 关于ServletContextServletContext这个名字不太贴切，不如叫AppContext,对整个web应用可用，而不是只针对一个servlet。123456&lt;web-app&gt; &lt;context-param&gt; &lt;param-name&gt;foo&lt;/param-name&gt; &lt;param-value&gt;bar&lt;/param-value&gt; &lt;/context-param&gt;&lt;/web-app&gt; 获取方式getServletContext().getInitParamter(“foo”); Spring MVC启动流程-IOC容器的启动在bean.xml中12345678&lt;bean id="simpleUrlMapping" class="org.springframework.web.servlet.handler.SimpleUrlHandlerMapping"&gt; &lt;property name="mappings"&gt; &lt;props&gt; &lt;prop key="/userlist.htm"&gt;userController&lt;/prop&gt; &lt;/props&gt; &lt;/property&gt;&lt;/bean&gt;&lt;bean id="userController" class="com.springmvc.controller.UserController"/&gt; 在web.xml中有如下配置123456789101112131415161718&lt;context-param&gt; &lt;param-name&gt;contextConfigLocation&lt;/param-name&gt; &lt;param-value&gt; /WEB-INF/applicationContext.xml &lt;/param-value&gt;&lt;/context-param&gt;&lt;servlet&gt; &lt;servlet-name&gt;springmvc&lt;/servlet-name&gt; &lt;servlet-class&gt;org.springframework.web.servlet.DispatcherServlet&lt;/servlet-class&gt; &lt;load-on-startup&gt;1&lt;/load-on-startup&gt;&lt;/servlet&gt;&lt;servlet-mapping&gt; &lt;servlet-name&gt;springmvc&lt;/servlet-name&gt; &lt;url-pattern&gt;/&lt;/url-pattern&gt;&lt;/servlet-mapping&gt;&lt;listener&gt; &lt;listener-class&gt;org.springframework.web.context.ContextLoaderListener&lt;/listener-class&gt;&lt;/listener&gt; 这里配置的listener就是IOC初始化的入口点，ContextLoaderListener实现ServletContextListener接口，这个接口是ServletContext的监听者，如果ServletContext（包括被创建、销毁等）发生变化，会调用不同的方法。当web容器启动的时候，发生的是ServletContext的创建，此时会调用ServletContextListener的contextInitialized方法开车~1234567891011121314151617contextInitialized() //初始化根上下文 initWebApplicationContext(event.getServletContext()) this.context = createWebApplicationContext(servletContext) //判断使用什么样的IOC容器 //如果没有在web.xml中指定，那么使用默认的WebApplicationContext contextClass = determineContextClass(ServletContext servletContext); //直接实例化容器 wac = BeanUtils.instantiateCLass(contextClass); ... configureAndRefreshWebApplicationContext(cwac, servletContext); //将IOC容器和web容器挂钩 wac.setServletContext(servletContext) //初始化容器 wac.refresh() //将web容器和IOC容器挂钩 servletContext.setAttribute(keyXXX, this.context); 简单地说，这个listener的作用是初始化当前web的根上下文信息（包括实例化和经典的refresh方法的调用）并将跟上下文和web容器挂钩（set某个属性而已） Ps：除了根上下文外，每个servlet都会持有一个自己的上下文，并且把根上下文作为父上下文。 Spring MVC启动流程-Spring WEB MVC的启动直接开车1234567891011DispatcherServlet#init() //模板方法具体由子类实现 initServletBean() //初始化当前servlet的上下文信息，当前上下文信息的父类是根上下文信息 //其余步骤和初始化根上下文基本一致，当前上下文也要加载servlet指定的bean配置文件 //这时候加载的bean就是当前上下文独有的，此时也可以获取父类中的bean 1.this.webApplicationContext = initWebApplicationContext(); //有个很重要的方法 onRefresh(wac); //空方法 2.initFrameworkServlet(); 小结：从上面的代码，可以看出servlet会持有自己的上下文信息（有自己的Bean定义空间）在初始化servlet上下文信息的过程中，会调用onRefresh(wac)方法，这个方法会调用DispatcherServlet的initStrategies方法，下面分析这个方法 Spring MVC的实现Spring DispatcherServlet的初始化123456789101112protected void initStrategies(ApplicationContext context) &#123; initMultipartResolver(context); initLocaleResolver(context); initThemeResolver(context); //重点方法，handlerMapping的作用是为http请求找到匹配的Controller initHandlerMappings(context); initHandlerAdapters(context); initHandlerExceptionResolvers(context); initRequestToViewNameTranslator(context); initViewResolvers(context); initFlashMapManager(context);&#125; URL请求和控制器（拦截器s）之间的映射关系是通过接口类HandlerMapping来封装的，HandlerMapping中定义了一个getHandler方法，通过该方法可以获得与HTTP请求对应的HandlerExecutionChain，其中封装了具体的Controller对象和拦截器链1234initHandlerMappings //找到所有的HandlerMapping类型的bean并存放到handlerMappings中 //DispatcherServlet#handlerMappings(List类型) BeanFactoryUtils.beansOfTypeIncludingAncestors(context,HandlerMapping.class,true,false) 下面看看HandlerMapping到底是个啥123456public interface HandlerMapping &#123; ...省略 //HandlerExecutionChain是核心 //通过该方法可以获得与HTTP请求对应的HandlerExecutionChain，其中封装了具体的Controller对象和拦截器链 HandlerExecutionChain getHandler(HttpServletRequest request) throws Exception;&#125; 再看看HandlerExecutionChain1234567public class HandlerExecutionChain &#123; ... private final Object handler; private HandlerInterceptor[] interceptors; private List&lt;HandlerInterceptor&gt; interceptorList; ...&#125; 这个数据结构基本满足了我们的一切需求，包括和当前url匹配的Controller和Interceptor我们看看到底是咋根据url匹配到这些内容的。以SimpleUrlHandlerMapping为例，我们看到具体的HandlerExecutionChain getHandler(HttpServletRequest request)是在AbstractHandlerMapping中实现123456HandlerExecutionChain getHandler(HttpServletRequest request) //先根据request找到匹配的handler //简单地讲是从handlerMap中根据url找到匹配的handler Object handler = getHandlerInternal(request); //1.根据request找到匹配的拦截器 2.将拦截器和handler封装成handlerExecutionChain HandlerExecutionChain executionChain = getHandlerExecutionChain(handler, request); 具体的getHandlerInternal逻辑是，先从handlerMap中查找当前url对应的handler，如果没找到的话根据正则表达式找到所有匹配的handler，再对结果进行排序，找到最符合的那个handler那么handlerMap是何时初始化的呢？首先hanlerMap位于AbstractUrlHandlerMapping中，该类是SimpleUrlHandlerMapping的父类我们看继承关系图，发现SimpleUrlHandlerMapping还实现了ApplicationContextAware接口这意味着在实例化该bean的时候会默认调用setApplicationContext方法，我们看到这个方法内部调用了initApplicationContext()方法123456789@Overridepublic void initApplicationContext() throws BeansException &#123; //在这个方法中找到了所有的interceptor super.initApplicationContext(); //注册url和handler关系 registerHandlers(this.urlMap); //将url和handler放到handlerMap中 this.handlerMap.put(urlPath, resolvedHandler);&#125; 小结：1.SimpleUrlHandlerMapping实现了ApplicationContextAware接口，在容器刷新的时候，会调用其setApplicationContext方法2.在上面方法的内部完成了，对SimpleUrlHandlerMapping的bean的属性中指明的url和handler进行保存，存放到handleMap中（以供handlerMapping调用getHandler()获取匹配的url）3.在DispatchServlet初始化的时候，会执行initHandlerMappings(context)4.该方法会找到所有的HandlerMapping类型的bean并存放到handlerMappings中（以供接下来遍历去寻找handler） Spring MVC对HTTP请求的分发处理Servlet的入口doService1234567891011121314151617181920212223doService(request,response) //1.准备ModelAndView 2.调用getHandler响应HTTP请求 //3.得到对应的ModelAndView 4.将ModelAndView交给视图对象去呈现 doDispatch(request,response) //刚方法视图从request的属性中handler，如果没有再遍历handlerMapping获取handler HandlerExecutionChain mappedHandler = getHandler(request,false) //遍历所有的handlerMapping直到找到匹配的HandlerChain for(HandlerMapping hm : this.handlerMappings) handler = hm.getHandler(request) HandlerInterceptor[] interceptors = mappedHandler.getInterceptors() //遍历interceptors for(HandlerInterceptor h : interceptors) //执行拦截器的前置方法 h.preHandle() //执行handler中的逻辑 ModelAndView mv = handler.handleRequest(request,response) //遍历interceptors for(HandlerInterceptor h : interceptors) //执行拦截器的后置方法 h.postHandle() //适用视图对ModelAndView数据进行展示 render(mv,request,response) Spring MVC视图的呈现1234567void render(modelView,request,response) //根据modelAndView去得到View对象 //遍历配置的viewResolvers来找到和modelAndView对应的View view = resolveViewName(modelAndView) //调用view实现对数据的呈现，通过httpResponse把视图呈现给http客户端 //View具有很多实现，包括JSP FreeMarker Excel PDF等 view.render(modedAndView,request,response) 小结 1）ServletContext启动2）执行ContextLoaderListener的contextInitialized方法3）该方法会启动并初始化根上下文(ApplicationContext)，并且和当前容器相互绑定，此时实例化的bean是web.xml中指定的applicationContext.xml中的bean123456&lt;context-param&gt; &lt;param-name&gt;contextConfigLocation&lt;/param-name&gt; &lt;param-value&gt; classpath:spring/applicationContext.xml &lt;/param-value&gt;&lt;/context-param&gt; 4）接着执行spring mvc的DispatchServlet的初始化工作，也就是调用init方法5）接着启动当前servlet自己的上下文，并且设置根上下文为当前上下文的父类，此时实例化的bean是web.xml中指定的springmvc-context中配置的bean12345678910&lt;servlet&gt; &lt;servlet-name&gt;springmvc&lt;/servlet-name&gt; &lt;servlet-class&gt;org.springframework.web.servlet.DispatcherServlet&lt;/servlet-class&gt; &lt;!--这里就是配置属于servlet自己的上下文中的bean的地方--&gt; &lt;init-param&gt; &lt;param-name&gt;contextConfigLocation&lt;/param-name&gt; &lt;param-value&gt;classpath:spring/springmvc-context.xml&lt;/param-value&gt; &lt;/init-param&gt; &lt;load-on-startup&gt;1&lt;/load-on-startup&gt;&lt;/servlet&gt; 6）在servlet的上下文启动过程中，同样会执行Spring的一些扩展点，比如通过BeanPostProcessor#postProcessBeforeInitialization会执行ApplicationContextAware的setApplicationContext方法，而SimpleUrlHandlerMapping就是ApplicationContextAware的子类，在该过程将配置的url和controller映射关系填充到了urlMap中7）执行initHandlerMappings(context)方法，该方法会将所有的HandlerMapping类型的bean都找到并存储在handlerMappings中8）当有请求到来的时候，会执行DispatchServlet的doService方法，该方法会遍历所有的handlerMappings调用其中handlerMapping的getHandler方法该方法会将找到的和当前request相匹配的handler以及inteceptor，并将其封装为handlerExecutorChain一并返回9）遍历执行interceptor的preHandle方法；执行handler.handleRequest方法；遍历执行interceptor的postHandle方法10) 视图的呈现]]></content>
      <categories>
        <category>Spring</category>
      </categories>
      <tags>
        <tag>Spring</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深入分析ConcurrentHashMap]]></title>
    <url>%2F2018%2F07%2F11%2FConcurrentHashMap%2F</url>
    <content type="text"><![CDATA[本文重点分析JDK1.7的ConcurrentHashMap说实话挺复杂的，硬着头皮啃吧~通过和1.7进行比较，简要分析了1.8中的ConcurrentHashMapPs有任何问题请在下方评论，或者邮箱联系我luyucareer@163.com 1.7中的ConcurrentHashMap1.整体认识先来看concurrentHashMap的数据结构，从整体上对它有个认识从图中可以看出，concurrentHashMap在hashMap的基础上抽象了一层这一层就是Segment[],看下Segment这个类12345678910111213static final class Segment&lt;K,V&gt; extends ReentrantLock implements Serializable &#123; //volatile类型的HashEntry数组 transient volatile HashEntry&lt;K,V&gt;[] table; //当前Segment中元素的数量 transient int count; //当前Segment结构变更的次数（put、remove） transient int modCount; //当count&gt;threashold时候，Segment内部进行rehash //table.length*loadFactor transient int threshold; //负载因子 final float loadFactor;&#125; 可以看到Segment继承自ReentrantLock，可以方便的进行加锁每个Segment可以视为一个hashMap，ConcurrentHashMap正是通过Segment实现的同时进行多个读写操作 2.初始化直接撸代码12345678910111213141516171819202122232425262728293031323334353637383940public ConcurrentHashMap(int initialCapacity, float loadFactor, int concurrencyLevel) &#123; if (!(loadFactor &gt; 0) || initialCapacity &lt; 0 || concurrencyLevel &lt;= 0) throw new IllegalArgumentException(); if (concurrencyLevel &gt; MAX_SEGMENTS) concurrencyLevel = MAX_SEGMENTS; // Find power-of-two sizes best matching arguments int sshift = 0; int ssize = 1; //concurrencyLevel默认是16，即有16个Segment //此时sshift=4,ssize=16 while (ssize &lt; concurrencyLevel) &#123; ++sshift; ssize &lt;&lt;= 1; &#125; //segmentShift=28,segmentMask=15 this.segmentShift = 32 - sshift; this.segmentMask = ssize - 1; //initialCapacity表示整个map中容纳kv的数量 if (initialCapacity &gt; MAXIMUM_CAPACITY) initialCapacity = MAXIMUM_CAPACITY; //对每个segment取均值 int c = initialCapacity / ssize; if (c * ssize &lt; initialCapacity) ++c; //默认每个槽的大小为2，确保最终每个segment中table的数量为2的n次方 int cap = MIN_SEGMENT_TABLE_CAPACITY; while (cap &lt; c) cap &lt;&lt;= 1; // create segments and segments[0] // segment的threshold=cap*loadFactor // HashEntry[]数组长度也为cap Segment&lt;K,V&gt; s0 = new Segment&lt;K,V&gt;(loadFactor, (int)(cap * loadFactor), (HashEntry&lt;K,V&gt;[])new HashEntry[cap]); Segment&lt;K,V&gt;[] ss = (Segment&lt;K,V&gt;[])new Segment[ssize]; UNSAFE.putOrderedObject(ss, SBASE, s0); // ordered write of segments[0] //将segments赋给当前concurrentHashMap，注意setment[0]已经初始化 this.segments = ss; &#125; 小结：1）默认情况下Segment数组长度为16，不可扩容2）假如initialCapacity=64,那么c=4,cap=4 则Segment[0]的thresHold=4*0.75=3 3.安全并发put1234567891011121314public V put(K key, V value) &#123; Segment&lt;K,V&gt; s; if (value == null) throw new NullPointerException(); int hash = hash(key); //找到当前key所属的segment int j = (hash &gt;&gt;&gt; segmentShift) &amp; segmentMask; //如果segment[j]为空，对segment[j]进行初始化 if ((s = (Segment&lt;K,V&gt;)UNSAFE.getObject // nonvolatile; recheck (segments, (j &lt;&lt; SSHIFT) + SBASE)) == null) // in ensureSegment //注意这里还没有进行加锁，所以可能多个线程同时对一个segment[i]执行初始化操作 s = ensureSegment(j); return s.put(key, hash, value, false); &#125; 先来看ensureSegment操作，相对简单直接写注释了1234567891011121314151617181920212223242526272829private Segment&lt;K,V&gt; ensureSegment(int k) &#123; final Segment&lt;K,V&gt;[] ss = this.segments; //可以理解为定位标识 long u = (k &lt;&lt; SSHIFT) + SBASE; Segment&lt;K,V&gt; seg; //如果ss[u]==null 才能继续 否则表示已经初始化过了 直接return即可 if ((seg = (Segment&lt;K,V&gt;)UNSAFE.getObjectVolatile(ss, u)) == null) &#123; //初始化的时候只初始了一个segment[0]，其实是为了给其他segment做标杆 //只有segment[i]用到的时候才赋值 //此时segment[0]可能早就扩容过了 Segment&lt;K,V&gt; proto = ss[0]; // use segment 0 as prototype int cap = proto.table.length; float lf = proto.loadFactor; int threshold = (int)(cap * lf); HashEntry&lt;K,V&gt;[] tab = (HashEntry&lt;K,V&gt;[])new HashEntry[cap]; //segment[u]是否真的未初始化 if ((seg = (Segment&lt;K,V&gt;)UNSAFE.getObjectVolatile(ss, u)) == null) &#123; // recheck Segment&lt;K,V&gt; s = new Segment&lt;K,V&gt;(lf, threshold, tab); //通过CAS操作，对segment[u]进行初始化 while ((seg = (Segment&lt;K,V&gt;)UNSAFE.getObjectVolatile(ss, u)) == null) &#123; if (UNSAFE.compareAndSwapObject(ss, u, null, seg = s)) break; &#125; &#125; &#125; return seg; &#125; Segment的put操作123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354final V put(K key, int hash, V value, boolean onlyIfAbsent) &#123; //对当前segment加锁 //scanAndLockForPut方法后续分析 HashEntry&lt;K,V&gt; node = tryLock() ? null : scanAndLockForPut(key, hash, value); V oldValue; try &#123; HashEntry&lt;K,V&gt;[] tab = table; //定位到segment中的HashEntry&lt;K,V&gt;[] tab int index = (tab.length - 1) &amp; hash; //保证获取到的链表头是最新的Unsafe.getObjectVolatile HashEntry&lt;K,V&gt; first = entryAt(tab, index); for (HashEntry&lt;K,V&gt; e = first;;) &#123; //表头非空，不断遍历，如果有相同的就覆盖，如果没有继续遍历直到为null if (e != null) &#123; K k; //== 和 equals &amp; hash if ((k = e.key) == key || (e.hash == hash &amp;&amp; key.equals(k))) &#123; oldValue = e.value; //如果并未指定"不存在才插入"，那么正常覆盖即可 //并且记录segment结构变化的modCount居然++了？ if (!onlyIfAbsent) &#123; e.value = value; ++modCount; &#125; break; &#125; e = e.next; &#125; else &#123; //如果node不为null，将node置于链表头部 if (node != null) node.setNext(first); else//如果node为空，也将node置于链表头部 node = new HashEntry&lt;K,V&gt;(hash, key, value, first); int c = count + 1; //如果超过了threshold并且tab的长度没超过最大值，进行扩容操作 //rehash后续分析 if (c &gt; threshold &amp;&amp; tab.length &lt; MAXIMUM_CAPACITY) rehash(node); else//node置为tab的表头,下次再获取first的时候就是node setEntryAt(tab, index, node); ++modCount; count = c; oldValue = null; break; &#125; &#125; &#125; finally &#123; unlock(); &#125; return oldValue; &#125; 言出必行，接下来分析scanAndLockForPut和rehash首先看看scanAndLockForPut1234567891011121314151617181920212223242526272829303132333435private HashEntry&lt;K,V&gt; scanAndLockForPut(K key, int hash, V value) &#123; //通过segment和hash值定位到tab[i] HashEntry&lt;K,V&gt; first = entryForHash(this, hash); HashEntry&lt;K,V&gt; e = first; HashEntry&lt;K,V&gt; node = null; int retries = -1; // negative while locating node //自旋操作获取锁 while (!tryLock()) &#123; HashEntry&lt;K,V&gt; f; // to recheck first below if (retries &lt; 0) &#123; if (e == null) &#123; if (node == null) // speculatively create node node = new HashEntry&lt;K,V&gt;(hash, key, value, null); retries = 0; &#125; else if (key.equals(e.key)) retries = 0; else e = e.next; &#125; //重试次数过多，那么不重试了，直接进入bq //就是reentrant.lock else if (++retries &gt; MAX_SCAN_RETRIES) &#123; lock(); break; &#125; //链表的头部是新的node，那么重新执行循环 else if ((retries &amp; 1) == 0 &amp;&amp; (f = entryForHash(this, hash)) != first) &#123; e = first = f; // re-traverse if entry changed retries = -1; &#125; &#125; return node;&#125; 简单地说就是，通过自旋获取锁，这里new了一个node私以为没啥大用接下来看rehash方法，在hashmap中并发时候resize经常会出问题，但是在concurrentHashMap则不存在这个问题，因为只能有一个线程执行rehash方法123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051private void rehash(HashEntry&lt;K,V&gt; node) &#123; HashEntry&lt;K,V&gt;[] oldTable = table; int oldCapacity = oldTable.length; int newCapacity = oldCapacity &lt;&lt; 1; threshold = (int)(newCapacity * loadFactor); HashEntry&lt;K,V&gt;[] newTable = (HashEntry&lt;K,V&gt;[]) new HashEntry[newCapacity]; int sizeMask = newCapacity - 1; for (int i = 0; i &lt; oldCapacity ; i++) &#123; HashEntry&lt;K,V&gt; e = oldTable[i]; if (e != null) &#123; HashEntry&lt;K,V&gt; next = e.next; int idx = e.hash &amp; sizeMask; if (next == null) // Single node on list newTable[idx] = e; else &#123; // Reuse consecutive sequence at same slot //找到最后一个在新table位置不同的node //换句话说在它后面都是在当前table的node //这个其实就是随缘，很大可能最后一个才是甚至最后一个也不是 HashEntry&lt;K,V&gt; lastRun = e; int lastIdx = idx; for (HashEntry&lt;K,V&gt; last = next; last != null; last = last.next) &#123; int k = last.hash &amp; sizeMask; if (k != lastIdx) &#123; lastIdx = k; lastRun = last; &#125; &#125; //位置不动的node直接放到新的table但是索引相同 newTable[lastIdx] = lastRun; // Clone remaining nodes //从头遍历lastRun前面的链，放到属于它的table //1.7还有点笨，还在老老实实的重新计算hash值 for (HashEntry&lt;K,V&gt; p = e; p != lastRun; p = p.next) &#123; V v = p.value; int h = p.hash; int k = h &amp; sizeMask; HashEntry&lt;K,V&gt; n = newTable[k]; newTable[k] = new HashEntry&lt;K,V&gt;(h, p.key, v, n); &#125; &#125; &#125; &#125; //put操作引发的rehash，rehash之后再put，放到链表头部 int nodeIndex = node.hash &amp; sizeMask; // add the new node node.setNext(newTable[nodeIndex]); newTable[nodeIndex] = node; table = newTable;&#125; 4.get操作123456789101112131415161718192021public V get(Object key) &#123; Segment&lt;K,V&gt; s; // manually integrate access methods to reduce overhead HashEntry&lt;K,V&gt;[] tab; int h = hash(key); //定位segment long u = (((h &gt;&gt;&gt; segmentShift) &amp; segmentMask) &lt;&lt; SSHIFT) + SBASE; if ((s = (Segment&lt;K,V&gt;)UNSAFE.getObjectVolatile(segments, u)) != null &amp;&amp; (tab = s.table) != null) &#123; //定位在table中的位置,找到所属的entry/node //通过UNSAFE.getObjectVolatile保证刚插入的node也能被看到 for (HashEntry&lt;K,V&gt; e = (HashEntry&lt;K,V&gt;) UNSAFE.getObjectVolatile (tab, ((long)(((tab.length - 1) &amp; h)) &lt;&lt; TSHIFT) + TBASE); e != null; e = e.next) &#123; K k; //== 和 equals &amp; hash if ((k = e.key) == key || (e.hash == h &amp;&amp; key.equals(k))) return e.value; &#125; &#125; return null;&#125; 5.remove操作这个方法很简单123456789101112131415161718192021222324252627282930313233343536final V remove(Object key, int hash, Object value) &#123; //当前segment加锁，失败则自旋 if (!tryLock()) scanAndLock(key, hash); V oldValue = null; try &#123; HashEntry&lt;K,V&gt;[] tab = table; int index = (tab.length - 1) &amp; hash; HashEntry&lt;K,V&gt; e = entryAt(tab, index); HashEntry&lt;K,V&gt; pred = null; //简单地说就是把目标从链上摘除 while (e != null) &#123; K k; HashEntry&lt;K,V&gt; next = e.next; if ((k = e.key) == key || (e.hash == hash &amp;&amp; key.equals(k))) &#123; V v = e.value; if (value == null || value == v || value.equals(v)) &#123; if (pred == null) setEntryAt(tab, index, next); else pred.setNext(next); ++modCount; --count; oldValue = v; &#125; break; &#125; pred = e; e = next; &#125; &#125; finally &#123; unlock(); &#125; return oldValue;&#125; 6.size()操作有意思的一个方法~吃过饭回来撸1234567891011121314151617181920212223242526272829303132333435363738394041424344454647public int size() &#123; // Try a few times to get accurate count. On failure due to // continuous async changes in table, resort to locking. final Segment&lt;K,V&gt;[] segments = this.segments; int size; boolean overflow; // true if size overflows 32 bits long sum; // sum of modCounts long last = 0L; // previous sum int retries = -1; // first iteration isn't retry try &#123; for (;;) &#123; //如果重试的次数超过3，那么对所有segment加锁 //而且如果此时segment还未初始化，先对其初始化 if (retries++ == RETRIES_BEFORE_LOCK) &#123; for (int j = 0; j &lt; segments.length; ++j) ensureSegment(j).lock(); // force creation &#125; sum = 0L; size = 0; overflow = false; //遍历segment，将每个segment的modCount累加到一起 //modCount表示当前segment数据结构变动的次数 for (int j = 0; j &lt; segments.length; ++j) &#123; Segment&lt;K,V&gt; seg = segmentAt(segments, j); if (seg != null) &#123; sum += seg.modCount; int c = seg.count; if (c &lt; 0 || (size += c) &lt; 0) overflow = true; &#125; &#125; //如果本次循环统计的modeCount和上次循环统计的modCount相等 //表示这期间并未发生任何变动 //那么我们可以认为本次统计的size就是实际的size if (sum == last) break; last = sum; &#125; &#125; finally &#123; //释放segment的锁 if (retries &gt; RETRIES_BEFORE_LOCK) &#123; for (int j = 0; j &lt; segments.length; ++j) segmentAt(segments, j).unlock(); &#125; &#125; return overflow ? Integer.MAX_VALUE : size;&#125; 简单地说，就是先不加锁，因为这个方法加锁的代价相当高昂。先去在循环中统计modCount，如果两次循环之间modCount未发生变动，那么将当前的size返回小结：1.put的时候分段加锁，如果加锁失败会有一个自旋的过程2.put操作node是插入到entry的头部3.在put操作触发rehash的时候（resize）因为put本身已经保证了线程安全，所以resize的时候也是线程安全的，不会出现hashmap的环形链(resize+get死循环) 7.并发问题的分析1)牛X的Unsafe包我们可以简单的把unsafe包中的操作，视为对地址的直接操作，类似C++和C中的指针保证了多线程间的可见性2）get操作我们可以看到get操作是没有加锁的，那么是怎么保证线程安全的？a)get和put并发先get再put，因为put操作是直接插到表头的，所以不影响正常遍历原链表先put再get，通过unsafe找到segment，而segment中的table是volatile的，所以能够保证执行get的时候可以看到新增的kvb)get和remove并发如果remove的节点，已经被get遍历过了，那么没啥问题反之，有分成两种情况，如果remove的是头节点，通过unsafe保证了可见；而如果remove的是中间段的节点，这里是通过Entry的volatile修饰的next保证可见的 1.8中的ConcurrentHashMap1)和1.7不同，1.8中的ConcurrentHashMap的数据结构和其HashMap完全相同，单单通过代码保证了线程安全性2)对每个table[i]通过synchronized关键字加锁，也无须自己实现自旋操作3)和1.8的hash相同，当链表长度太长的时候，会变为红黑树4)涉及的红黑树的内容暂时搁置，未去分析 ConcurrentHashMap自问自答1.为何get方法不需要加锁？在HashEntry类中，key，hash域都被声明为final的，value，next域被volatile所修饰，因此HashEntry对象几乎是不可变的，这是ConcurrentHashmap读操作并不需要加锁的一个重要原因 2.size()的原理？会去统计每个segment的modCount和，如果下次循环统计的modCount和跟上次统计的一样，认为此时的size（每个segment的count和）就是一个准确的值反之如果和上次统计的不一样，当循环执行到一定次数的时候会对每个segment进行加锁操作，再去统计size 3.rehash触发的条件？默认segment为16（不会再变动），每个segment中的HashEntry数组是扩容的目标，当segment中的元素size &gt; length(数组的长度)factor这个时候开始扩容，将length2 4.一些细节segment其实是懒加载的，只有用到的时候才会初始化，不过当调用size方法的时候会全部初始化1.6中HashEntry的next是final修饰的，所以新增的时候只能在最前面新增且删除的时候比较笨重需要全量复制；1.7中改为了volatile修饰在进行rehash过程中，不会被其他get方法读取到中间状态，因为是一个新的复制操作，只有老table和新tableput的时候分段加锁，如果加锁失败会有一个自旋的过程put操作node是插入到entry的头部在put操作触发rehash的时候（resize）因为put本身已经保证了线程安全，所以resize的时候也是线程安全的，不会出现hashmap的环形链 参考链接:http://www.jasongj.com/java/concurrenthashmap/http://www.importnew.com/28263.html var gitment = new Gitment({ id: '深入分析ConcurrentHashMap', owner: 'Luyu05', repo: 'luyu.net', oauth: { client_id: '66302af0aad3978dc224', client_secret: '2f69936866c0b9a0905542e613096d964aa35b20', }, }) gitment.render('container')]]></content>
      <categories>
        <category>多线程</category>
      </categories>
      <tags>
        <tag>多线程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深入分析HashMap]]></title>
    <url>%2F2018%2F07%2F10%2FHashMap%2F</url>
    <content type="text"><![CDATA[带着问题深入分析HashMapPs有任何问题请在下方评论，或者邮箱联系我luyucareer@163.com Q:HashMap的数据结构Node[] table123456789101112static class Node&lt;K,V&gt; implements Map.Entry&lt;K,V&gt; &#123; final int hash; final K key; V value; Node&lt;K,V&gt; next; Node(int hash, K key, V value, Node&lt;K,V&gt; next) &#123; this.hash = hash; this.key = key; this.value = value; this.next = next; &#125;&#125; 这里想要多说几句关于final1)final修饰类，表示该类不希望被继承2)final修饰方法，两个作用，第一不希望该方法在子类被覆写；第二在早起java版本中final修饰的方法会转为内嵌调用，可以提升效率3)final修饰变量，如果是基本数据类型的变量，则其数值一旦在初始化之后便不能更改；如果是引用类型的变量，则在对其初始化之后便不能再让其指向另一个对象。but指向的对象内部可以随意修改而且还有一点final修饰的变量要么在定义的时候就赋值，要么在static代码块中赋值 Q:HashMap的threshold/factor/length/size这些参数的含义size:最常用的属性，表示map中k/v的个数length:表示table的长度，默认是16facotr:负载因子，默认为0.75threshold:等于length*factor,当map的size超过threshold时进行扩容(2倍) Q:1.7和1.8 HashMap的区别1)hash方式在JDK1.8的实现中，优化了高位运算的算法，通过hashCode()的高16位异或低16位实现的：(h = k.hashCode()) ^ (h &gt;&gt;&gt; 16)，主要是从速度、功效、质量来考虑的，这么做可以在数组table的length比较小的时候，也能保证考虑到高低Bit都参与到Hash的计算中，同时不会有太大的开销。JDK1.7中hash计算相对复杂这里不做分析Ps这里实际上1.8的hash更为简化了，因为此时即使发生碰撞也会有红黑树保证效率2)红黑树的引入JDK1.7中，当NODE变得特别长的时候，get和put操作复杂度最差会变为O(n)JDK1.8中，当NODE长度超过设定值（默认8）时候，会调用treeifyBin方法，将链表转为红黑树，最差的情况时间复杂度为O(log n)3)扩容时的优化 这里要多说几句之前理解的有些问题我们看1.7的代码可以发现，1.7的hash(key)这个方法和当前的table的容量有关系，所以扩容的时候一方面可能（注意这里说的是可能）需要重新计算每个key的hash值（私以为这也是1.7hash方法的缺点）；另一方面需要对每个key的hash值跟新的capacity-1进行与操作而1.8中hash方法已经完全和capacity解耦了，一方面在resize的时候一定不需要再去对key进行rehash了；另一方面扩容后只需要和capacity进行与操作，如果结果=0那么该元素仍然位于老的table[i]中；反之，该元素将位于table[i+oldCapacity]中 Q:以1.8为例，简要说明HashMap put操作和get操作的流程put操作:1)根据key计算hash值，再对bucket的length取模（&amp;操作）,定位到bucket的位置i2)判断bucket[i]是否是null，如果是执行resize进行扩容3)定位到bucket，之后遍历node，如果有key相同的则覆盖4)如果没有key相同的node，此时判断该bucket中node的个数，如果个数大于等于8，那么对链表进行树化并插入到树中；反之直接插入到链表末尾5)插入成功后，判断size是否大于threshold，如果大于进行resize操作get操作：1）根据key计算hash值，再对bucket的length取模（&amp;操作）,定位到bucket的位置i2）如果直接命中，那么直接返回即可3）未命中，如果是树则在树中查找当前key（O(log n)）;如果是链表那么在链表中查找当前key(O(n))4）如果没找到返回null Q:为什么HashMap的bucket数目要取2^n?而不是对hash更为友好的素数？对bucket的数量取模的操作，当bucket数量为2^n时候，可以转化为bucket&amp;(2^n-1),对计算机来说&amp;比%具有更高的效率 Q:为什么说hashMap是线程不安全的问题表现为多线程一起执行put操作，另外的线程执行get操作可能会导致死循环及使用迭代器时的fast-fail上（参考之前写过的一篇文章:foreach循环中为什么不要进行remove/add操作）这个要从hashMap的resize操作说起,resize操作最核心的方法是transfer方法 JDK1.7123456789101112131415void transfer(Entry[] newTable, boolean rehash) &#123; int newCapacity = newTable.length; for (Entry&lt;K,V&gt; e : table) &#123; while(null != e) &#123; Entry&lt;K,V&gt; next = e.next; if (rehash) &#123; e.hash = null == e.key ? 0 : hash(e.key); &#125; int i = indexFor(e.hash, newCapacity); e.next = newTable[i]; newTable[i] = e; e = next; &#125; &#125;&#125; 先来看最简单的单线程情况再来看多线程的情况假设线程1执行到next = e.next，放弃了cpu时间，轮到线程2执行线程2执行完毕，线程1继续执行，此时循环链表形成,执行对应的get时候将无法退出方法 Q:hashMap有一个线程写其他线程读会有什么问题吗？会有可见性问题，hashMap中table/size等都不是volatile的 参考链接：http://www.cnblogs.com/chengdabelief/p/7419776.htmlhttps://www.cnblogs.com/andy-zhou/p/5402984.htmlhttp://www.jasongj.com/java/concurrenthashmap/ var gitment = new Gitment({ id: '深入分析HashMap', owner: 'Luyu05', repo: 'luyu.net', oauth: { client_id: '66302af0aad3978dc224', client_secret: '2f69936866c0b9a0905542e613096d964aa35b20', }, }) gitment.render('container')]]></content>
      <categories>
        <category>多线程</category>
      </categories>
      <tags>
        <tag>多线程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深入分析java线程池]]></title>
    <url>%2F2018%2F07%2F09%2FThreadPool%2F</url>
    <content type="text"><![CDATA[1.线程池状态2.构造函数3.常用的几种线程池4.任务执行5.后处理6.线程池的终止Ps有任何问题请在下方评论，或者邮箱联系我luyucareer@163.com 线程池状态线程池的几种状态 RUNNING SHUTDOWN STOP TIDYING TERMINATED几种状态之间的流转关系如下图123456789101112131415private final AtomicInteger ctl = new AtomicInteger(ctlOf(RUNNING, 0));private static final int COUNT_BITS = Integer.SIZE - 3;private static final int CAPACITY = (1 &lt;&lt; COUNT_BITS) - 1;// runState is stored in the high-order bitsprivate static final int RUNNING = -1 &lt;&lt; COUNT_BITS; //111private static final int SHUTDOWN = 0 &lt;&lt; COUNT_BITS; //000private static final int STOP = 1 &lt;&lt; COUNT_BITS; //001private static final int TIDYING = 2 &lt;&lt; COUNT_BITS; //010private static final int TERMINATED = 3 &lt;&lt; COUNT_BITS; //011// Packing and unpacking ctlprivate static int runStateOf(int c) &#123; return c &amp; ~CAPACITY; &#125;private static int workerCountOf(int c) &#123; return c &amp; CAPACITY; &#125;private static int ctlOf(int rs, int wc) &#123; return rs | wc; &#125; 重点关注AtomicInteger ctl 共32位，其中高3位表示”线程池状态”，低29位代表”线程池中的任务数量”各个状态详细说明如下:12345678* RUNNING: Accept new tasks and process queued tasks* SHUTDOWN: Don't accept new tasks, but process queued tasks* STOP: Don't accept new tasks, don't process queued tasks,* and interrupt in-progress tasks* TIDYING: All tasks have terminated, workerCount is zero,* the thread transitioning to state TIDYING* will run the terminated() hook method* TERMINATED: terminated() has completed 构造函数123456789101112131415161718192021public ThreadPoolExecutor(int corePoolSize, int maximumPoolSize, long keepAliveTime, TimeUnit unit, BlockingQueue&lt;Runnable&gt; workQueue, ThreadFactory threadFactory, RejectedExecutionHandler handler) &#123; if (corePoolSize &lt; 0 || maximumPoolSize &lt;= 0 || maximumPoolSize &lt; corePoolSize || keepAliveTime &lt; 0) throw new IllegalArgumentException(); if (workQueue == null || threadFactory == null || handler == null) throw new NullPointerException(); this.corePoolSize = corePoolSize; this.maximumPoolSize = maximumPoolSize; this.workQueue = workQueue; this.keepAliveTime = unit.toNanos(keepAliveTime); this.threadFactory = threadFactory; this.handler = handler;&#125; 参数说明Ps 线程池中有两大容器，分别是workers(Set:存放线程 保存消费者) 和 queue(blockingQueue:存放任务 保存生产的任务) corePoolSize 核心线程的数量，线程池初始化后，每接到一个任务就会创建一个线程来执行任务，直到当前的线程数目到达corePoolSize，此时新的任务将会进入queue中，只有当queue满了之后，maximunPoolSize才发挥作用核心线程被保存在pool中，即使线程处于闲置状态也不会被回收，除非allowCoreThreadTimeOut被设置，从名字可以看出这是用来控制核心线程是否可以超时被回收的一个参数。Ps核心线程可以理解为工厂的长工 maximumPoolSize pool中所允许的最大线程数。线程池的queue满了之后，如果还有新的任务到来，此时如果线程数目小于maximumPoolSize，则会新建线程来执行任务。Ps 非核心线程可以理解为工厂的短工 最大值=maximumPoolSize-corePoolSize keepAliveTime 线程空闲的时间，默认情况该参数只针对”短工”有效（短工空闲太久就要被辞退），只有当配置allowCoreThreadTimeOut时该参数才对”长工”生效 unit keepAliveTime的单位 workQueue 上文提到的queue，用来保存等待执行的任务的阻塞队列 ThreadFactory 线程工厂，可以用户自己配置，默认的ThreadFactory 1.给线程命名 2.将线程设置为非守护线程 3.优先级设置为NORM handler 拒绝策略：当线程数=maximumPoolSize 且 queue已满 这时候新提交的任务会被拒绝（消费者已达到max，而待消费的任务也达到max） 1.AbortPolicy：直接抛出异常，默认策略； 2.CallerRunsPolicy：用调用者所在的线程来执行任务； 3.DiscardOldestPolicy：丢弃阻塞队列中靠最前的任务，并执行当前任务； 4.DiscardPolicy：直接丢弃任务； 常用的几种线程池FixedThreadPool12345public static ExecutorService newFixedThreadPool(int nThreads) &#123; return new ThreadPoolExecutor(nThreads, nThreads, 0L, TimeUnit.MILLISECONDS, new LinkedBlockingQueue&lt;Runnable&gt;());&#125; 此时corePoolSize = maximumPoolSize , 这意味着不再有任何”短工”，当线程池中的线程数量达到corePoolSize后，新来的任务被放到queue中，当queue满了之后，线程池将不再接收任何任务，如果再继续提交任务，则会直接执行拒绝策略。这里执行的是默认的拒绝策略，直接抛出异常。这里的queue使用的是LinkedBlockingQueue，queue的容量是MAX_VALUE，所以构造函数中的maximumPoolSize与keepAliveTime几乎无效而ThreadPoolExectutor中的allowCoreThreadTimeOut也未被设置，所以默认为false SingleThreadExecutor123456public static ExecutorService newSingleThreadExecutor() &#123; return new FinalizableDelegatedExecutorService (new ThreadPoolExecutor(1, 1, 0L, TimeUnit.MILLISECONDS, new LinkedBlockingQueue&lt;Runnable&gt;()));&#125; 相当于newFixedThreadPool(1)case：在for循环内部创建了过多线程池且未shutdown上面的case 就是在for循环内部创建了过多的singleThreadExecutor，每个singleThreadExecutor会维持一个核心线程（不调用shutdown，该线程不会释放的），最后导致OOM CachedThreadPool12345public static ExecutorService newCachedThreadPool() &#123; return new ThreadPoolExecutor(0, Integer.MAX_VALUE, 60L, TimeUnit.SECONDS, new SynchronousQueue&lt;Runnable&gt;());&#125; 任务一旦提交，首先会进入queue，此时的queue是synchronousQueue（不存储任何元素），所以会直接创建新的”短工”（线程），当这些线程闲置超过60s之后会被终止。CachedThreadPool 会有一个问题 可能会创建大量的线程 可能导致耗尽CPU 和 内存。所以 一定要控制并发量 任务执行线程池有两种执行任务的方式，execute() &amp; submit(),submit一般用于有返回值的场景并且该方法是从ThreadPoolExecutor的父类中继承的。以execute()为例子，分析内部的执行原理1234567891011121314151617181920212223242526272829//ThreadPoolExecutor.classpublic void execute(Runnable command) &#123; if (command == null) throw new NullPointerException(); //获取当前线程池ctl //value是volatile类型的 int c = ctl.get(); //正常情况，新任务新线程 if (workerCountOf(c) &lt; corePoolSize) &#123; if (addWorker(command, true)) return; c = ctl.get(); &#125; //bq offer 线程安全 //核心线程max，塞到队列 if (isRunning(c) &amp;&amp; workQueue.offer(command)) &#123; //入队成功，二次check状态 int recheck = ctl.get(); if (! isRunning(recheck) &amp;&amp; remove(command)) reject(command); //插入队列后发现 worker数量为0，需要新建worker //比如说cachedThreadExecutor，经常会有这种情况 else if (workerCountOf(recheck) == 0) addWorker(null, false); &#125; //队列满了，新建非核心线程 else if (!addWorker(command, false)) reject(command);&#125; 整个代码分三个步骤：a)如果当前线程数目小于corePoolSize,则直接调用addWorker创建新线程来执行任务，如果addWorker失败，再次读取ctl,并执行步骤bb)如果线程池处于RUNNING状态且当前任务进入queue成功，此时会对当前的ctl进行二次检查， 如果在这个过程中线程池不处于RUNNING状态，那么从queue中移除任务并执行拒绝策略 反之，如果二次检查通过，如果当前线程数目为0，则会创建新的工作线程，否则queue中的任务将无法执行。Ps等下分析addWorker(null, false)c)入队失败，此时调用addWorker(command,false)创建非核心线程，若创建失败执行拒绝策略。Ps显然addWorker方法需要再次对线程池状态进行判断，不然b中如果非RUNNING状态，也会执行到c addWorker() Ps有意思的一段code12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970private boolean addWorker(Runnable firstTask, boolean core) &#123; retry: //双层死循环，当新增worker成功时break，状态不满足时直接return，cas失败&amp;状态变更时continue //这里的目的是通过cas增加automicInteger类型的ctl的值 for (;;) &#123; int c = ctl.get(); int rs = runStateOf(c); // Check if queue empty only if necessary. if (rs &gt;= SHUTDOWN &amp;&amp; ! (rs == SHUTDOWN &amp;&amp; firstTask == null &amp;&amp; ! workQueue.isEmpty())) return false; for (;;) &#123; int wc = workerCountOf(c); if (wc &gt;= CAPACITY || wc &gt;= (core ? corePoolSize : maximumPoolSize)) return false; if (compareAndIncrementWorkerCount(c)) break retry; c = ctl.get(); // Re-read ctl if (runStateOf(c) != rs) continue retry; // else CAS failed due to workerCount change; retry inner loop &#125; &#125; //只有cas成功新建的worker才能开始工作 boolean workerStarted = false; boolean workerAdded = false; Worker w = null; try &#123; //类级别的锁 final ReentrantLock mainLock = this.mainLock; w = new Worker(firstTask); final Thread t = w.thread; if (t != null) &#123; mainLock.lock(); try &#123; // Recheck while holding lock. // Back out on ThreadFactory failure or if // shut down before lock acquired. int c = ctl.get(); int rs = runStateOf(c); if (rs &lt; SHUTDOWN || (rs == SHUTDOWN &amp;&amp; firstTask == null)) &#123; if (t.isAlive()) // precheck that t is startable throw new IllegalThreadStateException(); workers.add(w); int s = workers.size(); if (s &gt; largestPoolSize) largestPoolSize = s; workerAdded = true; &#125; &#125; finally &#123; mainLock.unlock(); &#125; if (workerAdded) &#123; t.start(); workerStarted = true; &#125; &#125; &#125; finally &#123; if (! workerStarted) addWorkerFailed(w); &#125; return workerStarted;&#125; a)读取当前线程池状态 如果rs&gt;=SHUTDOWN（线程池处于SHUTDOWN,STOP,TIDYING,TERMINATED状态），这个时候addWorker是要失败的，除了一种情况，当前线程池处于SHUTDOWN状态，且firstTask=null且queue不为空，我们知道这种情况，线程池是要执行完queue中的任务的，所以这种情况是可以增加worker的。b) 内层循环，如果当前线程的数量满足要求（核心线程数&lt;corePoolSize 非核心线程&lt;maximumPoolSize），那么通过CAS使woker自增，成功跳出双重循环，否则继续自旋c) 获取锁，再次判断线程状态，如果rs&lt;SHUTDOWN 或者rs=SHUTDOWN且当前入参的firstTask=null 才可以继续向下执行，将woker添加到wokers(Set)中，然后释放锁并启动线程；否则直接释放锁。Ps这里加锁的目的一是为了wokers添加元素，二是为了更新largestPoolSized) t.start()启动任务 Worker12345678910111213141516171819202122232425private final class Worker extends AbstractQueuedSynchronizer implements Runnable&#123; private static final long serialVersionUID = 6138294804551838833L; /** Thread this worker is running in. Null if factory fails. */ final Thread thread; /** Initial task to run. Possibly null. */ Runnable firstTask; /** Per-thread task counter */ volatile long completedTasks; Worker(Runnable firstTask) &#123; setState(-1); // inhibit interrupts until runWorker this.firstTask = firstTask; //构造函数传入的ThreadFactory去创建线程 this.thread = getThreadFactory().newThread(this); &#125; /** Delegates main run loop to outer runWorker */ public void run() &#123; runWorker(this); &#125; /**省略部分代码*/ Worker继承自AQS且实现了Runnable接口 Ps这里state设置为-1的目的是这时候禁止中断，直到线程开始运行才可以中断线程启动的时候，实际上是调用了runWorker方法123456789101112131415161718192021222324252627282930313233343536373839404142434445final void runWorker(Worker w) &#123; Thread wt = Thread.currentThread(); Runnable task = w.firstTask; w.firstTask = null; w.unlock(); // allow interrupts boolean completedAbruptly = true;//突然完成 try &#123; while (task != null || (task = getTask()) != null) &#123; //shutDwon的时候，会去遍历works中的work执行tryLock如果成功 //代表当前线程处于idle状态，那么将其中断 w.lock(); // If pool is stopping, ensure thread is interrupted; // if not, ensure thread is not interrupted. This // requires a recheck in second case to deal with // shutdownNow race while clearing interrupt if ((runStateAtLeast(ctl.get(), STOP) || (Thread.interrupted() &amp;&amp; runStateAtLeast(ctl.get(), STOP))) &amp;&amp; !wt.isInterrupted()) wt.interrupt(); try &#123; beforeExecute(wt, task); Throwable thrown = null; try &#123; task.run(); &#125; catch (RuntimeException x) &#123; thrown = x; throw x; &#125; catch (Error x) &#123; thrown = x; throw x; &#125; catch (Throwable x) &#123; thrown = x; throw new Error(x); &#125; finally &#123; afterExecute(task, thrown); &#125; &#125; finally &#123; task = null; w.completedTasks++; w.unlock(); &#125; &#125; completedAbruptly = false; &#125; finally &#123; processWorkerExit(w, completedAbruptly); &#125;&#125; a)方法一开始就调用unlock方法，此时会将state置为0（代表解锁状态，此时允许线程中断）interruptWorkers()方法只有在state&gt;=0才能执行b)有趣的循环，首先执行传递进来的任务，如果当前任务执行完毕，会调用getTask方法去queue中提取任务执行c)当前有任务执行时，首先获取锁置state=1 如果当前线程池状态&gt;=STOP 且当前线程未被中断，那么中断当前线程 如果当前线程池状态=STOP 那么中断当前线程d) 终于可以执行任务了，这里有两个模板方法，beforeExecute and afterExecute，ThreadPoolExecutor中给了空实现e)最后置task=null 并将当前Woker的工作量++ 最后释放锁 接下来继续循环获取queue中的任务 getTask()1234567891011121314151617181920212223242526272829303132333435363738394041424344454647private Runnable getTask() &#123; boolean timedOut = false; // Did the last poll() time out? retry: for (;;) &#123; int c = ctl.get(); int rs = runStateOf(c); // 1）rs &gt;= STOP 2) rs = SHUTDOWN &amp;&amp; workQueue为空 // 无需去队列拿去任务，此时可以销毁当前worker if (rs &gt;= SHUTDOWN &amp;&amp; (rs &gt;= STOP || workQueue.isEmpty())) &#123; decrementWorkerCount(); return null; &#125; boolean timed; // Are workers subject to culling? for (;;) &#123; int wc = workerCountOf(c); //如下两种情况需要引入超时机制 //1）核心线程具有超时机制 2）worker数量大于corePoolSize timed = allowCoreThreadTimeOut || wc &gt; corePoolSize; //当前worker数量小于max &amp;&amp; !(符合超时机制且超时) if (wc &lt;= maximumPoolSize &amp;&amp; ! (timedOut &amp;&amp; timed)) break; //超时执行CAS if (compareAndDecrementWorkerCount(c)) return null; // else CAS failed due to workerCount change; retry inner loop c = ctl.get(); // Re-read ctl if (runStateOf(c) != rs) continue retry; &#125; try &#123; Runnable r = timed ? workQueue.poll(keepAliveTime, TimeUnit.NANOSECONDS) : workQueue.take(); if (r != null) return r; //此时再次进入for循环，会执行workerCount-1 &amp; return null timedOut = true; &#125; catch (InterruptedException retry) &#123; timedOut = false; &#125; &#125;&#125; a) 首先根据线程池的状态进行下一步的决策，如果当前线程池状态&gt;=STOP or =SHUTDOWN且queue为空 那么woker数量– 并返回nullb)接下来判断是否需要超时机制，如果allowCoreThreadTimeOut 或者 当前线程数大于corePoolSize（说明有短工） 那么需要有超时控制c)如果当前线程的数量处于正常状态，&lt;=maximumPoolSize 且timeout timed其中有一个是false(非有超时机制且已经超时的情况) 那么跳出内层循环 直接去从queue中获取任务 并return Ps如果当前线程数目大于max 或 线程有超时机制并已经超时 那么workerCount –d) 如果从queue中取值结果为null 那么timeout会被置true 再次执行内层循环 简单说就是queue中无任务 且 有超时机制 那么workcount要– （这里ctl– 与之等价） Ps 阻塞队列的take 在队列为空的时候 会一直阻塞 而poll 超时后会返回一个null 后处理在runwoker方法中 finally代码块中调用了 processWorkerExit方法 用来对Worker进行后处理123456789101112131415161718192021222324252627private void processWorkerExit(Worker w, boolean completedAbruptly) &#123; if (completedAbruptly) // If abrupt, then workerCount wasn't adjusted decrementWorkerCount(); final ReentrantLock mainLock = this.mainLock; mainLock.lock(); try &#123; completedTaskCount += w.completedTasks; workers.remove(w); &#125; finally &#123; mainLock.unlock(); &#125; tryTerminate(); int c = ctl.get(); if (runStateLessThan(c, STOP)) &#123; if (!completedAbruptly) &#123; int min = allowCoreThreadTimeOut ? 0 : corePoolSize; if (min == 0 &amp;&amp; ! workQueue.isEmpty()) min = 1; if (workerCountOf(c) &gt;= min) return; // replacement not needed &#125; addWorker(null, false); &#125;&#125; a)如果是被意外终止（用户异常），那么将worker的数量-1b)加锁去更新共享参数completedTaskCount 加上当前worker处理的任务数量 并从set中移除当前wokerc)调用tryTerminate方法（下文有源码）d)再次判断当前ctl，如果当前状态&lt;STOP,1）意外中断，那么需要补上一个worker；2）非意外中断，但当前worker已经比最小值还低，那么需要补上一个worker tryTerminate()1234567891011121314151617181920212223242526272829303132333435final void tryTerminate() &#123; for (;;) &#123; int c = ctl.get(); //1.running 2.tidying or terminated 3.shutdown and queue is not empty if (isRunning(c) || runStateAtLeast(c, TIDYING) || (runStateOf(c) == SHUTDOWN &amp;&amp; ! workQueue.isEmpty())) return; //wokercount &gt; 0 有资格去终止 if (workerCountOf(c) != 0) &#123; // Eligible to terminate //terminate one worker interruptIdleWorkers(ONLY_ONE); return; &#125; final ReentrantLock mainLock = this.mainLock; mainLock.lock(); try &#123; //set ctl -&gt; ctlof(TIDYING,0) if (ctl.compareAndSet(c, ctlOf(TIDYING, 0))) &#123; try &#123; terminated(); &#125; finally &#123; //set ctl -&gt; ctlof(TERMINATED,0) ctl.set(ctlOf(TERMINATED, 0)); termination.signalAll(); &#125; return; &#125; &#125; finally &#123; mainLock.unlock(); &#125; // else retry on failed CAS &#125;&#125; a)如果当前状态either (SHUTDOWN and pool and queue empty) or (STOP and pool empty) 那么直接returnb)如果当前woker数！=0 那么可以去中断，那么调用interruptIdleWorkers(ONLY_ONE) 去终止Workerc)如果当前worker = 0 那么去终止线程池 首先将ctl置为TIDYING&amp;0 接下来调用terminated()终止线程池 ，最后将ctl置为TERMINATED&amp;0 线程池的终止shutdown()123456789101112131415public void shutdown() &#123; final ReentrantLock mainLock = this.mainLock; mainLock.lock(); try &#123; checkShutdownAccess(); //SHUTDOWN advanceRunState(SHUTDOWN); // 中断空闲的线程 interruptIdleWorkers(); onShutdown(); // hook for ScheduledThreadPoolExecutor &#125; finally &#123; mainLock.unlock(); &#125; tryTerminate(); &#125; shutDownNow()123456789101112131415161718public List&lt;Runnable&gt; shutdownNow() &#123; List&lt;Runnable&gt; tasks; final ReentrantLock mainLock = this.mainLock; mainLock.lock(); try &#123; checkShutdownAccess(); // STOP advanceRunState(STOP); // 中断所有线程 interruptWorkers(); //取出queue中的任务 最后return tasks = drainQueue(); &#125; finally &#123; mainLock.unlock(); &#125; tryTerminate(); return tasks; &#125; 几个细节1.当调用shutdown的时候，有可能将所有的worker线程都中断或者当前worker &lt; corePoolSize不过此时worker线程会执行processWorkerExit(curWorker,conpletedAbruptly true)该方法内部会将worker补齐至所需线程的最小值(1 or corePoolSize) Ps addWork(null,false)这里就是经典的 rs == SHUTDOWN &amp;&amp; firstTask == null &amp;&amp; ! workQueue.isEmpty()2.当调用shutdown时候有一行关键代码interruptIdleWorkers(),这个方法会执行worker.tryLock如果成功则对worker线程执行interrupt()；而当worker线程工作的时候会首先执行worker.lock()，也就是说正在工作的worker是不会被中断的3.shutdownNow不care queue中的排队线程，不过会返回当前存储任务的blocking queue4.workers中的worker有的是带着任务进入的集合，这部分要先执行所带的任务，之后再去queue中取任务执行；而没有任务的worker直接去queue中拉取任务执行5.Worker类继承自AQS，此时volatile类型的state表示worker是否可以中断6.对workers(普通的hashSet)的add/remove操作都需要上锁7.实际上在workers内部不区分core和non-core，当worker数量大于corePoolSize且超时时候，通过CAS操作对部分执行processWorkerExit8.通过block queue的poll(time)实现worker的超时机制9.普通的超时退出，不会对线程执行interrupt()，此时只是简单地从workers中remove；只有主动调用shutdown/shutdownNow才会 线程池自问自答1.线程池如何处理queue中的任务？worker执行完当前任务后，会从queue中获取任务并执行 2.线程池如何引入超时机制？通过从queue中获取任务过程引入，Blockingqueue.poll(time,timeUnit)；没有超时机制的时候调用take()方法阻塞在那里 3.shutdown和shutdownNow区别前者不再接收新的任务，但是会将当前池子内的任务执行完毕；后者直接中断所有worker线程，此时会将queue中任务返回 4.线程池有几种状态？RUNNING SHUTDOWN STOP TIDYING TERMINATE 5.Worker为啥继承AQS在woker执行任务的时候加锁，比如调用shutDwon方法时候，会遍历所有的worker如果当前worker没有加锁（视为idle线程）会被执行中断操作 6.jdk提供的几种线程池的区别FixedThreadPool 只有核心线程，没有短工（所以没有超时时间），queue使用的是无界的阻塞队列CachedTheadPool 没有核心线程，只有短工（默认超时时间为60s） queue是没有存储能力的阻塞队列 7.什么情况会调用addWorker(null,false)发现当前线程池中的Worker数量不够的时候当新增任务的时候，入队之后发现queue中worker数量为0；worker线程退出时候会检测当前线程的数量，如果小于最小值（0 or corePoolSize）需要addWorker(null,false) 8.CachedTheadPool中的SynchronousQueue既然没有存储能力，那有什么用？线程池的超时机制就是通过queue来实现的，queueu.poll(time,unit);所以SynchronousQueue可以用来控制超时机制 小结线程池的设计真的是巧妙，通过一些参数比如addWorker(Runnable firstTask, boolean core)表明是否是核心线程；completedAbruptly表示当前线程是否是意外退出，如果意外退出会有一些补偿机制12345678try &#123; while (task != null || (task = getTask()) != null) &#123; blala &#125; completedAbruptly = false; &#125; finally &#123; processWorkerExit(w, completedAbruptly); &#125; 再比如，interruptIdleWorkers(ONLY_ONE)和interruptIdleWorkers()分别表示中断一个线程和中断所有线程; var gitment = new Gitment({ id: '深入分析java线程池', owner: 'Luyu05', repo: 'luyu.net', oauth: { client_id: '66302af0aad3978dc224', client_secret: '2f69936866c0b9a0905542e613096d964aa35b20', }, }) gitment.render('container')]]></content>
      <categories>
        <category>多线程</category>
      </categories>
      <tags>
        <tag>多线程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深入理解happen-before原则]]></title>
    <url>%2F2018%2F07%2F06%2FDCL%2F</url>
    <content type="text"><![CDATA[happen-before 这个概念在最早接触并发编程的时候就知道，但是一直不得要领，不知道想表达的是什么。最近拜读了一些博客，还算有所领悟，于是写下来。先把教科书上的内容贴一下，最后结合几个例子彻底搞懂它。 happens before概念：如果存在hb(a,b)，那么操作a及a之前在内存上面所做的操作（如赋值操作等）都对操作b可见，即操作a影响了操作bPs:hb(a,b) presents “a happens before b”Ps:先行发生是一个逻辑上的概念，并非真实的执行的先后顺序 1）程序次序规则（Program Order Rule） 在一个线程内，按照程序代码顺序，书写在前面的操作Happens-Before书写在后面的操作 线程中上一个动作及之前的所有写操作在该线程执行下一个动作时对该线程可见（也就是说，同一个线程中前面的所有写操作对后面的操作可见），在同一个线程内，即使发生了指令重排序，书写在前面的代码也是先行发生于书写在后面的代码的 2）线程锁定规则（Monitor Lock Rule） An unlock on a monitor happens-before every subsequent lock on that monitor. 如果线程1解锁了monitor a，接着线程2锁定了a，那么，线程1解锁a及其之前的（写）操作都对线程2可见（线程1和线程2可以是同一个线程）。 3）volatile变量规则（volatile Variable Rule） A write to a volatile field happens-before every subsequent read of that volatile. 如果线程1写入了volatile变量v（这里和后续的“变量”都指的是对象的字段、类字段和数组元素），接着线程2读取了v，那么，线程1写入v及之前的写操作都对线程2可见（线程1和线程2可以是同一个线程）。 4）线程启动规则（Thread Start Rule） Thread对象的start()方法Happens-Before此线程的每一个动作。 5）线程终止规则（Thread Termination Rule） 线程中的所有操作都Happens-Before对此线程的终止检测。 6）线程中断规则（Thread Interruption Rule） 对线程interrupt()方法的调用Happens-Before被中断线程的代码检测到中断事件的发生，可以通过Thread.interrupt()方法检测到是否有中断发生。 7）对象终结规则（Finalizer Rule） 一个对象的初始化完成（构造函数执行结束）Happens-Before它的finalize()方法的开始。 8）传递性（Transitivity） 偏序关系的传递性：如果已知hb(a,b)和hb(b,c)，那么我们可以推导出hb(a,c)，即操作a Happens-Before 操作c。 很重要的特性，很多地方利用基本的先行发生保证了可见性 时间上的先后顺序和先行发生原则没有太大的关系12int i = 1;int j = 2; 同一个线程中显然有hb(i=1, j=2) 但是有可能发生指令重排序导致j=2先执行了，所以hb(i=1, j=2)并不代表在时间上也是i=1先于j=2发生 1234567private int value = 0;public void setValue(int value) &#123; this.value = value;&#125;public int getValue() &#123; return value;&#125; 假设线程1先执行了set方法，线程2才执行get方法，从时间上看set方法先于get发生，但是set和get并没有先行发生的关系，所以一个操作在时间上先于另一个操作发生也不能代表前者先行发生于后者 看几个利用happen before规则的例子例子1：以CopyOnWriteArrayList的set和get方法为例，我们看看是如何保证的hb(set, get)。在这个类的javadoc上有这样一段声明12345* Memory consistency effects: As with other concurrent* collections, actions in a thread prior to placing an object into a* &#123;@code CopyOnWriteArrayList&#125;* happen-before actions subsequent to the access or removal of that element from* the &#123;@code CopyOnWriteArrayList&#125; in another thread. 简单地说就是对并发容器的写先行发生于后续对它的读和remove操作，那这个如何保证的呢？123456789101112131415161718192021222324 public E set(int index, E element) &#123; final ReentrantLock lock = this.lock; lock.lock(); try &#123; Object[] elements = getArray(); E oldValue = get(elements, index); if (oldValue != element) &#123; int len = elements.length; Object[] newElements = Arrays.copyOf(elements, len); newElements[index] = element; setArray(newElements); &#125; else &#123; // Not quite a no-op; ensures volatile write semantics setArray(elements); &#125; return oldValue; &#125; finally &#123; lock.unlock(); &#125;&#125;final void setArray(Object[] a) &#123; array = a;&#125; 123456public E get(int index) &#123; return get(getArray(), index);&#125;final Object[] getArray() &#123; return array;&#125; 代码中的array是volatile类型的，而根据happen before原则，对array的写会先行发生于对它的读。根据happen before法则有hb(set之前的代码,setArray),hb(getArray,get之后的代码)根据传递性有hb(set,get) 这里有一行注解很值得推敲，// Not quite a no-op; ensures volatile write semantics我们看到在else中竟然也执行了一次set操作。其实这正是为了实现java doc中set happen before get而写的代码，不然当set一个相同的元素的时候就不能保证happen before了.而hb(set, get)这一特性可能会被其他地方使用以实现新的hb，所以即使相同元素的情况也要执行一次volatile的写。 例子2：1234567891011121314151617181920212223242526272829303132333435363738/** * Created by luyu on 2019/1/28. */public class Test0128 &#123; //static volatile int a = 1; static private boolean initialized = false; public static void main(String[] args) throws InterruptedException &#123; new Thread(new Runnable() &#123; public void run() &#123; doWork(); &#125; &#125;).start(); Thread.sleep(1000); new Thread(new Runnable() &#123; public void run() &#123; init(); &#125; &#125;).start(); &#125; public static void init() &#123; initialized = true; //a = 2; &#125; public static void doWork() &#123; while (!initialized) &#123; //int b = a; &#125; System.out.println("end loop"); &#125;&#125; 这个例子是经典的说明volatile关键字用处的例子，这个例子运行之后会发现doWork一直处于死循环的状态。通常的做法是对initialized添加一个volatile修饰符，但我们这里换一种方式跳出死循环。以加深对hb原则的理解。 具体的方法就是将上面code中关于a的注释都取消，这个时候我们发现console会打印出”end loop”。简单分析下原因，根据hb原则有hb(initialized = true, a = 2)，类似的hb(int b = a, while (!initialized))，而对volatile的写hb于对它的读，即 hb(a = 2, int b = a)，根据传递性最后推导出hb(initialized = true, while (!initialized))，所以这个时候可以跳出死循环。 例子3：FutureTask的get与set（jdk6)，result并非volatile变量，怎么保证写对读的可见呢？1234567891011121314151617181920void innerSet(V v) &#123; for (;;) &#123; int s = getState(); if (s == RAN) return; if (s == CANCELLED) &#123; // aggressively release to set runner to null, // in case we are racing with a cancel request // that will try to interrupt runner releaseShared(0); return; &#125; if (compareAndSetState(s, RAN)) &#123; result = v; releaseShared(0); done(); return; &#125; &#125;&#125; 123456789V innerGet(long nanosTimeout) throws InterruptedException, ExecutionException, TimeoutException &#123; if (!tryAcquireSharedNanos(0, nanosTimeout)) throw new TimeoutException(); if (getState() == CANCELLED) throw new CancellationException(); if (exception != null) throw new ExecutionException(exception); return result;&#125; 因为 hb(result = v, releaseShared（对volatile的写))hb(tryAcquireSharedNanos(对volatile的读), return result)hb(releaseShared, tryAcquireSharedNanos)所以 hb(result = v, return result) 线程不安全的DCL1234567891011121314public class Singleton &#123; private static Singleton instance = null; private Singleton()&#123;&#125; public static Singleton getInstance()&#123; if(instance == null)&#123; //① synchronized (Singleton.class) &#123; if(instance == null)&#123; //② instance = new Singleton(); //③ &#125; &#125; &#125; return instance; //④ &#125;&#125; 如果Thread1执行到③放弃了cpu时间，转到Therad2执行假设此时执行到①这个时候就很有可能出现线程不安全的问题先来分析下③这行代码都做了什么，大体可以分为三个步骤：1）分配内存2）初始化内存3）将instance指向内存当Thread1在执行过程中，CPU对这三个步骤进行了重排序，可能执行顺序为1)-&gt;3)-&gt;2)因为从单个线程的角度来看，即使顺序变为1 3 2对最终的结果也没有任何影响，但是对cpu来讲效率or利用率变高了但从多线程角度来看，如果cpu对③进行了重排序，Thread1在执行到3）时放弃了CPU时间，而Thread2执行到①，这个时候会返回一个false，直接将未完全初始化的instance返回给调用方（我理解这个时候已经不会再报NPE了，而是一个未经过初始化的内存可能会产生很多莫名其妙的问题），这显然是不行的所以需要对这段代码进行改造，方法很简单只要在instance前面添加volatile关键字就ok注意这里的volatile的作用是保证对其修饰的变量的构造不会发生重排序(对新对象的引用的写入操作不会与对象中各个域的写入操作重排序)]]></content>
      <categories>
        <category>多线程</category>
      </categories>
      <tags>
        <tag>多线程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[线程之间的协作]]></title>
    <url>%2F2018%2F07%2F05%2Fcoop-threads%2F</url>
    <content type="text"><![CDATA[本文讲述下线程间协作的集中方式Thread,join()CountdownLatchCyclicBarrierSemaphore Thread.join()适用于一个线程等待另外n个线程执行结束再去做某件事的场景，需要持有其他线程的引用123456789101112131415161718192021222324252627282930public class joinTest &#123; private static Thread[] ts = new Thread[10]; private static AtomicInteger ai = new AtomicInteger(0); public joinTest() &#123; for (int i = 0; i &lt; 10; i++) &#123; ts[i] = new Thread() &#123; @Override public void run() &#123; try &#123; Thread.sleep(100); &#125; catch (Exception e) &#123; &#125; ai.incrementAndGet(); &#125; &#125;; &#125; &#125; public static void main(String[] args) throws Exception &#123; new joinTest(); for(int i;i&lt;10;i++)&#123; ts[i].start(); &#125; for(int i;i&lt;10;i++)&#123; ts[i].join(); &#125; System.out.println(ai); &#125;&#125; 输出结果10 CountdownLatch和Thread.join类似，一个线程在另外n个线程都执行过某个点后才去执行（不需要等待线程执行完毕），此时不需要持有其他线程的引用12345678910111213141516171819202122232425262728293031323334public class controlThread &#123; private static final int ThreadCount = 50; private static AtomicInteger result = new AtomicInteger(0); private static CountDownLatch endCount = new CountDownLatch(ThreadCount); static void helper()&#123; for(int i=0;i&lt;50;i++)&#123; new Thread() &#123; @Override public void run() &#123; result.incrementAndGet(); endCount.countDown(); try &#123; Thread.sleep(5000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; System.out.println("awake"); &#125; &#125;.start(); &#125; &#125; public static void main(String[] args) throws Exception&#123; new Thread()&#123; @Override public void run()&#123; helper(); &#125; &#125;.start(); endCount.await(); print(result); &#125;&#125; 输出结果50过5s后 50个awake CyclicBarrier适用于一起开始的业务场景，n个线程大家互相等待，都完成了才继续进行比如多线程计算的场景而且是可重用的123456789101112131415161718192021222324252627282930313233public class cyclicBarrierTest &#123; private static CyclicBarrier cyclic = new CyclicBarrier(5,new BarrierRun()); private static volatile boolean flag = false; static class BarrierRun implements Runnable&#123; @Override public void run()&#123; if(!flag) print("we five are ready!"); else print("we five are ready again!"); &#125; &#125; public static void main(String[] args) throws Exception &#123; for (int i = 0; i &lt; 5; i++) &#123; new Thread() &#123; @Override public void run() &#123; try &#123; print(Thread.currentThread()+"'s worker has comed"); cyclic.await();//会等BarrierRun执行完毕再继续向下执行 flag = true ; print(Thread.currentThread()+"wait you five so long!"); cyclic.await(); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125; &#125;.start(); &#125; &#125;&#125; 输出结果Thread[Thread-1,5,main]’s worker has comedThread[Thread-2,5,main]’s worker has comedThread[Thread-3,5,main]’s worker has comedThread[Thread-0,5,main]’s worker has comedThread[Thread-4,5,main]’s worker has comedwe five are ready!Thread[Thread-2,5,main]wait you five so long!Thread[Thread-1,5,main]wait you five so long!Thread[Thread-3,5,main]wait you five so long!Thread[Thread-4,5,main]wait you five so long!Thread[Thread-0,5,main]wait you five so long!we five are ready again! Semaphore一般用于控制同时访问特定资源的线程数量12345678910public class SemaphoreTest &#123; private final Semaphore sem = new Semaphore(5); //声明5个许可数量 public void doBusiness() throws InterruptedExcep:on &#123; sem.acquire(); //获得一个许可（许可数量减一），没有许可就阻塞 try &#123; doExecute(); //单机同一时刻最多有5个线程能进入 &#125; finally &#123; sem.release(); //归还一个许可 &#125; &#125;]]></content>
      <categories>
        <category>多线程</category>
      </categories>
      <tags>
        <tag>多线程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java Web专题（四）]]></title>
    <url>%2F2018%2F06%2F27%2Fjava-web-4%2F</url>
    <content type="text"><![CDATA[1.前后端分离 VS 不分离2.Web容器3.Apache VS Tomcat4.Web应用的部署 前后端分离 VS 不分离a）不分离Controller(Servlet)/Model(JavaBean)/View(JSP可以使用fm等模板引擎)此时JSP=前端写的html+后端写的接口b）分离 1）客户端发送一个http请求（GET 最普通的请求只是获得静态资源），首先会先去CDN节点获取静态资源（html js css etc） Ps:CDN节点一般是web服务器 2）之后在客户端本地执行js调用后端接口（也是http请求） 3）这时候http请求一般会被nginx/apache/Tengine等web服务器接收到（在这一层可以实现负载均衡） 4）之后web服务器发送请求到tomcat 5）tomcat具体执行后端代码逻辑 6）最后将response（一般是json），返回到js 7）客户端渲染页面，呈现到用户面前前后端分离与不分离最大的区别在于，是客户端拼接最终的html还是服务端拼接最终的htmlc）node.js将controller层转移到前端，此时后端只负责mvc中的model层减少沟通成本，前端可自行封装json Web容器当一个请求到达服务端后，需要有人来实例化Servlet、新建线程处理请求、调用Servlet的doPost()和doGet()方法、管理Servlet的生死，这个人就是Web容器（ Servlet容器）Servlet没有main方法，它们受控于另一个Java应用（Web容器or Servlet容器）Web服务器应用（apache）接收HTTP请求，如果该请求需要某个Servlet处理，那么将其交给Servlet容器web.xml实际上是给容器用的(告诉容器如何使用Servlet和JSP)生撸一个web容器？1）main函数2）创建和服务器的socket连接3）采用BIO/NIO等4）管理并运行Servlet5）线程管理6）翻译JSP Apache VS Tomcatapache是web服务器，tomcat是servlet容器（也可做web服务器，性能较差）apache只支持静态页面（html），tomcat支持动态页面（asp、jsp）apache可以访问tomcat的资源，反之不可（一般通过AJP协议进行通讯） 为什么不单独使用tomcat？1.tomcat对静态文件支持不好，需要web服务器来提升对静态文件的处理性能2.利用web服务器进行负载均衡以及容错 Web应用的部署WEB-INF目录下的资源都不能通过路径url直接访问 如果url并未指定某一资源，而是定位到某个目录(一般是没有匹配的servlet)，这时候会寻找DD中配置的欢迎页面（从上到下）1234&lt;welcome-file-list&gt; &lt;welcome-file&gt;index.html&lt;/welcome-file&gt; &lt;welcome-file&gt;default.jsp&lt;/welcome-file&gt;&lt;/welcome-file-list&gt; 错误页面123456&lt;error-page&gt; &lt;exception-type&gt;java.lang.Throwable&lt;/exception-type&gt; &lt;!-- type和code只能存在一个 --&gt; &lt;error-code&gt;404&lt;/error-code&gt; &lt;location&gt;/errorPage.jsp&lt;/location&gt;&lt;/error-page&gt; 一个http请求到达后端的全过程1 用户在浏览器键入url，并敲回车2 首先去dns服务器解析当前的域名对应的实际ip地址与端口号3 解析成功后，会与目标ip和端口号，建立tcp连接（三次握手）Ps 这里可能有两条连接，客户端和nginx服务器，nginx服务器和tomcat服务器4 将HTTP请求（符合HTTP协议的数据+tcp头+ip头+以太网头）发送到目标服务器5 目标服务器对HTTP请求进行”剥离”，先是去以太网头，再去ip头6 此时请求可能到了nginx服务器，nginx服务器对于动态资源的请求会分发到tomcat（通过http协议）7 这个时候就到了tomcat，再去tcp头，接下来根据HTTP协议对数据进行封装，传递给后端代码 nginx如何与tomcat通信tomcat AJP协议只能和部分apache项目配合非apache程序比如nginx，只能使用http协议 http协议和tcp协议的关系HTTP协议：生成针对目标服务器端HTTP请求报文 //人类语言的一封信TCP协议：将HTTP请求报文分割成报文段（序号），把每个报文可靠地传给对方 //邮差 也可以说，TCP/IP协议是传输层协议，主要解决数据如何在网络中传输，而HTTP是应用层协议，主要解决如何包装数据（使传输的数据有意义）实际上socket是对TCP/IP协议的封装，Socket本身并不是协议，而是一个调用接口（API），通过Socket，我们才能使用TCP/IP协议 Pshttp1.1适用于解决包含多张图片的html页面（串行）http2.0在同一个tcp连接中，可以并行发送多个请求（IO多路复用） tomcat如何与客户端建立的tcp连接客户端发送http请求（包括TCP头，用于建立TCP连接）通过浏览器与tomcat开放的端口，进行连接接下来在这条通道进行数据的收发Ps可以把http请求视为，具有特定格式的数据（符合http协议）通过tcp进行传递 tomcat/jetty/nettytomcat和jetty是一个层面上的，可以称为servlet容器而servlet容器，通信部分要么是bio ，要么是nionetty就是对java原生的nio都一种封装，大概就是这样吧 就像如果用bio生撸一个servlet容器一样，你也可以机智一些通过nio生撸一个servlet容器当然如果你聪明绝顶，你会想到使用netty生撸一个servlet容器 简单地讲tomcat/jetty = bio/nio/netty + 管理并运行Servlet]]></content>
      <categories>
        <category>Java Web</category>
      </categories>
      <tags>
        <tag>Java Web</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java Web专题（三）]]></title>
    <url>%2F2018%2F06%2F27%2Fjava-web-3%2F</url>
    <content type="text"><![CDATA[1.Servlet2.Filter3.Listener参考《Head First Servlets and JSP》 Servlet简单的MVCServlet充当Controller，JSP充当View，具体的业务逻辑以及数据库的操作充当ModelServlet的生命周期1)容器加载Servlet类2)实例化Servlet Ps容器让其成为一个普通的对象3)init(servletConfig)（可以覆盖该无参的init方法，执行一些初始化逻辑，比如数据库连接）4)service()-&gt;doPost()/doGet() etc5)destroy()HttpServletRequest中的属性举几个常见的例子1)cookies2)session3)localPort/Addr tomcat配置的地址端口号4)serverPort/Addr apache的地址和端口号5)remotePort/Addr 客户端的地址和端口号5)parameterValues/parameterHttpServletResponse1)setContentType 通知客户端浏览器当前消息类型2)getWriter/getOutputStream 获取输出流将内容输出到客户端浏览器3)sendRedirect 重定向ServletConfig VS ServletContext每个servlet有一个ServletConfig每个Web应用有一个ServletContext ServletConfig：执行Servlet的init方法时会为该Servlet创建一个唯一的ServletConfig，容器从Servlet的配置项中读取初始化参数并封装到ServletConfig中,并传递给init方法123&lt;Servlet&gt; &lt;init-param&gt; key value &lt;/init-param&gt;&lt;/Servlet&gt; 容器只能读取一次ServletConfig，就是在执行init方法的时候（这意味着如果修改servlet配置，包括initParam，需要重启容器）直接在doXX()方法中调用getServletConfig().getInitParamterNames()&amp;getInitParamter()即可获取initParam此时如果想要将initParam传递到某个Servlet或者JSP，可以利用forward指定目标，此时的模式是点对点，并不是整个应用所有部分都能访问到这些参数这时候引出ServletContext ServletContext：针对整个应用，所以下面的配置并未嵌套在某个servlet中，而是位于web-app中在每一个Servlet中，都可以这样调用getServletContext().getInitParamter(“paramName”)123&lt;context-param&gt;key value&lt;/context-param&gt; Ps二者都没有setInitParamter方法（getInitParamter），意味着无法动态改变内容如何保证ServletContext的线程安全？1）如果对ServletA的doXX()加synchronized,此时ServletB仍然可以修改context，即使该方法有加synchronized。synchronized关键字此时的作用范围是对象，如果Servlet非单例，此时如果new ServletA加的锁也是不生效的2）synchronized(getServletContext())补充Servlet是单例的param更贴近于初始化阶段（get）attribute更贴近于动态配置（set/get） FilterFilter最主要的几个方法init(FilterConfig)doFilter(ServletRequest,ServletResponse)destroy()Filter们通过chain连接在一起，在doFilter中有一个重要的方法chain.doFilter(ServletRequest,ServletResponse)为了防止servlet直接将response（HttpServletResponse）返回给容器而不经过filter执行doFilter后面的代码，一般将response进行包装后传递给下一个filter或者servlet 举个栗子Filter1234567891011public void doFilter(ServletRequest request, ServletResponse response, FilterChain chain) &#123; System.out.println("hello filter"); chain.doFilter(request, response); try &#123; Thread.sleep(3000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; System.out.println("bye filter"); &#125; Servlet12345678910PrintWriter out = response.getWriter(); out.println("&lt;!DOCTYPE HTML PUBLIC \"-//W3C HTML 4.01 Transitional//EN\"&gt;"); out.println("&lt;HTML&gt;"); out.println(" &lt;HEAD&gt;&lt;TITLE&gt;A Servlet Written By Milk&lt;/TITLE&gt;&lt;/HEAD&gt;"); out.println("&lt;BODY&gt;"); out.println("Filter test"); out.println("&lt;/BODY&gt;"); out.println("&lt;/HTML&gt;"); out.flush(); out.close(); 此时浏览器几乎立刻显示html内容，而控制台过了3s后才打印”bye filter”可见此时doFilter后面的代码，对返回到客户端的response没任何影响如何想要先执行chain.doFilter之后的代码，再返回结果给客户端应该怎么办？思路：这时候需要自己造一个假的response给到servlet,filter还要将加的response中的内容写到真的response中，再返回给客户端123456789101112131415161718public class MyResponse extends HttpServletResponseWrapper &#123; private PrintWriter cachedWriter; private CharArrayWriter bufferedWriter; public MyResponse(HttpServletResponse response) &#123; super(response); bufferedWriter = new CharArrayWriter(); cachedWriter = new PrintWriter(bufferedWriter); &#125; public PrintWriter getWriter() throws IOException &#123; return cachedWriter; &#125; public CharArrayWriter getBufferedWriter() &#123; return bufferedWriter; &#125;&#125; 1234567891011121314public void doFilter(ServletRequest request, ServletResponse response, FilterChain chain) &#123; System.out.println("hello filter"); HttpServletResponse httpResponse = (HttpServletResponse) response; MyResponse myResponse = new MyResponse(httpResponse); chain.doFilter(request,myResponse); response.getWriter().write(myResponse.getBufferedWriter().toString()); try &#123; Thread.sleep(3000); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; System.out.println("bye filter"); &#125; ListenerJava Web的main函数—Listener,在Servlet之前,完成某些全局的初始化工作Ps我们当然也可以将listener的工作，放在某个servlet中执行，前提是你确保这个servlet能够最先运行，这明显不靠谱 ServletContextListener监听ServletContext的初始化和撤销初始化时：从ServletContext得到上下文初始化参数 使用初始化参数（String）建立一个dog或者数据库连接（Object） 把数据库连接或者dog(Object)存储为一个ServletContext属性，供整个Web应用使用撤销时：关闭数据库连接 1）创建监听者类2）在web.xml中放置一个listener元素123&lt;listener&gt;listener class&lt;/listener&gt; Ps 容器根据listener class实现的接口类型得知监听的目标]]></content>
      <categories>
        <category>Java Web</category>
      </categories>
      <tags>
        <tag>Java Web</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java Web专题（二）]]></title>
    <url>%2F2018%2F06%2F26%2Fjava-web-2%2F</url>
    <content type="text"><![CDATA[1.四层模型 &amp; 七层模型2.HTTP3.HTTPS 四层模型 &amp; 七层模型网络七层模型是一个标准，而非实现。(OSI:Open Systems Interconnection Model)网络四层模型是一个实现的应用模型。网络四层模型由七层模型简化合并而来。TCP/IP 模型将 OSI 模型由七层简化为四层TCP/IP 协议中每层技术举例：网络访问层（数据链路层）：ARP、RARP互联网层（网络层）：ICMP、IP传输层：TCP、UDP应用层：DNS、FTP、HTTP、SMTP、TELNET、IRC、WHOIS原文链接：https://juejin.im/post/59a0472f5188251240632f92 HTTP1）首先作为发送端的客户端在应用层 (HTTP 协议)发出一个想看某个 Web 页面的 HTTP 请求。2）接着，为了传输方便，在传输层(TCP 协议)把从应用层处收到的 数据(HTTP 请求报文)进行分割，并在各个报文上打上标记序号及端 口号后转发给网络层。3）在网络层(IP 协议)，增加作为通信目的地的 MAC 地址后转发给 链路层。这样一来，发往网络的通信请求就准备齐全了。4）接收端的服务器在链路层接收到数据，按序往上层发送，一直到应用层。当传输到应用层，才能算真正接收到由客户端发送过来的 HTTP 请求。参考：《图解HTTP》 HTTPSHTTPS同时采用两种加密方式，首先通过非对称加密交换稍后要使用的对称加密的密钥；之后再使用对称加密进行正常通信（非对称加密性能较差）Ps:对称加密和非对称加密（公钥加密，私钥解密。私钥加密，公钥解密）此时并不能保证客户端获取到的公钥是值得信赖的，此时需要证书的帮忙一次性工作：1）服务端运营人员将自己的公钥提交到证书认证机构2）认证机构判定申请者身份后，使用机构的私钥对服务端的公钥进行加密并颁发证书正常请求流程：1）双方先确定使用哪种加密算法以及密钥长度2）服务端将(证书+服务端公钥)发送到客户端，客户端通过内置在浏览器内的机构公钥对证书进行解密，如果二者相等表示服务端公钥可信任3）客户端将对称加密密钥通过服务端公钥加密发送到服务端4）服务端使用私钥进行解密获知对称加密的密钥]]></content>
      <categories>
        <category>Java Web</category>
      </categories>
      <tags>
        <tag>Java Web</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java Web专题（一）]]></title>
    <url>%2F2018%2F06%2F26%2Fjava-web-1%2F</url>
    <content type="text"><![CDATA[1.GET请求和POST请求2.RESTFUL API3.Session VS Cookie4.Forward VS Redirect GET请求和POST请求可以简单的认为HHTP中的POST GET PUT DELETE对应着CRUD区别：1）从语义上讲GET更偏向于查询操作，即GET请求不会对后台数据产生影响；而POST则不同2）一般来讲GET请求是幂等的Ps看到一个关于幂等很好的定义 abs(a) = abs(abs(a))3）GET将数据放在url上，而POST将数据放在HTTP body中；POST相对安全4）如果使用POST就无法保存书签 RESTFUL API参考链接：https://juejin.im/entry/59e460c951882542f578f2f01）URL表示资源；两种方式 /employees &amp; /employees/562）名词+方法替换动词url /createEmployee -&gt; POST + /employees /updateEmployee -&gt; PUT /employees/563）条件筛选时，使用?添加条件 GET /employees?state=internal&amp;maturity=senior4）非资源请求使用动词url GET /calculate?para2=23&amp;para2=432 POST（创建） GET（读取） PUT（更新） DELETE（删除） /employees 创建一个新员工 列出所有员工 批量更新员工信息 删除所有员工 /employees/56 （错误） 获取56号员工的信息 更新56号员工的信息 删除56号员工 Session &amp; CookieHTTP是一个无状态协议，如何维持同一个用户的会话？对于客户的第一个请求，容器会生成一个唯一的sessionId给到客户端，客户再以后的每一个请求中发回这个sessionId，这样容器就知道客户的身份了 客户端和容器如何交换SessionId？最常用的方法是通过cookie，在服务端给到客户端的response中，header中有一个叫Set-Cookie的属性，用于存放JSESSIONID;在客户端到服务端的request中，header中有一个叫Cookie的属性，用于存放JSESSIONID cookie和session的异同？二者都生成在服务端，而cookie（保存在客户端）可以视为session（保存在服务端）的一个助手，当然除了当助手之外cookie也可以实现自己的一些功能，比如用户名字的填充（js进行填充）看一段服务器端生成cookie的代码12345678910111213public class CookieTest extends HttpServlet &#123; public void doPost(HttpServletRequest request, HttpServletResponse response) &#123; response.setContenType("text/html"); //得到表单中用户填写的用户名 String name = request.get("username"); Cookie cookie = new Cookie("username",name); //jsessionId就是-1，浏览器关闭cookie死亡 cookie.setMaxAge(-1); response.setCookie(cookie); ... &#125;&#125; 而之后每次，服务端收到来自客户端的请求，都可以通过request.getCookies()获得所有的cookiePs看起来好像服务端为客户端创建的每个cookie，再之后客户端发送到请求中都会携带~ 如果禁用cookie这套机制怎么玩的转？用户禁用cookie后，会忽视response的header中的Set-Cookie；此时需要换一种办法来交换sessionId1）URL重写origin url + ;jsessionid=1231231注意这里是服务端返回给客户端的，在响应发回的html中（比如href）2）表单隐藏 分布式环境如何实现session共享？参考链接：https://blog.csdn.net/u010028869/article/details/50773174?ref=myread1）nginx对特定的sessionId进行映射，即一个用户的请求只会交给一个固定的机器处理优点：无须对session做任何处理只要存在本机即可缺点：如果某台机器挂掉，那么这台机器保存session的用户都要重新登录2）session复制，即所有的服务器共享session，任何一个服务器上session发生改变，都同步到其他服务器优点：容错性较好，即使机器挂掉也ok缺点：容易造成网络堵塞3）session第三方共享，redis（集群） or db（集群） 补充session的过时是由容器感知到的，可以在web-app下面配置1&lt;session-timeout/&gt; 当然也可以直接在代码中session.setMaxInactiveInterval()方法设置过期时间 cookie设计的初衷是为了支持会话状态，不过也可以用来做一些其他事情1）服务器和客户端直接交换key/value2）常用于填写登录用户的用户名 Forward VS Redirect二者都可以称为请求转发，前者称为直接请求转发，后者称为间接请求转发Redirect:客户端发送一个http请求，服务端Servlet1处理，之后执行response.sendRedirect(“target url”)，之后客户端再次向target url发送http请求Forward:客户端发送一个http请求，服务端Servlet1处理，之后调用requestDispatcher.forward(request,response)，进行请求转发，直接转发到Servlet2处理，最后处理完成后将response返回到客户端举个例子：Servlet将模型传递到jsp就是通过forward12RequestDispatcher view = request.getRequestDispatcher("result.jsp");view.forward(request,response);]]></content>
      <categories>
        <category>Java Web</category>
      </categories>
      <tags>
        <tag>Java Web</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JAVA中的设计模式]]></title>
    <url>%2F2018%2F06%2F24%2FDesign-Pattern%2F</url>
    <content type="text"><![CDATA[记录日常开发中常用的一些设计模式以及自己的一些体会 FACADE模式门面模式是对象的结构模式，外部与一个子系统的通信必须通过一个统一的门面对象进行。门面模式提供一个高层次的接口，使得子系统更易于使用。——-《JAVA与模式》优点： 屏蔽细节 客户端调用优雅 保护某些不希望被客户端访问的public方法 1234567891011121314151617181920212223242526272829303132Before://小猫管理者会喂猫，又会写情书，别的管理者都需要请求他来写情书，所以该方法开放class CatKeeper &#123; public void writeLoveLetter()&#123;&#125; public void feedCat()&#123;&#125;&#125;class DogKeeper &#123; public void feedDog()&#123;&#125;&#125;//此时动物园需要和每个动物管理员打交道，而且可能会调用写情书的方法这显然是不合理的class Zoo &#123; CatKeeper ck = new CatKeeper(); DogKeeper dk = new DogKeeper(); public void feedAnimals()&#123; ck.feedCat(); dk.feedDog(); &#125;&#125;After:class ZooKeeper &#123; CatKeeper ck = new CatKeeper(); DogKeeper dk = new DogKeeper(); public void feedAnimals()&#123; ck.feedCat(); dk.feedDog(); &#125;&#125;//此时动物园可以通过统一的zooKeeper饲养动物，同时也不会调用写情书的方法，而小猫管理员可以继续帮别的管理员写情书了class Zoo &#123; ZooKeeper zk = new ZooKeeper(); zk.feedAnimals();&#125; Ps: HttpServlet中的方法doGet/doPost入参为HttpServletRequest和HttpServletResponse，这两个参数是从tomcat中传递而来，实际上是对原始的request和response进行了一层封装，也就是facade模式，之后才传递到servlet中，，目的就是屏蔽request和response中某些不希望被外部访问的方法 模板方法模式常用于解决某些通用的部分中嵌有特异的部分通用的抽象，特异的定义抽象方法由各个子类实现12345678910111213141516171819202122232425262728293031323334353637383940Before:class Luyu&#123; public void visitZoo() &#123; buyTicket(); seeLion(); seeHawk(); goHome(); &#125; &#125;class HH&#123; public void visitZoo() &#123; buyTicket(); seeCat(); seeDolphin(); goHome(); &#125; &#125;After:abstract class Visitor&#123; abstract void seeMyFavorateAnimals(); public void visitZoo() &#123; buyTicket(); seeMyFavorateAnimals(); goHome(); &#125; &#125;class Luyu extends Visitor &#123; @Override public void seeMyFavorateAnimals() &#123; seeLion(); seeHawk(); &#125;&#125;class HH extends Visitor &#123; @Override public void seeMyFavorateAnimals() &#123; seeCat(); seeDolphin(); &#125;&#125; Ps 在spring中，对于抽象类的bean定义需要设置其abstract=”true” 单例模式参考文章：https://www.cnblogs.com/java-my-life/archive/2012/03/31/2425631.html饿汉模式1234567public class EagerSingleton &#123; private static EagerSingleton instance = new EagerSingleton(); private EagerSingleton()&#123;&#125; public static EagerSingleton getInstance() &#123; return instance; &#125;&#125; EagerSingleton类被加载的时候，会初始化静态变量，由JVM保证线程安全优点：简单无须同步缺点：不需要的时候也实例化，浪费空间Ps:说点题外话，这里说是饿汉其实也没有那么饿，其实真正new实例是在触发初始化的时候，而不是一上来就new类的生命周期：加载、验证、准备、解析、初始化、使用和卸载其中在准备阶段为类变量分配内存并设置类变量初始值，而初始化阶段才会进行类变量的赋值操作，常见的触发初始化的条件是new、get/put static fields当多个线程触发该类初始化时，由JVM保证类初始化的线程安全懒汉模式12345678910public class LazySingleton &#123; private static LazySingleton instance = null; private LazySingleton()&#123;&#125; public static synchronized LazySingleton getInstance()&#123; if(instance == null)&#123; instance = new LazySingleton(); &#125; return instance; &#125;&#125; 优点：在需要的时候才去实例化且线程安全缺点：每次调用getInstance方法获取实例时，都需要同步性能较差双重检查加锁1234567891011121314public class Singleton &#123; private volatile static Singleton instance = null; private Singleton()&#123;&#125; public static Singleton getInstance()&#123; if(instance == null)&#123; synchronized (Singleton.class) &#123; if(instance == null)&#123; instance = new Singleton(); &#125; &#125; &#125; return instance; &#125;&#125; 很经典的一个关于volatile关键字使用的例子，如果去掉该关键字，那么getInstance方法变为线程不安全很可能Thread1在执行instance = new Singleton()的时候，发生了指令重排，真正的实例化尚未完成，但是instance已经不为null这时候TThread2在第一重检查的时候通过了，直接返回了一个中间态的实例给到调用方，这是我们不能接受的。优点：不需要每次都同步且线程安全缺点：使用了volatile关键字使得虚拟机的一些优化手段被禁用，可能会对性能有影响静态内部类模式12345678910111213141516public class Singleton &#123; private Singleton()&#123;&#125; /** * 类级的内部类，也就是静态的成员式内部类，该内部类的实例与外部类的实例 * 没有绑定关系，而且只有被调用到时才会装载，从而实现了延迟加载。 */ private static class SingletonHolder&#123; /** * 静态初始化器，由JVM来保证线程安全 */ private static Singleton instance = new Singleton(); &#125; public static Singleton getInstance()&#123; return SingletonHolder.instance; &#125;&#125; 优点：懒汉模式且由JVM保证保证线程安全缺点：引入了静态内部类，代码稍显复杂1）上述方法都存在一个问题，如果客户端通过反射使得私有构造函数变为可访问，之后实例化，这个时候会产生多个”单例”此时一个方法是在构造函数中，对于创建的第二个实例抛出异常（AtomInteger可以保证）2）单例在序列化的时候相对复杂，首先实现Serializable接口，每个域都要置为transient，还需要提供一个readResolve方法枚举模式12345Enum Singleton &#123; INSTANCE; //类的方法 public void myMethod()&#123;&#125;&#125; 简洁优雅，有效抵挡反射、序列化攻击 代理模式-静态VS动态优雅地修改某个类中的既有方法且不修改原有代码静态代理PART1123interface ISubject &#123; void request();&#125; 123456class SubjectImpl implements ISubject &#123; @Override public void request()&#123; System.out.print("origin request"); &#125;&#125; 123456789101112class SubjectProxy implements ISubject &#123; private ISubject subject; public SubjectProxy(ISubject s)&#123; this.subject = s; &#125; @Override public void request()&#123; System.out.print("before origin request"); System.out.print("origin request"); System.out.print("after origin request"); &#125;&#125; 上面就是一个标准的代理模式，但是这样可能会有一些问题，如果系统内还有其他接口也有request方法，而我们也想对其代理相同的逻辑，这时候需要再去创建一个代理类换句话说有多少个接口，就需要创建多少个代理类，那这样我们的系统就显得过于臃肿了，这时候我们引出动态代理动态代理模式PART1 &amp; PART2同上，被代理的对象一定要实现接口1234567891011121314class RequestInvocationHandler implements InvocationHandler &#123; private Object target; public RequestInvocationHandler(Object target) &#123; this.target = target; &#125; public Object invoke(Object proxy,Method method,Object[] args) thorws Throwable &#123; //只对request方法进行代理 if(method.getName.equals("request"))&#123; System.out.print("before origin request"); method.invoke(obj,args); System.out.print("after origin request"); &#125; &#125;&#125; 这样上面这个handler就具有一定的通用性了，可以供所有接口使用。1234ISubject subject = new SubjectImpl();RequestInvocationHandler handler = new RequestInvocationHandler(subject);ISubject proxy = (ISubject)Proxy.newProxyInstance(subject.getClass().getClassLoader(),subject.getClass().getInterfaces(),handler);proxy.request(); 使用动态代理最大的问题就是只能对实现了接口的类进行代理，使用广度有些受限CGLIB动态字节码：对目标对象进行继承扩展，为其生成相应的子类，而子类可以通过覆写来扩展父类的行为，只要将横切逻辑的实现放到子类中，然后让系统使用扩展后的目标对象的子类，就可以达到与代理模式相同的效果。1234567891011class RequestCallback implements MethodInterceptor &#123; public Obeject intercept(Obeject object, Method method, Object[] args, MethodProxy proxy) &#123; //只对request方法进行代理 if(method.getName.equals("request"))&#123; System.out.print("before origin request"); method.invoke(obj,args); System.out.print("after origin request"); &#125; return null; &#125;&#125; 使用方式12345Enhancer enhancer = new Enhancer();enhancer.setSuperclass(SubjectImpl.class);enhancer.setCallback(new RequestCallback());SubjectImpl proxy = (SubjectImpl)enhancer.create();proxy.request(); 使用CGLIB对类进行扩展的唯一限制就是无法对final方法进行覆写 责任链模式-管道模型多条流水线，相似度较高，但仍有部分不同的模块或者流水线的顺序不同before:123456789101112131415class processLine0 &#123; public void process()&#123; module0(); module1(); module3(); &#125;&#125;class processLine1 &#123; public void process()&#123; module1(); module5(); module3(); module0(); &#125;&#125; after:1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556//框架层面代码//阀接口public interface Valve &#123; public void invoke(UniParam uni);&#125;//管道接口public interface Pipeline &#123; public void addValve(Valve valve); public void doInvoke(UniParam uni);&#125;public class MyPipeline implements PipeLine &#123; private List&lt;Valve&gt; valves = Lists.newArrayList(); private Valve getFirst()&#123; Collections.isEmpty(valves) ? throw Exception : return valves.get(0); &#125; @Override public void addValve(Valve valve)&#123; valves.add(valve); &#125; @Override public void doInvoke(UniParam uni)&#123; int cur = 0; while(cur &lt;= valves.size())&#123; valves.get(cur++).invoke(uni); &#125; &#125;&#125;//业务层面 自己实现//X=0,1,3,5public class ModuleX implements Valve &#123; @Override public void invoke(UniParam uni)&#123; //module自己的逻辑 doMyThing(uni); &#125;&#125;class processLine0 &#123; public void process()&#123; PipeLine pl = new PipeLine(); Uniparam uni = new Uniparam(); pl.add(new Module0()); pl.add(new Module1()); pl.add(new Module3()); pl.doInvoke(uni); &#125;&#125;class processLine1 &#123; public void process()&#123; PipeLine pl = new PipeLine(); pl.add(new Module1()); pl.add(new Module5()); pl.add(new Module3()); pl.add(new Module0()); pl.doInvoke(uni); &#125;&#125; 是不是灵活了许多缺点是valve的invoke方法必须要是同一个模板（入参返回值相同） 策略模式1234567891011121314151617181920212223242526272829interface Calculator &#123; int doExecute(int a, int b);&#125;class AddCalculator implements Calculator&#123; public int doExecute(int a, int b)&#123; return a+b; &#125;&#125;class MinusCalculator implements Calculator&#123; public int doExecute(int a, int b)&#123; return a-b; &#125;&#125;public class Test &#123; Calculator cal; public void setCal(Calculator cal)&#123; this.cal = cal; &#125; public void doCalculate(int a, int b)&#123; cal.doExecute(a, b); &#125; public static void main(String[] args) &#123; Test t = new Test(); t.setCal(new AddCalculator()); t.doCalculator(1,2); t.setCal(new MinusCalculator()); t.doCalculator(2,1); &#125;&#125; Spring中的bean的实例化过程采用的就是策略模式（reflect or CGLIB），默认是CGLIB]]></content>
      <categories>
        <category>Java 进阶</category>
      </categories>
      <tags>
        <tag>Java 进阶</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[读书笔记-暗时间]]></title>
    <url>%2F2018%2F06%2F02%2Fdark-time%2F</url>
    <content type="text"><![CDATA[读《暗时间》的一些体会与摘抄 重视知识的本质 抓住不变量，大量新技术只是一层皮，背后的支撑技术其实都是十多年不变的东西。底层知识永远都不过时；算法数据结构永远不过时；基本程序设计理论永远不过时；分析问题和解决问题的能力永远不过时； 区分手册知识和本质知识，以Netty为例，io/nio以及netty的部分源码就是其本质的知识，而具体的api使用方式就属于手册知识；zk的zab算法和paxos算法就是本质知识，而具体的api也是手册知识 关于Writing 博客应该是思考和学习的副产品，不要为了写文章而写文章 回顾知识 老生常谈的一个话题，既然开始写博客，那么就养成回顾博客的习惯好了，这样也督促自己写博客要更加认真 学习的金钥匙 组内有分享的习惯，我一般会在其他人分享某个主题之前有意识地去接触相关内容，这样一来自己遇到不懂的可以和分享者讨论，二来带着基础和问题去听分享会收获很多内容 人的大脑 情绪大脑VS理性大脑，情绪大脑早在远古就已经存在并且为物种的生存繁衍做出卓越的贡献了，它们的进化年代要比要比理性大脑久远得多，它们就像漫长岁月中陪伴着生物一路进化走来的老功臣，拥有强大的权利和力量，却没有意识到世界已经在最近的500年发生了迅速而巨大的变化]]></content>
      <categories>
        <category>我思故我在</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[JAVA中的I/O]]></title>
    <url>%2F2018%2F05%2F29%2FBIO-NIO-AIO%2F</url>
    <content type="text"><![CDATA[可以把整个IO分为两个阶段 等待数据到达 从内核拷贝数据到用户空间 I/O的三种方式 缓存IO 最常见的一种IO方式，磁盘-&gt;内核缓冲区-&gt;用户空间 直接IO 磁盘-&gt;用户空间，该方式IO见于数据库系统 内存映射 用户空间的一部分区域和内核缓冲区共享，比如epoll模式中 UNIX的5种I/O模型用钓鱼来类比操作系统的IO钓鱼的时候，刚开始鱼是在鱼塘里面的，我们的钓鱼动作的最终结束标志是鱼从鱼塘中被我们钓上来，放入鱼篓中。这里面的鱼塘就可以映射成磁盘，中间过渡的鱼钩可以映射成内核空间，最终放鱼的鱼篓可以映射成用户空间。一次完整的钓鱼（IO）操作，是鱼（文件）从鱼塘（硬盘）中转移（拷贝）到鱼钩（内核空间）再到鱼篓（用户空间）的过程。 阻塞I/O模型 等待数据到达+将数据从内核空间复制到用户空间，在等待数据阶段（等待鱼儿上钩阶段）拿着鱼竿啥都不做一直盯着鱼竿看，应用进程通过系统调用 recvfrom 接收数据，但由于内核还未准备好数据报，应用进程就会阻塞住，直到内核准备好数据报，recvfrom 完成数据报复制工作，应用进程才能结束阻塞状态。 非阻塞I/O模型 我们钓鱼的时候，在等待鱼儿咬钩的过程中，我们可以做点别的事情，比如玩一把王者荣耀、看一集《延禧攻略》等等。但是，我们要时不时的去看一下鱼竿，一旦发现有鱼儿上钩了，就把鱼钓上来。 应用进程通过 recvfrom 调用不停的去和内核交互，直到内核准备好数据。如果没有准备好，内核会返回error，应用进程在得到error后，过一段时间再发送recvfrom请求。在两次发送请求的时间段，进程可以先做别的事情。这种方式钓鱼，和阻塞IO比，所使用的工具没有什么变化，但是钓鱼的时候可以做些其他事情，增加时间的利用率。 信号驱动模型 我们钓鱼的时候，为了避免自己一遍一遍的去查看鱼竿，我们可以给鱼竿安装一个报警器。当有鱼儿咬钩的时候立刻报警。然后我们再收到报警后，去把鱼钓起来。 映射到Linux操作系统中，这就是信号驱动IO。应用进程在读取文件时通知内核，如果某个 socket 的某个事件发生时，请向我发一个信号。在收到信号后，信号对应的处理函数会进行后续处理。 应用进程预先向内核注册一个信号处理函数，然后用户进程返回，并且不阻塞，当内核数据准备就绪时会发送一个信号给进程，用户进程便在信号处理函数中开始把数据拷贝的用户空间中。这种方式钓鱼，和前几种相比，所使用的工具有了一些变化，需要有一些定制（实现复杂）。但是钓鱼的人就可以在鱼儿咬钩之前彻底做别的事儿去了。等着报警器响就行了。 I/O复用 我们钓鱼的时候，为了保证可以最短的时间钓到最多的鱼，我们同一时间摆放多个鱼竿，同时钓鱼。然后哪个鱼竿有鱼儿咬钩了，我们就把哪个鱼竿上面的鱼钓起来。 映射到Linux操作系统中，这就是IO复用模型。多个进程的IO可以注册到同一个管道上，这个管道会统一和内核进行交互。当管道中的某一个请求需要的数据准备好之后，进程再把对应的数据拷贝到用户空间中。 IO多路转接是多了一个select函数，多个进程的IO可以注册到同一个select上，当用户进程调用该select，select会监听所有注册好的IO，如果所有被监听的IO需要的数据都没有准备好时，select调用进程会阻塞。当任意一个IO所需的数据准备好之后，select调用就会返回，然后进程在通过recvfrom来进行数据拷贝。这里的IO复用模型，并没有向内核注册信号处理函数，所以，他并不是非阻塞的。进程在发出select后，要等到select监听的所有IO操作中至少有一个需要的数据准备好，才会有返回，并且也需要再次发送请求去进行文件的拷贝。这种方式的钓鱼，通过增加鱼竿的方式，可以有效的提升效率。 Ps：其实IO复用和阻塞IO很像，只不过是高效率版本的阻塞IO几种IO多路复用模型 select poll epoll 操作方式 遍历 遍历 回调 底层实现 数组 链表 哈希表 IO效率 线性遍历O(n) 线性遍历O(n) 事件通知方式，每当fd准备就绪，系统注册的回调函数会被调用O(1) 最大连接数 1024(x86) 2048(x64) 无上限 无上限 fd拷贝 每次调用select都需要把fd集合从用户态拷贝到内核态 每次调用poll都需要把fd集合从用户态拷贝到内核态 调用epoll_ct时拷贝进内核并保存，之后每次epoll_wait不拷贝 信号驱动I/O 我们钓鱼的时候，为了避免自己一遍一遍的去查看鱼竿，我们可以给鱼竿安装一个报警器。当有鱼儿咬钩的时候立刻报警。然后我们在收到报警后，去把鱼钓起来。 映射到Linux操作系统中，这就是信号驱动IO。应用进程在读取文件时通知内核，如果某个 socket 的某个事件发生时，请向我发一个信号。在收到信号后，信号对应的处理函数会进行后续处理。 通过系统调用sigaction执行一个信号处理函数（此系统调用立即返回，进程继续工作，它是非阻塞的）；当数据准备就绪后，为该进程生成一个SIGIO信号，通知应用程序调用recvfrom读取数据（此时仍然需要进程自身去调用recvfrom） 异步I/O 我们钓鱼的时候，采用一种高科技钓鱼竿，即全自动钓鱼竿。可以自动感应鱼上钩，自动收竿，更厉害的可以自动把鱼放进鱼篓里。然后，通知我们鱼已经钓到了，他就继续去钓下一条鱼去了。 映射到Linux操作系统中，这就是异步IO模型。应用进程把IO请求传给内核后，完全由内核去操作文件拷贝。内核完成相关操作后，会发信号告诉应用进程本次IO已经完成。 告知内核启动某个操作，并让内核在整个操作完成后通知我们（包括将数据从内核复制到用户空间） Ps异步I/O模型由内核通知我们I/O操作何时已经完成。而信号驱动I/O由内核告诉我们合适可以在进程内执行一个I/O操作 同步VS异步数据从内核到用户空间的复制是否需要当前进程参与，换句话讲read的逻辑代码是否在当前进程执行~ 阻塞VS非阻塞等待数据阶段是否是阻塞的（只盯着鱼竿看），这么看的话多路复用也是阻塞的 JAVA中常用的三种IO可以把JAVA中的IO看作是堆操作系统的各种IO模型的封装。比如 linux JAVA中的NIO和AIO都是基于epoll实现的，在Windows上是基于IOCP实现的，这对程序员来讲是透明的。 BIO（阻塞IO）同步阻塞：数据从内核到用户空间的复制在进程内且调用recvfrom后需要等待传统的服务器使用的就是BIO，每当客户端过来一个请求便开启一个新的线程，这种方式最大的问题就是过度依赖于线程。而线程是相对昂贵的资源 线程本身占用较大的内存，如果成千上万个请求发送到服务端，此时JVM的内存会被占用很多 线程切换的成本很高，甚至比线程本身的业务逻辑花的时间还要多 线程创建和销毁成本也相对较高 客户端个数/IO线程数 = 1:1 NIO（非阻塞IO）同步非阻塞：数据从内核到用户空间的复制在进程内且调用recvfrom后无需等待nginx使用的就是NIO用来接收数以千万计的客户端请求，之后再分发到不同的服务端tomcat容器之前采用bio，现在已经改为nio客户端个数/IO线程数 = M:1 AIO（异步IO）异步非阻塞：数据从内核到用户空间的复制不在进程内且当前进程根本无须调用recvfrom也就无须等待客户端个数/IO线程数 = M:0]]></content>
      <categories>
        <category>I/O</category>
      </categories>
      <tags>
        <tag>I/O</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Zookeeper应用场景]]></title>
    <url>%2F2018%2F05%2F25%2Fzk-apply%2F</url>
    <content type="text"><![CDATA[zk的原理和数据结构，在前面已经简单介绍过了，接下来看看如何利用zk的特性实现一些神奇的功能 命名服务分布式数据库自增id的唯一性 利用znode SEQUENTIAL不同的客户端对同一个zk集群调用create方法，创建SEQUENTIAL节点（对于其父节点唯一） 配置管理在web运行过程中，动态的修改一些配置信息，让其立即生效，而不需要重启项目同一个web应用运行在不同的机器上，要保证对配置信息的修改能够对所有机器都可见 将配置信息保存在某个节点内，一旦配置信息发生变化，会通知客户端（推模式）也可以客户端每次加载配置信息时，读取最新的数据（拉模式）仔细思考下，其实和存放在数据库中效果类似，但是zk集群能够保证服务的稳定性，而如果使用数据库必然使用分布式数据库，此时一样需要zk来协调 RPC服务的注册与发现比如业界知名的dubbo 对于service的注册和发现都是基于zk实现的 服务提供方：在名为serviceName的父节点下创建ip+port子节点（同一个serviceName的节点下有多个以ip+port为名字的节点）服务消费方：根据serviceName去zk查询可提供服务的ip和port执行rpc调用（tcp方式） Master选举以分布式数据库为例子，所有的数据库服务器在同一个zk父节点下创建同名的临时节点，举个例子 /master/election/master 存储当前机器的ip和提供数据库服务的port这时候只有一个服务器能够创建成功，这个创建成功的服务器就成为了Master，而其他创建失败的服务器需要在父节点/master/election上注册一个子节点变更的watcher一旦发现master挂了，就重新选举masteror每个客户端在 /master/election/下注册临时有序节点 /client1,/client2 etc并且在父节点注册watcher默认选取client序号最小的客户端作为master，加入该客户端宕机，那么对应的节点将被删除，此时父节点下序号最小的节点对应的客户端当选master 分布式锁假设现在有两个不同客户端的线程都要处理某个共享资源，如何保证资源能够被预期地被处理？这时候需要分布式锁，加锁的过程就是在zk指定节点下创建子节点比如/locks/lock创建成功的线程表示获取锁成功，而失败的客户端会对这个父节点进行监听释放锁的时候分为两种情况，第一种情况，机器宕机，此时临时节点被删除；第二种情况，获取锁的线程执行完业务逻辑主动删除该节点接下来其他想要获取锁的线程会得到通知，并再次进行竞争 分布式队列分布式环境，如何保证消息的有序消费，如何保证消息不会重复消费？自己yy的一个方案利用zk实现的FIFO队列，在特定的目录下创建SEQUENTIAL节点，出队列的时候通过getChild()返回当前队列中所有的元素，找到最小序号的节点，如果该节点下有子节点（表示该任务已经被消费），那么再去找次小的节点，假设该节点下无子节点，那么在该节点下创建子节点（表示当前任务已被消费）这样可以保证任务被有序消费，且不会被重复消费 Ps 但是此时的场景仅仅是有序，并非任务1消费过后 再去消费任务2]]></content>
      <categories>
        <category>Java中间件</category>
      </categories>
      <tags>
        <tag>Zookeeper</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ZAB协议]]></title>
    <url>%2F2018%2F05%2F24%2FZAB%2F</url>
    <content type="text"><![CDATA[zk本身提供分布式系统的协调服务；为了防止zk本身挂掉影响整个分布式集群，引入了ZAB协议ZAB协议是zookeeper中专门设计的一种支持崩溃恢复的原子广播协议。Ps:这篇文章必须加精 概述消息广播 Ps：以和zk集群follower相连接的client发送一个写请求为例 客户端提交事务 服务器生成与之对应的事务Proposal 并将该Proposal（提议）发给所有的Follower机器 Follower收到Proposal后，将其以事务日志的形式写入到本地磁盘中 写入成功后反馈给Leader一个ACK Leader收到Quorum的ACK后，自身先完成对事务的提交 之后广播Commit消息给所有Follower服务器 Follower执行事务的提交 崩溃恢复 Leader选举算法：确保提交已经被旧Leader提交的事务Proposal，同时丢弃已经被旧Leader跳过的事务Proposal选举算法：保证新选举出来的Leader服务器拥有集群中所有机器最高编号（ZXID最大）的事务ProposalPs:ZXID（事务的id）：低32位是简单的计数器，客户端每来一个事务请求，服务器产生一个新的事务的时候，都会对计数器加1；而高32位则代表了Leader周期epoch编号（朝代，每当新Leader产生就要++) 详解ZAB协议的三个阶段： 发现(elect leader) Follower将自己最后接收的Proposal的epoch值发送给准Leader 接收到来自过半Follower的epoch后，会从这些epoch中选择最大的epoch并++，再发送给过半的Follower Follower收到新的epoch，检测最后处理过得proposal的epoch，如果小于新的epoch，那么更新为新的epoch，同时向准Leader发送Ack消息（包含当前的epoch以及历史处理过得Proposal的集合 ） 准Leader收到过半Ack后，会从这过半的Ack中选择epoch最大的（如果相等选zxid最大的），将其历史Proposal的集合置为当前的初始化事务集合 同步 准Leader会发送消息给Quorum中的Follower（消息内容=新的epoch+准Leader初始化的历史Proposal集合） Follower接收到消息后，如果发现自己的epoch与准Leader的不相等，那么无法同步（不在一个朝代）；反之，Follower会接受并处理每一个Proposal，并发送反馈给Leader 当准Leader收到过半Follower的反馈后，会向Follower发送Commit消息 Follower收到Commit后会依次处理并提交准Leader发来的且未处理过的事务Ps完成该步骤后，准Leader才真正称为Leader 广播 客户端提交事务 服务器生成与之对应的事务Proposal 并将该Proposal（提议）发给所有的Follower机器 Follower收到Proposal后，将其以事务日志的形式写入到本地磁盘中 写入成功后反馈给Leader一个ACK Leader收到Quorum的ACK后，自身先完成对事务的提交 之后广播Commit消息给所有Follower服务器 Follower执行事务的提交 在ZAB协议中，每一个zk节点都有可能处于以下三个状态： LOOKING：Leader选举阶段 FOLLOWING：Follower服务器和Leader保持同步状态 LEADING：Leader服务器作为Leader领导Follower状态 且每一个节点，都会循环执行这三个阶段，发现、同步、广播Ps: Leader和Follower之间保持一个tcp长连接 ，如果在指定的超时时间内Leader无法从过半Follower获取到心跳包，那么Leader就结束对当前”epoch”的领导 其他的Follower会转换到LOOKING状态开始新的一轮选举 延伸思考1.准Leader如何选举？答：投票选举，每个服务器都会生成投票信息（myid,ZXID）,首先投给自己，之后再广播给集群中的其他机器，集群中的服务器接收到投票后会判断收到的票的有效性（包括是否是本轮投票，是否来自LOOKING状态的服务器），接下来将别人的票和自己的票做PK，优先PK ZXID，如果一样那么PK myid，将自己的票更新为PK胜利方的信息，并再次将票广播给其他机器，每次投票后，服务器都会统计所有投票，判断是否有过半的机器收到相同的投票信息 Ps myid并不重要，可能仅仅是代表一个序号，只要不重复就ok 2.如何保证不会丢失已经提交的提议（指的是整个集群）？答：这个是相当的有意思，假设所有子民{0，1，2，3，4，5}，第一个朝代国王是0，子民是{1，2，3}，Ps成立朝代的标准是子民要超过半数，在某个提议整个朝代commit后（假设当前zxid=3），国王生病，这时候要更换新的国王，假设是5，而新的朝代子民也要超过半数，那么这就意味着新的王国内部必然有前朝子民，假设是{3，4，1}，新的国王会更新朝代epoch+1，并从当前子民中找到epoch最大或者epoch相等&amp;zxid最大的（这种方式可以防止”复辟”，即小epoch大zxid当选leader），将其提交过的事务更新为自己做过的事务，并将自己的zxid置为0，然后勒令子民们和自己做过的事务进行同步，哈哈，这样就保证了朝代更替不会导致已经被提交的事务丢失~Ps 这里说的zxid指的是zxid的低32位 总结ZooKeeper集群：1）读写数据库2）保证一致性的集群3）可以感知客户端的变化4）主要解决分布式应用中经常遇到的一些数据管理问题 Ps:分布式系统的节点可以通过第三方节点进行数据共享，而这个第三方节点（zk）肯定也是由集群构成的（不然会有单点问题），所以第三方节点间也要通过协议保证（ZAB协议）数据的一致性]]></content>
      <categories>
        <category>Java中间件</category>
      </categories>
      <tags>
        <tag>Zookeeper</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Zookeeper概览]]></title>
    <url>%2F2018%2F05%2F24%2FZookeeper%2F</url>
    <content type="text"><![CDATA[首先Zookeeper（下文简称zk）是一种”分布式协调服务”，我们常见的分布式系统中都有zk的影子。zk通过简单的架构和API解决了分布式系统难以协调的问题。这样开发人员可以更专注于核心应用程序的逻辑，而不用为分布式环境下的应用程序的协调而担心。 zk的数据结构zk的数据结构类似文件系统，zk的基本存储单元是节点（Znode），每一个Znode具有唯一确定的路径 Znode的结构Znode由data acl child stat组成 data 代表当前Znode存储的数据信息 acl 代表当前Znode的访问权限 stat Znode的元数据 版本号 每当和Znode相关联的数据发生变化时，对应的版本号也会增加。当多个zk客户端在同一znode上操作时，版本号很重要。 ACL 存储当前znode的访问权限 时间戳 数据长度 - 存储在znode中数据总量 child 表示当前节点的子节点们 Znode的类型 持久节点(PERSISTENT) 该类型节点只有创建它的客户端主动执行删除操作才会被删除，否则会一直存在 临时节点(EPHEMERAL) 该类型节点，和客户端会话紧密相关，当客户端和zk集群断开连接时，临时节点会自动删除。临时节点不允许有子节点 顺序节点(SEQUENTIAL) 创建这个类型的节点时候会自动在其节点名后面追加一个整型数字（该数字由父节点维护） zk的事务zk的事务是指能够改变zk服务器状态的操作，一般包括节点的创建、删除、节点内容更新、客户端会话创建与失效等。对于每一个事务，zk都会为其分配一个全局唯一的事务ID（ZXID）；每一个ZXID对应一次更新操作 Session客户端连接到服务器，会向客户端分配SessionId，同一个会话中的请求按照FIFO顺序执行。客户端以特定的时间间隔发送心跳，如果时间间隔大于服务器指定的超时时间，那么视为会话过期，将判定客户端死机。]]></content>
      <categories>
        <category>Java中间件</category>
      </categories>
      <tags>
        <tag>Zookeeper</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深入分析ReentrantLock]]></title>
    <url>%2F2018%2F05%2F23%2FReentrantLock%2F</url>
    <content type="text"><![CDATA[本文分析了ReentrantLock结合AQS加锁的全过程Doug Lea大神高屋建瓴 UML 关键代码分析AQS#acquire方法说明 AQS中最为核心的方法 这是一个模板方法模式，AQS虽然实现了acquire，和release方法，但是里面调用的tryAcquire和tryRelease是由子类来定制的 可以认为同步状态的维护、获取、释放动作是由子类实现的功能，而动作成功与否的后续行为由AQS框架来实现 相当于AQS做了架构设计，具体的实现由我们自己定义 该方法对线程的中断不能及时响应（jdk提供了acquireInterruptibly()该方法可以及时响应中断） ReentrantLock类持有Sync类，而Sync类是AQS的子类12345public final void acquire(int arg) &#123; if (!tryAcquire(arg) &amp;&amp; acquireQueued(addWaiter(Node.EXCLUSIVE), arg)) selfInterrupt();&#125; 整体流程 tryAcquire#具体实现由子类提供，该方法去获取锁成功返回true反之false addWaiter#将获取失败的线程加入到队列的尾部 acquireQueued#线程进入队列后不会立即挂起，acquireQueued方法会让线程进行自旋，当满足挂起条件的时候（前置节点状态为SIGNAL）挂起该线程 selfInterrupt#线程自我中断 ReentrantLock#NonfairSync#nonfairTryAcquirePs:模板方法tryAcquire的具体实现1234567891011121314151617181920212223final boolean nonfairTryAcquire(int acquires) &#123; final Thread current = Thread.currentThread(); //此时AQS的state代表当前线程获取原子状态的次数，如果次数为零，那么就说明这个线程放弃了锁（也有可能其他线程占据着锁从而需要等待）， //如果次数大于1，也就是获得了重进入的效果，而其他线程只能被park住，直到这个线程重进入锁次数变成0而释放原子状态。 int c = getState(); if (c == 0) &#123; //不同线程同时执行CAS操作，只有一个能成功获取到锁 if (compareAndSetState(0, acquires)) &#123; //获取到锁的线程，将被设置为当前的独占线程 setExclusiveOwnerThread(current); return true; &#125; &#125; //如果当前线程本就持有锁，那么增加state的值 else if (current == getExclusiveOwnerThread()) &#123; int nextc = c + acquires; if (nextc &lt; 0) // overflow throw new Error("Maximum lock count exceeded"); setState(nextc); return true; &#125; return false;&#125; Ps:关于AQS的state ReentrantLock用它来表示所有线程已经重复获取该锁的次数 Semaphore用它来表示剩余的许可数量 Ps共享锁采用的就是这个 FutureTask用它来表示任务的状态（尚未开始、正在运行、已完成以及已取消） AQS#Node123456789101112131415161718192021222324252627282930static final class Node &#123; static final Node SHARED = new Node(); static final Node EXCLUSIVE = null; //表示等待的线程已经取消或者中断 static final int CANCELLED = 1; //表示后一个节点需要唤醒，当前节点如果释放锁，则需要唤醒后继节点 static final int SIGNAL = -1; //表示当前的节点是一个条件等待，即需要等待其他的条件满足才能够被加入到同步队列，等待被唤醒 static final int CONDITION = -2; static final int PROPAGATE = -3; volatile int waitStatus; volatile Node prev; volatile Node next; volatile Thread thread; Node nextWaiter; final boolean isShared() &#123; return nextWaiter == SHARED; &#125; final Node predecessor() throws NullPointerException &#123; Node p = prev; if (p == null) throw new NullPointerException(); else return p; &#125;&#125; Node节点中最核心的是waitStatus，此处waitStatus的取值分别可以为： 1表示等待的线程已经取消或者中断；-1表示后一个节点需要唤醒，当前节点如果释放锁，则需要唤醒后继节点；-2表示当前的节点是一个条件等待，即需要等待其他的条件满足才能够被加入到同步队列，等待被唤醒-3表示下一个acquireShared应无条件传播(在读写锁中会遇到，后面会专门写文章分析读写锁)0表示初始状态 AQS#addWaiter、AQS#enq12345678910111213141516171819private Node addWaiter(Node mode) &#123; Node node = new Node(Thread.currentThread(), mode); // Try the fast path of enq; backup to full enq on failure Node pred = tail; //如果队列不为空 if (pred != null) &#123; //新node前置节点置为tail node.prev = pred; //如果tail=pred，那么置tail为node if (compareAndSetTail(pred, node)) &#123; //老tail也就是现在的倒数第二个node的后置节点置为node也就是新tail pred.next = node; return node; &#125; &#125; //如果队列为空执行enq or cas失败（竞争激烈） enq(node); return node;&#125; 1234567891011121314151617private Node enq(final Node node) &#123; for (;;) &#123; Node t = tail; //如果队列为空，初始化队列 if (t == null) &#123; // Must initialize if (compareAndSetHead(new Node())) //注意head节点是空节点 不表示任何线程 tail = head; &#125; else &#123; node.prev = t; //不为空，老tail也就是现在的倒数第二个node的后置节点置为node也就是新tail if (compareAndSetTail(t, node)) &#123; t.next = node; return t; &#125; &#125; &#125;&#125; AQS#acquireQueued()node插入队尾之后，不会立刻挂起，如果前置节点是head会尝试执行自旋；自旋失败会执行shouldParkAfterFailedAcquire方法，该方法1）判断前置节点是否为-1，如果是那么可以安全挂起2）如果不是，清除无效的线程（被取消）3）cas将pred状态置为-1，下次再进入该方法返回true，可以安全挂起当前线程123456789101112131415161718192021222324252627final boolean acquireQueued(final Node node, int arg) &#123; boolean failed = true; try &#123; boolean interrupted = false; for (;;) &#123; final Node p = node.predecessor(); //如果node的前置节点是头节点 //这里有一个有意思的地方，在队列里面是公平的排队获取锁，所谓的非公平是指队列里被释放的node和新来的尚未入队的node if (p == head &amp;&amp; tryAcquire(arg)) &#123; //自旋成功，这个head没啥用 setHead(node); p.next = null; // help GC failed = false; return interrupted; &#125; //当p的waitStatus为SIGNAL的时候第一个条件才返回true //Ps只有当前置节点状态为SIGNAL的时候，当前节点才能被安全地挂起 //对线程的中断不能及时响应 if (shouldParkAfterFailedAcquire(p, node) &amp;&amp; parkAndCheckInterrupt()) interrupted = true; &#125; &#125; finally &#123; if (failed) cancelAcquire(node); &#125;&#125; AQS#shouldParkAfterFailedAcquire1234567891011121314151617private static boolean shouldParkAfterFailedAcquire(Node pred, Node node) &#123; int ws = pred.waitStatus; //该状态表示后一个节点需要唤醒，当前节点如果释放锁，则需要唤醒后继节点 if (ws == Node.SIGNAL) return true; //该状态表示线程已经取消 if (ws &gt; 0) &#123; do &#123; node.prev = pred = pred.prev; &#125; while (pred.waitStatus &gt; 0); pred.next = node; &#125; else &#123; //否则通过cas将pred状态置为-1，一般是从0-&gt;-1;这样下次循环就可以顺利地将当前线程挂起了 compareAndSetWaitStatus(pred, ws, Node.SIGNAL); &#125; return false;&#125; AQS#parkAndCheckInterrupt123456private final boolean parkAndCheckInterrupt() &#123; //挂起当前线程，不再执行自旋 当被唤醒之后继续自旋 调用tryAcquire但是此时和新来的未入队的线程一起竞争 LockSupport.park(this); //返回当前线程中断位状态 return Thread.interrupted();&#125; 执行流程图 透过实例看Node12345678910111213141516171819202122public static void main(String[] args) &#123; final ReentrantLock = new ReentrantLock(); Thread thread5 = new Thread( new Runnable() &#123; @Overide public void run() &#123; lock.lock(); try&#123; Thread.sleep(60*1000); &#125; catch (Execption e) &#123;&#125; finnaly &#123; lock.unlock(); &#125; &#125; &#125;, "thread5" ); //依次类推thread6~thread8 thread5.start(); thread6.start(); thread7.start(); thread8.start();&#125; 线程5获取锁时候此时Node结构如下，可以看到head节点是一个空节点，刚入队的node的状态为0 与Condition的联动我们知道condition的await/signal方法经常和ReentrantLock联动，深入源码看看二者的关系Condition类位于AQS中123456public class ConditionObject implements Condition, java.io.Serializable &#123; /** First node of condition queue. */ private transient Node firstWaiter; /** Last node of condition queue. */ private transient Node lastWaiter; &#125; 而Node的结构是这样的12345volatile int waitStatus;volatile Node prev;volatile Node next;volatile Thread thread;Node nextWaiter; 私以为这样设计虽然最大程度实现了复用但是可读性不强，这里的waitStatus等于2的时候表示的是位于Condition队列中的节点，而Condition队列就是nextWaiter，显然这是一个单向队列，而且显然一个Condition对应一个等待队列。 提起Condition必然要看它核心的await/signal方法，我们知道这二者都需要调用它们的线程持有锁资源，不然会直接报错先看看await方法12345678910111213141516171819202122232425public final void await() throws InterruptedException &#123; if (Thread.interrupted()) throw new InterruptedException(); //向waiter队列添加node Node node = addConditionWaiter(); //释放资源，显然当前线程位于同步队列的头部 int savedState = fullyRelease(node); int interruptMode = 0; //如果不在同步队列中 while (!isOnSyncQueue(node)) &#123; //挂起当前线程 LockSupport.park(this); //检查中断 if ((interruptMode = checkInterruptWhileWaiting(node)) != 0) break; &#125; //其他线程调用signal后，当前线程被唤醒会执行这段代码，此时该线程的node已经位于同步队列中了 //和调用lock一样，进入队列后并非立即挂起 if (acquireQueued(node, savedState) &amp;&amp; interruptMode != THROW_IE) interruptMode = REINTERRUPT; if (node.nextWaiter != null) // clean up if cancelled unlinkCancelledWaiters(); if (interruptMode != 0) reportInterruptAfterWait(interruptMode);&#125; 看看addConditionWaiter1234567891011121314151617private Node addConditionWaiter() &#123; Node t = lastWaiter; // 清除等待队列中的无效node（status&gt;0） if (t != null &amp;&amp; t.waitStatus != Node.CONDITION) &#123; unlinkCancelledWaiters(); t = lastWaiter; &#125; //构造node，注意这里的CONDITION = -2; Node node = new Node(Thread.currentThread(), Node.CONDITION); //入队操作 if (t == null) firstWaiter = node; else t.nextWaiter = node; lastWaiter = node; return node;&#125; 入队之后释放当前锁资源123456789101112131415161718final int fullyRelease(Node node) &#123; boolean failed = true; try &#123; //重入次数 int savedState = getState(); //更新AQS当前线程以及state值 //唤醒同步队列后面挂起的线程 if (release(savedState)) &#123; failed = false; return savedState; &#125; else &#123; throw new IllegalMonitorStateException(); &#125; &#125; finally &#123; if (failed) node.waitStatus = Node.CANCELLED; &#125;&#125; await方法大概就是这样了，接着看看signal方法干了啥123456789public final void signal() &#123; //是否持有锁，未持有报错 if (!isHeldExclusively()) throw new IllegalMonitorStateException(); Node first = firstWaiter; if (first != null) //释放等待队列的头节点 doSignal(first);&#125; 123456789private void doSignal(Node first) &#123; do &#123; //从等待队列中出队 if ( (firstWaiter = first.nextWaiter) == null) lastWaiter = null; first.nextWaiter = null; &#125; while (!transferForSignal(first) &amp;&amp; (first = firstWaiter) != null);&#125; 1234567891011final boolean transferForSignal(Node node) &#123; //cas将node状态由-2更新为0 if (!compareAndSetWaitStatus(node, Node.CONDITION, 0)) return false; //入同步队列 Node p = enq(node); int ws = p.waitStatus; if (ws &gt; 0 || !compareAndSetWaitStatus(p, ws, Node.SIGNAL)) LockSupport.unpark(node.thread); return true;&#125; 主流程到这里就结束了，利用AQS内部的两个队列（一个双向一个单向）实现了lock与conditon的联动 ReentrantLock自问自答1.如何竞争一把锁？ReentrantLock内部使用AQS来维持一整套锁机制，在两个线程同时调用lock方法时，会通过CAS同时对state状态进行更新（此时为0，欲更新为1）compareAndSetState(0, acquires)；更新失败的线程会进入AQS『监狱』-Node链中，会放到队尾；但是此时该线程不会被立刻挂起，还会执行一段时间的自旋,直到当前的node的前置node状态(volatile status)被置为SIGNAL 2.为什么说是非公平的？公平锁如何实现有啥缺点？因为之前获取锁失败的线程可能正在queue（node链）中排队，而新来的一个线程直接调用cas操作去获取锁；公平锁相对非公平锁很简单，就是在新线程执行CAS之前判断下queue中是否有等待的线程，如果没有才能执行CAS操作，公平锁效率可能会低一些，可能每个线程都要经历从内核态到用户态的切换比较耗时 参考资料http://ifeve.com/%E9%80%8F%E8%BF%87reentrantlock%E7%AA%A5%E6%8E%A2aqs/https://www.jianshu.com/p/28387056eeb4]]></content>
      <categories>
        <category>多线程</category>
      </categories>
      <tags>
        <tag>多线程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[foreach循环中为什么不要进行remove/add操作]]></title>
    <url>%2F2018%2F05%2F17%2Fremove-add-foreach%2F</url>
    <content type="text"><![CDATA[先来看一段代码，摘自阿里巴巴的java开发手册12345678List&lt;String&gt; a = new ArrayList&lt;String&gt;(); a.add("1"); a.add("2"); for (String temp : a) &#123; if("1".equals(temp))&#123; a.remove(temp); &#125; &#125; 此时执行代码，没有问题，但是需要注意，循环此时只执行了一次。具体过程后面去分析。再来看一段会出问题的代码12345678List&lt;String&gt; a = new ArrayList&lt;String&gt;(); a.add("1"); a.add("2"); for (String temp : a) &#123; if("2".equals(temp))&#123; a.remove(temp);&#125; &#125; 输出为:Exception in thread “main” java.util.ConcurrentModificationException是不是很奇怪？接下来将class文件，反编译下，结果如下123456789101112List a = new ArrayList(); a.add("1"); a.add("2"); Iterator i$ = a.iterator(); do &#123; if(!i$.hasNext()) break; String temp = (String)i$.next(); if("1".equals(temp)) a.remove(temp);&#125; while(true); 几个需要注意的点： foreach遍历集合，实际上内部使用的是iterator。 代码先判断是否hasNext，然后再去调用next，这两个函数是引起问题的关键。 这里的remove还是list的remove方法。 先去观察下list.remove()方法中的核心方法fastRemove()方法。12345678private void fastRemove(int index) &#123; modCount++; int numMoved = size - index - 1; if (numMoved &gt; 0) System.arraycopy(elementData, index+1, elementData, index, numMoved); elementData[--size] = null; // clear to let GC do its work &#125; 注意下,modCount++,此处先不表，下文再说这个参数。顺路观察下list.add()方法12345public boolean add(E e) &#123; ensureCapacityInternal(size + 1); // Increments modCount!! elementData[size++] = e; return true; &#125; 注意第二行的注释，说明这个方法也会使modCount++ 再去观察下，iterator()方法123public Iterator&lt;E&gt; iterator() &#123; return new Itr(); &#125; 123456789101112131415161718192021222324252627282930313233343536373839404142private class Itr implements Iterator&lt;E&gt; &#123; int cursor; // index of next element to return int lastRet = -1; // index of last element returned; -1 if no such int expectedModCount = modCount; public boolean hasNext() &#123; return cursor != size; &#125; @SuppressWarnings("unchecked") public E next() &#123; checkForComodification();//万恶之源 int i = cursor; if (i &gt;= size) throw new NoSuchElementException(); Object[] elementData = ArrayList.this.elementData; if (i &gt;= elementData.length) throw new ConcurrentModificationException(); cursor = i + 1; return (E) elementData[lastRet = i]; &#125; public void remove() &#123; if (lastRet &lt; 0) throw new IllegalStateException(); checkForComodification(); try &#123; ArrayList.this.remove(lastRet); cursor = lastRet; lastRet = -1; expectedModCount = modCount; &#125; catch (IndexOutOfBoundsException ex) &#123; throw new ConcurrentModificationException(); &#125; &#125; final void checkForComodification() &#123; if (modCount != expectedModCount) throw new ConcurrentModificationException(); &#125; &#125; 几个需要注意的点： 在iterator初始化的时候（也就是for循环开始处），expectedModCount = modCount，猜测是和当时list内部的元素数量有关系(已证实)。 当cursor != size的时候，hasNext返回true next()函数的第一行，checkForComodification()这个函数就是报错的原因 这个函数就是万恶之源 第39行，mod != expectedModCount 就会抛出ConcurrentModificationException() 接下来分析文章开头的第一个例子，为啥不会报错？第一个例子执行完第一次循环后，mod = 3 expectedModCount =2 cursor = 1 size = 1 所以程序在执行hasNext()的时候会返回false，所以程序不会报错。第二个例子执行完第二次循环后,mod = 3 expectdModCount = 2 cursor = 2 size = 1 此时cursor != size 程序认定还有元素，继续执行循环，调用next方法但是此时mod != expectedModCount 所以此时会报错。道理我们都懂了，再看一个例子123456789101112public static void main(String[] args) throws Exception &#123; List&lt;String&gt; a = new ArrayList&lt;String&gt;(); a.add("1"); a.add("2"); for (String temp : a) &#123; System.out.println(temp); if("2".equals(temp))&#123; a.add("3"); a.remove("2"); &#125; &#125;&#125; 此时输出为：12显然，程序并没有执行第三次循环，第二次循环结束，cursor再一次等于size，程序退出循环。 与remove类似，将文章开头的代码中remove替换为add，我们会发现无论是第一个例子还是第二个例子，都会抛出ConcurrentModificationException错误。原因同上，代码略。 手册上推荐的代码如下12345Iterator&lt;String&gt; it = a.iterator(); while(it.hasNext())&#123; String temp = it.next(); if(删除元素的条件)&#123; it.remove(); &#125; &#125; 此时remove是iterator的remove，我们看一下它的源码：1234567891011121314public void remove() &#123; if (lastRet &lt; 0) throw new IllegalStateException(); checkForComodification(); try &#123; ArrayList.this.remove(lastRet); cursor = lastRet; //index of last element returned;-1 if no such lastRet = -1; expectedModCount = modCount; &#125; catch (IndexOutOfBoundsException ex) &#123; throw new ConcurrentModificationException(); &#125; &#125; 注意第10行，第8行，所以此时程序不会有之前的问题。但是手册上推荐的方法，在多线程环境还是有可能出现问题，一个线程执行上面的代码，一个线程遍历迭代器中的元素，同样会抛出CocurrentModificationException。如果要并发操作，需要对iterator对象加锁。 平时遍历list，然后删除某个元素的时候，如果仅仅删除第一个且删除之后调用break //代表着此时不会再去执行iterator.next方法 也就不会触发万恶之源而如果要删除所有的某元素，则会报错，谨记！Ps再来看一个佐证12345678910111213public static void main(String[] args) &#123; ArrayList&lt;Integer&gt; list = new ArrayList&lt;&gt;(); list.add(1); list.add(2); list.add(3); for(int i : list)&#123; System.out.println(i); if(i == 2)&#123; list.remove((Object)2); &#125; &#125; &#125; 此时只会输出12当把remove对象改为3时候，再次报错。]]></content>
      <categories>
        <category>Java 进阶</category>
      </categories>
      <tags>
        <tag>Java 进阶</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[How to Digest a Framework]]></title>
    <url>%2F2018%2F05%2F17%2Fhow-to-read-a-framework%2F</url>
    <content type="text"><![CDATA[找到入口 UML画出主流程（纲举目张） 研究重要细节，推荐新启UML 沉浸code外，仍需不时跳出code，最好参考说明文档，弄清核心功能实现方式 全局搜索、查找方法调用处以及debug是好帮手 除UML外，最好贴上核心code，并附上自己的一些理解与说明 核心功能，想象一下如果换作你怎么实现，最好用code实现 最最最后，当然是自己造一个轮子了]]></content>
      <categories>
        <category>我思故我在</category>
      </categories>
      <tags>
        <tag>我思故我在</tag>
      </tags>
  </entry>
</search>
